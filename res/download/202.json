{
    "items": [
        {
            "tags": [
                "kubernetes"
            ],
            "owner": {
                "reputation": 481,
                "user_id": 3564491,
                "user_type": "registered",
                "accept_rate": 56,
                "profile_image": "https://i.stack.imgur.com/Qyav0.gif?s=128&g=1",
                "display_name": "Vartika",
                "link": "https://stackoverflow.com/users/3564491/vartika"
            },
            "is_answered": false,
            "view_count": 21,
            "answer_count": 2,
            "score": 0,
            "last_activity_date": 1523525595,
            "creation_date": 1523519606,
            "question_id": 49790905,
            "body_markdown": "We are using kubernetes for deployment of our microservices , In production mode we will not be able to access kubernetes directly , So is there any way to perform operation like delete a pod using rest api calls or perform any operation through http requests.\r\n\r\nWe need to delete a pod or its replica set to restart the pod forcefully.\r\n\r\nLike we access a remote kubernetes and delete a particular pod at runtime",
            "link": "https://stackoverflow.com/questions/49790905/delete-kubernetes-pod-at-run-time",
            "title": "Delete kubernetes pod at run time",
            "body": "<p>We are using kubernetes for deployment of our microservices , In production mode we will not be able to access kubernetes directly , So is there any way to perform operation like delete a pod using rest api calls or perform any operation through http requests.</p>\n\n<p>We need to delete a pod or its replica set to restart the pod forcefully.</p>\n\n<p>Like we access a remote kubernetes and delete a particular pod at runtime</p>\n"
        },
        {
            "tags": [
                "python",
                "pandas"
            ],
            "owner": {
                "reputation": 98,
                "user_id": 5438045,
                "user_type": "registered",
                "accept_rate": 62,
                "profile_image": "https://www.gravatar.com/avatar/fb60547b3150b54bf3b0210bf1e31314?s=128&d=identicon&r=PG&f=1",
                "display_name": "Pear",
                "link": "https://stackoverflow.com/users/5438045/pear"
            },
            "is_answered": true,
            "view_count": 200,
            "accepted_answer_id": 44187221,
            "answer_count": 2,
            "score": 0,
            "last_activity_date": 1523525592,
            "creation_date": 1495734348,
            "question_id": 44186719,
            "body_markdown": "I&#39;ve tried the following(pd is pandas):\r\n    \r\n\r\n     for i, chunk in pd.read_excel(os.path.join(INGEST_PATH,file), chunksize=5):\r\n\r\nbut I am getting this error:\r\n \r\n\r\n    NotImplementedError: chunksize keyword of read_excel is not implemented\r\n\r\nI&#39;ve tried searching for other methods but most of them are for CSV files, not xlsx, I also have pandas version 0.20.1 &lt;br&gt;\r\nAny help is appreciated. ",
            "link": "https://stackoverflow.com/questions/44186719/how-to-split-a-large-excel-file-using-pandas",
            "title": "How to split a large excel file using Pandas?",
            "body": "<p>I've tried the following(pd is pandas):</p>\n\n<pre><code> for i, chunk in pd.read_excel(os.path.join(INGEST_PATH,file), chunksize=5):\n</code></pre>\n\n<p>but I am getting this error:</p>\n\n<pre><code>NotImplementedError: chunksize keyword of read_excel is not implemented\n</code></pre>\n\n<p>I've tried searching for other methods but most of them are for CSV files, not xlsx, I also have pandas version 0.20.1 <br>\nAny help is appreciated. </p>\n"
        },
        {
            "tags": [
                "javascript",
                "jquery",
                "carousel",
                "bootstrap-carousel"
            ],
            "owner": {
                "reputation": 230,
                "user_id": 5364300,
                "user_type": "registered",
                "accept_rate": 78,
                "profile_image": "https://graph.facebook.com/1039212162808878/picture?type=large",
                "display_name": "Krystian Manthey",
                "link": "https://stackoverflow.com/users/5364300/krystian-manthey"
            },
            "is_answered": false,
            "view_count": 9,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523525589,
            "creation_date": 1523525589,
            "question_id": 49793015,
            "body_markdown": "I have 3 bootstrap carousels, which are all auto-sliding. If I click another slide, this is activated. But when its auto-sliding to another slide, this slide doesnt show as activated. What am I doing wrong here: https://bm-translations.de/km.php/#video\r\n\r\nmy code: \r\n\r\n    // handles the carousel buttons\r\n\t$(&#39;[id^=carousel-selector-]&#39;).click( function(){\r\n\t  var id_selector = $(this).attr(&quot;id&quot;);\r\n\t  var id = id_selector.substr(id_selector.length -1);\r\n\t  id = parseInt(id);\r\n\t  $(&#39;.carousel&#39;).carousel(id);\r\n\t  $(&#39;[id^=carousel-selector-]&#39;).removeClass(&#39;selected&#39;);\r\n\t  $(this).addClass(&#39;selected&#39;);\r\n\t});\r\n\r\n\t// when the carousel slides, auto update\r\n\t$(&#39;.carousel&#39;).on(&#39;slide.bs.carousel&#39;, function (e) {\r\n\t  var id = $(&#39;.item.active&#39;).data(&#39;slide-number&#39;);\r\n\t  id = parseInt(id)+1;\r\n\t  $(&#39;[id^=carousel-selector-]&#39;).removeClass(&#39;selected&#39;);\r\n\t  $(&#39;[id=carousel-selector-&#39;+id+&#39;]&#39;).addClass(&#39;selected&#39;);\r\n\t});\r\n\r\nOn auto slide its not adding the class selected. Whats wrong with the above code?",
            "link": "https://stackoverflow.com/questions/49793015/update-active-element-for-auto-slide-bootstrap-carousels",
            "title": "update active element for auto slide bootstrap carousels",
            "body": "<p>I have 3 bootstrap carousels, which are all auto-sliding. If I click another slide, this is activated. But when its auto-sliding to another slide, this slide doesnt show as activated. What am I doing wrong here: <a href=\"https://bm-translations.de/km.php/#video\" rel=\"nofollow noreferrer\">https://bm-translations.de/km.php/#video</a></p>\n\n<p>my code: </p>\n\n<pre><code>// handles the carousel buttons\n$('[id^=carousel-selector-]').click( function(){\n  var id_selector = $(this).attr(\"id\");\n  var id = id_selector.substr(id_selector.length -1);\n  id = parseInt(id);\n  $('.carousel').carousel(id);\n  $('[id^=carousel-selector-]').removeClass('selected');\n  $(this).addClass('selected');\n});\n\n// when the carousel slides, auto update\n$('.carousel').on('slide.bs.carousel', function (e) {\n  var id = $('.item.active').data('slide-number');\n  id = parseInt(id)+1;\n  $('[id^=carousel-selector-]').removeClass('selected');\n  $('[id=carousel-selector-'+id+']').addClass('selected');\n});\n</code></pre>\n\n<p>On auto slide its not adding the class selected. Whats wrong with the above code?</p>\n"
        },
        {
            "tags": [
                "react-native",
                "expo"
            ],
            "owner": {
                "reputation": 16,
                "user_id": 8130623,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-QYkhOd15b78/AAAAAAAAAAI/AAAAAAAAAOE/PW2TeGJVZ_I/photo.jpg?sz=128",
                "display_name": "Denis",
                "link": "https://stackoverflow.com/users/8130623/denis"
            },
            "is_answered": true,
            "view_count": 232,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1523525579,
            "creation_date": 1511713814,
            "last_edit_date": 1511718389,
            "question_id": 47498391,
            "body_markdown": "I run command exp build:android.\r\nAfter some minutes it displays:\r\n\r\n    [exp] Building...\r\n    [exp] Build started, it may take a few minutes to complete.\r\n    [exp] Build ID: (some id here)\r\n    [exp] Run `exp build:status` to monitor it.\r\n\r\nWhen I run command &quot;exp build:status&quot;, I see this error:\r\n\r\n    |[exp] Error: Can&#39;t find package.json\r\n    /[exp] There is an error with your project. See above logs for information.\r\n\r\n\r\nDo you have any idea what it could be. Package.json does exist in the project.\r\n\r\nUpdate: exp is called at the project root (package.json is in same folder).\r\nMy project was created with &quot;Create-react-native-app&quot;.\r\nexp version: 46.0.3\r\nnodejs version v6.11.2\r\nnpm version 3.10.10",
            "link": "https://stackoverflow.com/questions/47498391/command-exp-buildandroid-error-cant-find-package-json",
            "title": "command &quot;exp build:android&quot;, error &quot;Can&#39;t find package.json&quot;",
            "body": "<p>I run command exp build:android.\nAfter some minutes it displays:</p>\n\n<pre><code>[exp] Building...\n[exp] Build started, it may take a few minutes to complete.\n[exp] Build ID: (some id here)\n[exp] Run `exp build:status` to monitor it.\n</code></pre>\n\n<p>When I run command \"exp build:status\", I see this error:</p>\n\n<pre><code>|[exp] Error: Can't find package.json\n/[exp] There is an error with your project. See above logs for information.\n</code></pre>\n\n<p>Do you have any idea what it could be. Package.json does exist in the project.</p>\n\n<p>Update: exp is called at the project root (package.json is in same folder).\nMy project was created with \"Create-react-native-app\".\nexp version: 46.0.3\nnodejs version v6.11.2\nnpm version 3.10.10</p>\n"
        },
        {
            "tags": [
                "ajax",
                "spring-boot",
                "https",
                "ssl-certificate"
            ],
            "owner": {
                "reputation": 13,
                "user_id": 4218574,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/e7d387fc5655aa50f2c6944c2b4b063a?s=128&d=identicon&r=PG&f=1",
                "display_name": "jyoti doddagoudar",
                "link": "https://stackoverflow.com/users/4218574/jyoti-doddagoudar"
            },
            "is_answered": false,
            "view_count": 30,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523525567,
            "creation_date": 1523525135,
            "last_edit_date": 1523525567,
            "question_id": 49792862,
            "body_markdown": "I have created one self SSL-Certificate of format i,e demo.ssl.cer.p7b.\r\nAdded this certificate in JDK and web browser under trusted certificate storage.\r\n\r\nWhen the application was working on HTTP,  I used to receive the request payload in JSON format which was correct.\r\n\r\nNow after enabling HTTPS on server side,the request payload format has been changed and it is not in JSON.\r\n\r\nNot able to understand the data and also not sure how the message is changed and delivered to the controller. Can anyone please suggest more information on understanding the same.\r\n\r\nNote : Posting through AJAX.\r\n\r\nRequest payload\r\n\r\n   \r\n\r\n    \r\n\r\n    {\r\n              &quot;producerAPI&quot;: {\r\n                &quot;host&quot;: &quot;xx.xx.xxx.xxx&quot;,\r\n                &quot;port&quot;: &quot;84xx&quot;,\r\n                &quot;context&quot;: &quot;iProducer/getDataDB&quot;\r\n              },\r\n              &quot;consumerAPI&quot;: {\r\n                &quot;host&quot;: &quot;xx.xx.xxx.xxx&quot;,\r\n                &quot;port&quot;: &quot;84xx&quot;,\r\n                &quot;context&quot;: &quot;ConsumerAPI/consumerAPIXml&quot;\r\n              },\r\n             &quot;dataMapper&quot;:{\r\n                            &quot;inputFormat&quot;:&quot;json&quot;,\r\n                            &quot;outputFormat&quot;:&quot;xml&quot;\r\n                }\r\n            \r\n            }\r\n\r\nOnce request reaches controller, we are getting below mentioned format\r\n    \r\n    producerAPI%5Bhost%5D=localhost&amp;producerAPI%5Bport%5D=84XX&amp;producerAPI%5Bcontext%5D=iPaaSProducer%2FgetDataFromDB&amp;consumerAPI%5Bhost%5D=xx.xx.xxx.xxx&amp;consumerAPI%5Bport%5D=80XX&amp;consumerAPI%5Bcontext%5D=ConsumerAPI%2FconsumerAPIXml&amp;dataMapper%5BinputFormat%5D=json&amp;dataMapper%5BoutputFormat%5D=xml\r\n\r\n \r\n\r\n\r\n",
            "link": "https://stackoverflow.com/questions/49792862/request-payload-changed-from-json-to-other-format-on-ajax-call-over-https",
            "title": "Request Payload Changed from JSON to other format on ajax call over HTTPS",
            "body": "<p>I have created one self SSL-Certificate of format i,e demo.ssl.cer.p7b.\nAdded this certificate in JDK and web browser under trusted certificate storage.</p>\n\n<p>When the application was working on HTTP,  I used to receive the request payload in JSON format which was correct.</p>\n\n<p>Now after enabling HTTPS on server side,the request payload format has been changed and it is not in JSON.</p>\n\n<p>Not able to understand the data and also not sure how the message is changed and delivered to the controller. Can anyone please suggest more information on understanding the same.</p>\n\n<p>Note : Posting through AJAX.</p>\n\n<p>Request payload</p>\n\n<pre><code>{\n          \"producerAPI\": {\n            \"host\": \"xx.xx.xxx.xxx\",\n            \"port\": \"84xx\",\n            \"context\": \"iProducer/getDataDB\"\n          },\n          \"consumerAPI\": {\n            \"host\": \"xx.xx.xxx.xxx\",\n            \"port\": \"84xx\",\n            \"context\": \"ConsumerAPI/consumerAPIXml\"\n          },\n         \"dataMapper\":{\n                        \"inputFormat\":\"json\",\n                        \"outputFormat\":\"xml\"\n            }\n\n        }\n</code></pre>\n\n<p>Once request reaches controller, we are getting below mentioned format</p>\n\n<pre><code>producerAPI%5Bhost%5D=localhost&amp;producerAPI%5Bport%5D=84XX&amp;producerAPI%5Bcontext%5D=iPaaSProducer%2FgetDataFromDB&amp;consumerAPI%5Bhost%5D=xx.xx.xxx.xxx&amp;consumerAPI%5Bport%5D=80XX&amp;consumerAPI%5Bcontext%5D=ConsumerAPI%2FconsumerAPIXml&amp;dataMapper%5BinputFormat%5D=json&amp;dataMapper%5BoutputFormat%5D=xml\n</code></pre>\n"
        },
        {
            "tags": [
                "scala",
                "gatling"
            ],
            "owner": {
                "reputation": 949,
                "user_id": 3296266,
                "user_type": "registered",
                "accept_rate": 92,
                "profile_image": "https://www.gravatar.com/avatar/0f1bc86f9813f0afebd0a1d3be911f20?s=128&d=identicon&r=PG&f=1",
                "display_name": "Stanko",
                "link": "https://stackoverflow.com/users/3296266/stanko"
            },
            "is_answered": false,
            "view_count": 15,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523525566,
            "creation_date": 1523524912,
            "last_edit_date": 1523525566,
            "question_id": 49792767,
            "body_markdown": "I&#39;m having issues with loading a JSON file in Gatling. It works with an absolute path but not with a relative. Where should JSON files be stored? I&#39;ve tried `/home/dev/gatling-charts-highcharts-bundle-2.3.0/user-files/data` but the file could not be found.\r\n\r\nPiece of my code:\r\n\r\n    def addCredential(status_code: Option[Seq[Int]], username: Option[String]) = {\r\n    \t\tfeed(random_user)\r\n    \t\t\t.exec(http(&quot;[POST] /users/[user]/credentials&quot;)\r\n    \t\t\t.post(&quot;/users/%s/credentials&quot;.format(username getOrElse &quot;${username}&quot;))\r\n    \t\t\t.body(RawFileBody(&quot;credential.json&quot;)).asJSON\r\n    \t\t\t.check(status.in(202, 404, 409)))\r\n    \t}\r\n\r\n\r\nThe file `credential.json` can be found if I give the absolute path but this is not optimal because several people use the simulations.",
            "link": "https://stackoverflow.com/questions/49792767/gatling-where-to-place-json-files",
            "title": "Gatling where to place JSON files?",
            "body": "<p>I'm having issues with loading a JSON file in Gatling. It works with an absolute path but not with a relative. Where should JSON files be stored? I've tried <code>/home/dev/gatling-charts-highcharts-bundle-2.3.0/user-files/data</code> but the file could not be found.</p>\n\n<p>Piece of my code:</p>\n\n<pre><code>def addCredential(status_code: Option[Seq[Int]], username: Option[String]) = {\n        feed(random_user)\n            .exec(http(\"[POST] /users/[user]/credentials\")\n            .post(\"/users/%s/credentials\".format(username getOrElse \"${username}\"))\n            .body(RawFileBody(\"credential.json\")).asJSON\n            .check(status.in(202, 404, 409)))\n    }\n</code></pre>\n\n<p>The file <code>credential.json</code> can be found if I give the absolute path but this is not optimal because several people use the simulations.</p>\n"
        },
        {
            "tags": [
                "javascript",
                "arrays",
                "mongodb",
                "meteor"
            ],
            "owner": {
                "reputation": 8,
                "user_id": 9629367,
                "user_type": "registered",
                "profile_image": "https://graph.facebook.com/2121672264722623/picture?type=large",
                "display_name": "Jo&#235;l Rimaz",
                "link": "https://stackoverflow.com/users/9629367/jo%c3%abl-rimaz"
            },
            "is_answered": true,
            "view_count": 46,
            "accepted_answer_id": 49771522,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1523525563,
            "creation_date": 1523437124,
            "last_edit_date": 1523439877,
            "question_id": 49770795,
            "body_markdown": "So I&#39;m coming from a pure javascript background and I&#39;m working on a meteor project on which I have the following MongoDB collection, named &quot;Semaines&quot;:\r\n\r\n    {\r\n        _id: /*random ID*/,\r\n        id_utilisateur: /*user ID*/,\r\n        isCreated: true,\r\n        jours: {\r\n        \tlundi: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\r\n        \tmardi: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\r\n        \tmercredi: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\r\n        \tjeudi: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\r\n        \tvendredi: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\r\n        \tsamedi: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\r\n        \tdimanche: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\r\n        }\r\n    }\r\nUsing the following method, I want to update the element at the specified day and index (here representing hours):\r\n\r\n        //sent values are : idUt = Meteor.userId(), day = &quot;vendredi&quot;, hour = 0, score = 10\r\n        &#39;semaines.updateTable&#39;(idUt, day, hour, score){\r\n  \t        check(idUt, String);\r\n            check(day, String);\r\n  \t        check(hour, Number);\r\n  \t        check(score, Number);\r\n  \t        Semaines.update({id_utilisateur: idUt},{$set : {&quot;jours.$[day].$[hour]&quot;: score}});\r\n        }\r\nThis doesn&#39;t seem to work. I think it&#39;s because the query actually is, with the values I&#39;m sending:\r\n\r\n        $set : {jours.vendredi.0 : 10}\r\nThis obviously will not work, but I fail to see how my query can become this, which I assume would work:\r\n\r\n        $set : {jours.vendredi[0] : 10}",
            "link": "https://stackoverflow.com/questions/49770795/meteor-update-an-element-in-a-specified-array-index-in-mongodb",
            "title": "Meteor : Update an element in a specified array index in MongoDB",
            "body": "<p>So I'm coming from a pure javascript background and I'm working on a meteor project on which I have the following MongoDB collection, named \"Semaines\":</p>\n\n<pre><code>{\n    _id: /*random ID*/,\n    id_utilisateur: /*user ID*/,\n    isCreated: true,\n    jours: {\n        lundi: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n        mardi: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n        mercredi: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n        jeudi: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n        vendredi: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n        samedi: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n        dimanche: [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n    }\n}\n</code></pre>\n\n<p>Using the following method, I want to update the element at the specified day and index (here representing hours):</p>\n\n<pre><code>    //sent values are : idUt = Meteor.userId(), day = \"vendredi\", hour = 0, score = 10\n    'semaines.updateTable'(idUt, day, hour, score){\n        check(idUt, String);\n        check(day, String);\n        check(hour, Number);\n        check(score, Number);\n        Semaines.update({id_utilisateur: idUt},{$set : {\"jours.$[day].$[hour]\": score}});\n    }\n</code></pre>\n\n<p>This doesn't seem to work. I think it's because the query actually is, with the values I'm sending:</p>\n\n<pre><code>    $set : {jours.vendredi.0 : 10}\n</code></pre>\n\n<p>This obviously will not work, but I fail to see how my query can become this, which I assume would work:</p>\n\n<pre><code>    $set : {jours.vendredi[0] : 10}\n</code></pre>\n"
        },
        {
            "tags": [
                "scala",
                "arrays",
                "equals",
                "equality"
            ],
            "owner": {
                "reputation": 5747,
                "user_id": 363258,
                "user_type": "registered",
                "accept_rate": 100,
                "profile_image": "https://www.gravatar.com/avatar/6d8594f35e881e0b804da5c9f5d875ae?s=128&d=identicon&r=PG",
                "display_name": "olle kullberg",
                "link": "https://stackoverflow.com/users/363258/olle-kullberg"
            },
            "is_answered": true,
            "view_count": 1180,
            "accepted_answer_id": 3213808,
            "answer_count": 2,
            "score": 10,
            "last_activity_date": 1523525559,
            "creation_date": 1278684436,
            "last_edit_date": 1523525559,
            "question_id": 3213368,
            "body_markdown": "    scala&gt; List(1,2,3) == List(1,2,3)\r\n    \r\n    res2: Boolean = true\r\n    \r\n    scala&gt; Map(1 -&gt; &quot;Olle&quot;) == Map(1 -&gt; &quot;Olle&quot;)\r\n    \r\n    res3: Boolean = true\r\n\r\nBut when trying to do the same with Array, it does not work the same. Why?\r\n\r\n    scala&gt; Array(&#39;a&#39;,&#39;b&#39;) == Array(&#39;a&#39;,&#39;b&#39;)\r\n    \r\n    res4: Boolean = false\r\n\r\nI have used 2.8.0.RC7 and 2.8.0.Beta1-prerelease.\r\n\r\n\r\n",
            "link": "https://stackoverflow.com/questions/3213368/strange-behaviour-of-the-array-type-with-operator",
            "title": "Strange behaviour of the Array type with `==` operator",
            "body": "<pre><code>scala&gt; List(1,2,3) == List(1,2,3)\n\nres2: Boolean = true\n\nscala&gt; Map(1 -&gt; \"Olle\") == Map(1 -&gt; \"Olle\")\n\nres3: Boolean = true\n</code></pre>\n\n<p>But when trying to do the same with Array, it does not work the same. Why?</p>\n\n<pre><code>scala&gt; Array('a','b') == Array('a','b')\n\nres4: Boolean = false\n</code></pre>\n\n<p>I have used 2.8.0.RC7 and 2.8.0.Beta1-prerelease.</p>\n"
        },
        {
            "tags": [
                "python",
                "arrays",
                "numpy"
            ],
            "owner": {
                "reputation": 5822,
                "user_id": 1082349,
                "user_type": "registered",
                "accept_rate": 53,
                "profile_image": "https://www.gravatar.com/avatar/ffbba800a06c2ba0908a2a1467841c73?s=128&d=identicon&r=PG",
                "display_name": "FooBar",
                "link": "https://stackoverflow.com/users/1082349/foobar"
            },
            "is_answered": true,
            "view_count": 43,
            "answer_count": 4,
            "score": 2,
            "last_activity_date": 1523525555,
            "creation_date": 1523520374,
            "question_id": 49791146,
            "body_markdown": "Let&#39;s say I have\r\n\r\n    arr = np.arange(6)\r\n    arr\r\n    array([0, 1, 2, 3, 4, 5])\r\n\r\nand I decide that I want to treat an array &quot;like a circle&quot;: When I run out of material at the end, I want to start at index 0 again. That is, I want a convenient way of selecting `x` elements, starting at index `i`.\r\n\r\nNow, if `x == 6`, I can simply do\r\n\r\n    i = 3\r\n    np.hstack((arr[i:], arr[:i]))\r\n    Out[9]: array([3, 4, 5, 0, 1, 2])\r\n\r\nBut is there a convenient way of generally doing this, even if `x &gt; 6`, without having to manually breaking the array apart and thinking through the logic?\r\n\r\nFor example:\r\n\r\n    print(roll_array_arround(arr)[2:17])\r\n\r\nshould return.\r\n\r\n    array([2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0])",
            "link": "https://stackoverflow.com/questions/49791146/rolling-array-around",
            "title": "Rolling array around",
            "body": "<p>Let's say I have</p>\n\n<pre><code>arr = np.arange(6)\narr\narray([0, 1, 2, 3, 4, 5])\n</code></pre>\n\n<p>and I decide that I want to treat an array \"like a circle\": When I run out of material at the end, I want to start at index 0 again. That is, I want a convenient way of selecting <code>x</code> elements, starting at index <code>i</code>.</p>\n\n<p>Now, if <code>x == 6</code>, I can simply do</p>\n\n<pre><code>i = 3\nnp.hstack((arr[i:], arr[:i]))\nOut[9]: array([3, 4, 5, 0, 1, 2])\n</code></pre>\n\n<p>But is there a convenient way of generally doing this, even if <code>x &gt; 6</code>, without having to manually breaking the array apart and thinking through the logic?</p>\n\n<p>For example:</p>\n\n<pre><code>print(roll_array_arround(arr)[2:17])\n</code></pre>\n\n<p>should return.</p>\n\n<pre><code>array([2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0])\n</code></pre>\n"
        },
        {
            "tags": [
                "c++",
                "c++11",
                "lambda"
            ],
            "owner": {
                "reputation": 636,
                "user_id": 7699037,
                "user_type": "registered",
                "accept_rate": 100,
                "profile_image": "https://www.gravatar.com/avatar/ae22aa9776941949ddf401f3eb7fc318?s=128&d=identicon&r=PG&f=1",
                "display_name": "Mike van Dyke",
                "link": "https://stackoverflow.com/users/7699037/mike-van-dyke"
            },
            "is_answered": true,
            "view_count": 68,
            "accepted_answer_id": 49791758,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1523525552,
            "creation_date": 1523520726,
            "last_edit_date": 1523522066,
            "question_id": 49791260,
            "body_markdown": "I have the following class which contains a lambda member variable:\r\n\r\n    template &lt;typename Callable&gt;\r\n    class task {\r\n      private:\r\n        Callable lambda;\r\n    \r\n      public:\r\n        task(Callable l) : lambda(l) {}\r\n\r\n        void execute() {\r\n            lambda();\r\n        }\r\n    };\r\n\r\nNow I want to create a function which accepts a object of any class and a member function pointer of that class, then creates lambda, creates a task from that lambda and finally returns the task. But I can&#39;t figure out the return type of the function:\r\n\r\n    template &lt;typename C, typename F, typename ...Args&gt;\r\n    /* return type ?*/ create_task(C&amp; obj, F func, Args... args) {\r\n        auto l = [&amp;obj, func, args...] {\r\n            (obj.*func)(args...);\r\n        };\r\n    \r\n        task&lt;decltype(l)&gt; t {l};\r\n    \r\n        return t;\r\n    }\r\n\r\nHow can this be done in `C++11`? I&#39;m also open for other suggestions, BUT they&#39;ll have to do without dynamic memory allocation.",
            "link": "https://stackoverflow.com/questions/49791260/creating-and-returning-a-class-containing-lambda-member-variable-in-c11",
            "title": "Creating and returning a class containing lambda member variable in C++11",
            "body": "<p>I have the following class which contains a lambda member variable:</p>\n\n<pre><code>template &lt;typename Callable&gt;\nclass task {\n  private:\n    Callable lambda;\n\n  public:\n    task(Callable l) : lambda(l) {}\n\n    void execute() {\n        lambda();\n    }\n};\n</code></pre>\n\n<p>Now I want to create a function which accepts a object of any class and a member function pointer of that class, then creates lambda, creates a task from that lambda and finally returns the task. But I can't figure out the return type of the function:</p>\n\n<pre><code>template &lt;typename C, typename F, typename ...Args&gt;\n/* return type ?*/ create_task(C&amp; obj, F func, Args... args) {\n    auto l = [&amp;obj, func, args...] {\n        (obj.*func)(args...);\n    };\n\n    task&lt;decltype(l)&gt; t {l};\n\n    return t;\n}\n</code></pre>\n\n<p>How can this be done in <code>C++11</code>? I'm also open for other suggestions, BUT they'll have to do without dynamic memory allocation.</p>\n"
        },
        {
            "tags": [
                "window",
                "settings"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9635236,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/d31fa448fe5742787fe5b7fa1ad25d39?s=128&d=identicon&r=PG&f=1",
                "display_name": "Hemant",
                "link": "https://stackoverflow.com/users/9635236/hemant"
            },
            "is_answered": false,
            "view_count": 9,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523525538,
            "creation_date": 1523525538,
            "question_id": 49793004,
            "body_markdown": "[enter image description here][1]\r\n\r\nI want my number seperator should look like following.\r\nBut in my windows 10 systerm I am getting number seperatory as per following.\r\n[enter image description here][2]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/0e4tt.png\r\n\r\n  [2]: https://i.stack.imgur.com/sJn9y.png\r\n\r\n\r\nKindly assist me how to change the number format as per following.\r\n1234,56,789.00\r\n",
            "link": "https://stackoverflow.com/questions/49793004/need-assistance-to-change-number-separator-in-windows-10",
            "title": "Need assistance to change number separator in windows 10",
            "body": "<p><a href=\"https://i.stack.imgur.com/0e4tt.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n\n<p>I want my number seperator should look like following.\nBut in my windows 10 systerm I am getting number seperatory as per following.\n<a href=\"https://i.stack.imgur.com/sJn9y.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n\n<p>Kindly assist me how to change the number format as per following.\n1234,56,789.00</p>\n"
        },
        {
            "tags": [
                "php",
                "laravel-5",
                "utf-8"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 2823235,
                "user_type": "registered",
                "accept_rate": 25,
                "profile_image": "https://www.gravatar.com/avatar/cb5b828cf08a9ddd2c1b150edf036b70?s=128&d=identicon&r=PG&f=1",
                "display_name": "user2823235",
                "link": "https://stackoverflow.com/users/2823235/user2823235"
            },
            "is_answered": false,
            "view_count": 18,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523525528,
            "creation_date": 1523524832,
            "last_edit_date": 1523525528,
            "question_id": 49792741,
            "body_markdown": "I&#39;ve the following setup for laravel :\r\n\r\n - PHP versions : `7.2*`\r\n - Laravel : `5.5`\r\n\r\nthis is the error :\r\n\r\n&gt;&quot;message&quot;: &quot;Malformed UTF-8 characters, possibly incorrectly encoded&quot;,\r\n&quot;exception&quot;: &quot;InvalidArgumentException&quot;,\r\n&quot;file&quot;:&quot;/var/www/html/XXXXXX/vendor/laravel/framework/src/Illuminate/Http/JsonResponse.php&quot;, &quot;line&quot;:75\r\n\r\nHow to solve this ?",
            "link": "https://stackoverflow.com/questions/49792741/malformed-utf-8-characters-possibly-incorrectly-encoded-in-laravel",
            "title": "Malformed UTF-8 characters, possibly incorrectly encoded in Laravel",
            "body": "<p>I've the following setup for laravel :</p>\n\n<ul>\n<li>PHP versions : <code>7.2*</code></li>\n<li>Laravel : <code>5.5</code></li>\n</ul>\n\n<p>this is the error :</p>\n\n<blockquote>\n  <p>\"message\": \"Malformed UTF-8 characters, possibly incorrectly encoded\",\n  \"exception\": \"InvalidArgumentException\",\n  \"file\":\"/var/www/html/XXXXXX/vendor/laravel/framework/src/Illuminate/Http/JsonResponse.php\", \"line\":75</p>\n</blockquote>\n\n<p>How to solve this ?</p>\n"
        },
        {
            "tags": [
                "apache-spark",
                "apache-zeppelin"
            ],
            "owner": {
                "reputation": 702,
                "user_id": 8110607,
                "user_type": "registered",
                "accept_rate": 67,
                "profile_image": "https://i.stack.imgur.com/14rAB.png?s=128&g=1",
                "display_name": "Soheil Pourbafrani",
                "link": "https://stackoverflow.com/users/8110607/soheil-pourbafrani"
            },
            "is_answered": false,
            "view_count": 11,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523525527,
            "creation_date": 1523524963,
            "last_edit_date": 1523525527,
            "question_id": 49792787,
            "body_markdown": "Trying to run Spark `Zeppelin` on `Yarn` cluster, I set the following variables in `zeppeline-env.sh`:\r\n\r\n    export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop\r\n    export SPARK_HOME=/opt/spark\r\n    export SPARK_SUBMIT_OPTIONS=&quot;--executor-memory 512m --driver-memory 512m --executor-cores 2 --queue onlineQ&quot;\r\n\r\nBut it didn&#39;t use the `SPARK_SUBMIT_OPTIONS` values in deploying job on YARN! For example `executor-memory` is `1024m` as default. When I set `executor-memory` in the Zeppelin `interpreter` setting, it will be applied. As I should set `--queue` property for Spark Submit, is there any other way (than SPARK_SUBMIT_OPTIONS variable) to set it?",
            "link": "https://stackoverflow.com/questions/49792787/spark-submit-options-didnt-work-in-apache-zeppelin-0-7-3",
            "title": "SPARK_SUBMIT_OPTIONS didn&#39;t work in Apache Zeppelin 0.7.3",
            "body": "<p>Trying to run Spark <code>Zeppelin</code> on <code>Yarn</code> cluster, I set the following variables in <code>zeppeline-env.sh</code>:</p>\n\n<pre><code>export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop\nexport SPARK_HOME=/opt/spark\nexport SPARK_SUBMIT_OPTIONS=\"--executor-memory 512m --driver-memory 512m --executor-cores 2 --queue onlineQ\"\n</code></pre>\n\n<p>But it didn't use the <code>SPARK_SUBMIT_OPTIONS</code> values in deploying job on YARN! For example <code>executor-memory</code> is <code>1024m</code> as default. When I set <code>executor-memory</code> in the Zeppelin <code>interpreter</code> setting, it will be applied. As I should set <code>--queue</code> property for Spark Submit, is there any other way (than SPARK_SUBMIT_OPTIONS variable) to set it?</p>\n"
        },
        {
            "tags": [
                "openlayers-3"
            ],
            "owner": {
                "reputation": 10476,
                "user_id": 219187,
                "user_type": "registered",
                "accept_rate": 67,
                "profile_image": "https://www.gravatar.com/avatar/beb1dab6841ba29ff5bc56fafeb7cf37?s=128&d=identicon&r=PG",
                "display_name": "theDmi",
                "link": "https://stackoverflow.com/users/219187/thedmi"
            },
            "is_answered": true,
            "view_count": 567,
            "accepted_answer_id": 34491560,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1523525523,
            "creation_date": 1449557925,
            "question_id": 34149816,
            "body_markdown": "I have a OL3 map with one tile layer and one vector layer. Since the features on the vector layer don&#39;t stand out enough against the tile layer in the background, I want to desaturate the tile layer.\r\n\r\nI&#39;m aware of [the Hue/Saturation Example](http://openlayers.org/en/v3.9.0/examples/hue-saturation.html), but this approach works only with WebGL. WebGL in turn does not support vector layers.\r\n\r\n**How can I desaturate an OpenLayers 3 tile layer when using the canvas renderer?**\r\n\r\nNOTE: I cannot desaturate the tiles on the server, because I don&#39;t control the server that hosts the tiles.",
            "link": "https://stackoverflow.com/questions/34149816/desaturate-tile-layer-in-openlayers-3",
            "title": "Desaturate tile layer in OpenLayers 3",
            "body": "<p>I have a OL3 map with one tile layer and one vector layer. Since the features on the vector layer don't stand out enough against the tile layer in the background, I want to desaturate the tile layer.</p>\n\n<p>I'm aware of <a href=\"http://openlayers.org/en/v3.9.0/examples/hue-saturation.html\" rel=\"nofollow\">the Hue/Saturation Example</a>, but this approach works only with WebGL. WebGL in turn does not support vector layers.</p>\n\n<p><strong>How can I desaturate an OpenLayers 3 tile layer when using the canvas renderer?</strong></p>\n\n<p>NOTE: I cannot desaturate the tiles on the server, because I don't control the server that hosts the tiles.</p>\n"
        },
        {
            "tags": [
                "node.js",
                "realm",
                "realm-object-server"
            ],
            "owner": {
                "reputation": 278,
                "user_id": 4082459,
                "user_type": "registered",
                "accept_rate": 50,
                "profile_image": "https://www.gravatar.com/avatar/b694bab8b6f12b036b5fa62d08eaebff?s=128&d=identicon&r=PG&f=1",
                "display_name": "Kristian Fox",
                "link": "https://stackoverflow.com/users/4082459/kristian-fox"
            },
            "is_answered": false,
            "view_count": 82,
            "answer_count": 1,
            "score": 3,
            "last_activity_date": 1523525519,
            "creation_date": 1521808131,
            "last_edit_date": 1522927873,
            "question_id": 49449693,
            "body_markdown": "I&#39;ve installed Realm Object Server using the docker container method on a VM on the google cloud platform. The container is running and I am able to connect in a browser and see the ROS page. I am able to connect to it using Realm Studio and add a user.\r\n\r\nI have a nodeJS app running locally on a Mac and I&#39;m trying to use that to sign in and write to realm on the server. When I run the app I get an error and the user returned is an empty object. Not sure what I&#39;m doing wrong.\r\nI&#39;m new to NodeJS.\r\n\r\nCode:\r\n\r\n            var theRealm;\r\n        const serverUrl = &quot;http://xx.xx.xx.xx:9080&quot;;\r\n        const username = &quot;xxxx&quot;;\r\n        const password = &quot;xxxx&quot;;\r\n\r\n        var token = &quot;long-token-for-enterprise-trial&quot;;\r\n        Realm.Sync.setFeatureToken(token);\r\n\r\n        console.log(&quot;Will log in user&quot;);\r\n        Realm.Sync.User.login(serverUrl, username, password)\r\n            .then(user =&gt; {\r\n                                                               ``\r\n            // user is logged in\r\n            console.log(&quot;user is logged in &quot; + util.inspect(user));\r\n                // do stuff ...\r\n                console.log(&quot;Will create config&quot;);\r\n                const config = {\r\n                    schema:[\r\n                        schema.interventionSchema,\r\n                        schema.reportSchema\r\n                    ],\r\n                    sync: {\r\n                        user: user,\r\n                        url: serverUrl\r\n                    }\r\n                };\r\n                console.log(&quot;Will open realm with config: &quot; + config);\r\n                const realm = Realm.open(config)\r\n                    .then(realm =&gt; {\r\n                        // use the realm instance here\r\n                        console.log(&quot;Realm is active &quot; + realm);\r\n                        console.log(&quot;Will create Realm&quot;);\r\n                        theRealm = new Realm({\r\n                            path:&#39;model/realm_db/theRealm.realm&#39;,\r\n                            schema:[\r\n                                schema.interventionSchema,\r\n                                schema.reportSchema\r\n                            ]\r\n                        });\r\n                        console.log(&quot;Did create Realm: &quot; + theRealm);\r\n                    })\r\n                    .catch(error =&gt; {\r\n                        // Handle the error here if something went wrong\r\n                        console.log(&quot;Error when opening Realm: &quot; + error);\r\n                    });\r\n                })\r\n            .catch(error =&gt; {\r\n                // an auth error has occurred\r\n                console.log(&quot;Error when logging in user: &quot; + error);\r\n            });\r\n\r\n\r\nOutput:\r\n\r\n    Will log in user\r\n    Server is running...\r\n    user is logged in {}\r\n    Will create config\r\n    Will open realm with config: [object Object]\r\n    TypeError: Cannot read property &#39;token_data&#39; of undefined\r\n        at performFetch.then.then (/pathToProject/node_modules/realm/lib/user-methods.js:203:49)\r\n        at &lt;anonymous&gt;\r\n        at process._tickCallback (internal/process/next_tick.js:188:7)\r\n    TypeError: Cannot read property &#39;token_data&#39; of undefined\r\n        at performFetch.then.then (/pathToProject/node_modules/realm/lib/user-methods.js:203:49)\r\n        at &lt;anonymous&gt;\r\n        at process._tickCallback (internal/process/next_tick.js:188:7)\r\n\r\n\r\nError @ user-methods.js:203:49\r\n\r\n    const tokenData = json.access_token.token_data;\r\n\r\n\r\njson is:\r\n{ user_token: \r\n   { token: &#39;xxxxxxxx&#39;,\r\n     token_data: \r\n      { app_id: &#39;io.realm.Auth&#39;,\r\n        identity: &#39;xxxxxxx&#39;,\r\n        salt: &#39;xxxxxxxx&#39;,\r\n        expires: 1522930743,\r\n        is_admin: false } } }; \r\n\r\nSo json.access_token.token_data is undefined but json. user_token.token_data would not be.",
            "link": "https://stackoverflow.com/questions/49449693/unable-to-connect-to-realm-object-server-using-nodejs",
            "title": "Unable to connect to Realm Object Server using NodeJs",
            "body": "<p>I've installed Realm Object Server using the docker container method on a VM on the google cloud platform. The container is running and I am able to connect in a browser and see the ROS page. I am able to connect to it using Realm Studio and add a user.</p>\n\n<p>I have a nodeJS app running locally on a Mac and I'm trying to use that to sign in and write to realm on the server. When I run the app I get an error and the user returned is an empty object. Not sure what I'm doing wrong.\nI'm new to NodeJS.</p>\n\n<p>Code:</p>\n\n<pre><code>        var theRealm;\n    const serverUrl = \"http://xx.xx.xx.xx:9080\";\n    const username = \"xxxx\";\n    const password = \"xxxx\";\n\n    var token = \"long-token-for-enterprise-trial\";\n    Realm.Sync.setFeatureToken(token);\n\n    console.log(\"Will log in user\");\n    Realm.Sync.User.login(serverUrl, username, password)\n        .then(user =&gt; {\n                                                           ``\n        // user is logged in\n        console.log(\"user is logged in \" + util.inspect(user));\n            // do stuff ...\n            console.log(\"Will create config\");\n            const config = {\n                schema:[\n                    schema.interventionSchema,\n                    schema.reportSchema\n                ],\n                sync: {\n                    user: user,\n                    url: serverUrl\n                }\n            };\n            console.log(\"Will open realm with config: \" + config);\n            const realm = Realm.open(config)\n                .then(realm =&gt; {\n                    // use the realm instance here\n                    console.log(\"Realm is active \" + realm);\n                    console.log(\"Will create Realm\");\n                    theRealm = new Realm({\n                        path:'model/realm_db/theRealm.realm',\n                        schema:[\n                            schema.interventionSchema,\n                            schema.reportSchema\n                        ]\n                    });\n                    console.log(\"Did create Realm: \" + theRealm);\n                })\n                .catch(error =&gt; {\n                    // Handle the error here if something went wrong\n                    console.log(\"Error when opening Realm: \" + error);\n                });\n            })\n        .catch(error =&gt; {\n            // an auth error has occurred\n            console.log(\"Error when logging in user: \" + error);\n        });\n</code></pre>\n\n<p>Output:</p>\n\n<pre><code>Will log in user\nServer is running...\nuser is logged in {}\nWill create config\nWill open realm with config: [object Object]\nTypeError: Cannot read property 'token_data' of undefined\n    at performFetch.then.then (/pathToProject/node_modules/realm/lib/user-methods.js:203:49)\n    at &lt;anonymous&gt;\n    at process._tickCallback (internal/process/next_tick.js:188:7)\nTypeError: Cannot read property 'token_data' of undefined\n    at performFetch.then.then (/pathToProject/node_modules/realm/lib/user-methods.js:203:49)\n    at &lt;anonymous&gt;\n    at process._tickCallback (internal/process/next_tick.js:188:7)\n</code></pre>\n\n<p>Error @ user-methods.js:203:49</p>\n\n<pre><code>const tokenData = json.access_token.token_data;\n</code></pre>\n\n<p>json is:\n{ user_token: \n   { token: 'xxxxxxxx',\n     token_data: \n      { app_id: 'io.realm.Auth',\n        identity: 'xxxxxxx',\n        salt: 'xxxxxxxx',\n        expires: 1522930743,\n        is_admin: false } } }; </p>\n\n<p>So json.access_token.token_data is undefined but json. user_token.token_data would not be.</p>\n"
        },
        {
            "tags": [
                "wsdl",
                "onvif"
            ],
            "owner": {
                "reputation": 101,
                "user_id": 3918375,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/2d96c2b9aeb32a17b6469357224d338c?s=128&d=identicon&r=PG&f=1",
                "display_name": "Niko",
                "link": "https://stackoverflow.com/users/3918375/niko"
            },
            "is_answered": false,
            "view_count": 4,
            "answer_count": 0,
            "score": 1,
            "last_activity_date": 1523525514,
            "creation_date": 1523525514,
            "question_id": 49792993,
            "body_markdown": "I am trying to write an ONVIF client to manage an ONVIF compliant device. For a specific service, I can get the wsdl file from the device, or get it from onvif.org. What is the difference between these two methods?",
            "link": "https://stackoverflow.com/questions/49792993/onvif-wsdl-files-from-website-vs-from-device",
            "title": "ONVIF: wsdl files from website vs from device",
            "body": "<p>I am trying to write an ONVIF client to manage an ONVIF compliant device. For a specific service, I can get the wsdl file from the device, or get it from onvif.org. What is the difference between these two methods?</p>\n"
        },
        {
            "tags": [
                "php",
                "mysql",
                "laravel"
            ],
            "owner": {
                "reputation": 36,
                "user_id": 7402185,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/58bf7f2a1a8fe5c3c5a9e19eeb309e24?s=128&d=identicon&r=PG&f=1",
                "display_name": "chen",
                "link": "https://stackoverflow.com/users/7402185/chen"
            },
            "is_answered": false,
            "view_count": 49,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523525514,
            "creation_date": 1523525514,
            "question_id": 49792992,
            "body_markdown": "In laravel, I tested this:\r\n    \r\n    Record::create([&#39;data&#39; =&gt; 2147483648]);exit;\r\n\r\nWhen I check mysql table, `data` column value is `-2147483648`, but `data` column type is `bigint(20)`. \r\n\r\nMy php version is 7.1, mysql version is 5.6, is there any problem with my environment ?",
            "link": "https://stackoverflow.com/questions/49792992/laravel-insert-big-integer-into-mysql-resulted-wrong-value",
            "title": "laravel Insert big integer into mysql resulted wrong value",
            "body": "<p>In laravel, I tested this:</p>\n\n<pre><code>Record::create(['data' =&gt; 2147483648]);exit;\n</code></pre>\n\n<p>When I check mysql table, <code>data</code> column value is <code>-2147483648</code>, but <code>data</code> column type is <code>bigint(20)</code>. </p>\n\n<p>My php version is 7.1, mysql version is 5.6, is there any problem with my environment ?</p>\n"
        },
        {
            "tags": [
                "cordova-plugins"
            ],
            "owner": {
                "reputation": 8,
                "user_id": 9253318,
                "user_type": "registered",
                "profile_image": "https://i.stack.imgur.com/rhrN9.jpg?s=128&g=1",
                "display_name": "MOTZI",
                "link": "https://stackoverflow.com/users/9253318/motzi"
            },
            "is_answered": true,
            "view_count": 15,
            "accepted_answer_id": 49792988,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1523525500,
            "creation_date": 1519138039,
            "question_id": 48887900,
            "body_markdown": "I first created a simple cordova plugin that containt one single java class *(`Hello.java` for instance)* that shows alert : everything works fine.\r\n\r\nNow i want to add more features to my plugin, i added another java class *(`Alert.java`)* and in my `Hello.java` i want to use `Alert.java` methods, i instantiated `Alert.java` class but after building, it shows error as it doesn&#39;t recognize it : `Alert myAlert = new Alert();` \r\n\r\nThanks in advance for clearing what i&#39;m doing wrong *(i only know basics in Java and completly new to cordova)*\r\n",
            "link": "https://stackoverflow.com/questions/48887900/cordova-plugin-multiple-classes",
            "title": "Cordova plugin multiple classes",
            "body": "<p>I first created a simple cordova plugin that containt one single java class <em>(<code>Hello.java</code> for instance)</em> that shows alert : everything works fine.</p>\n\n<p>Now i want to add more features to my plugin, i added another java class <em>(<code>Alert.java</code>)</em> and in my <code>Hello.java</code> i want to use <code>Alert.java</code> methods, i instantiated <code>Alert.java</code> class but after building, it shows error as it doesn't recognize it : <code>Alert myAlert = new Alert();</code> </p>\n\n<p>Thanks in advance for clearing what i'm doing wrong <em>(i only know basics in Java and completly new to cordova)</em></p>\n"
        },
        {
            "tags": [
                "sql-server",
                "file",
                "ldf"
            ],
            "owner": {
                "reputation": 38,
                "user_id": 5893639,
                "user_type": "registered",
                "accept_rate": 50,
                "profile_image": "https://www.gravatar.com/avatar/e89a053f3d28b76d110bddb36b8e7402?s=128&d=identicon&r=PG&f=1",
                "display_name": "codeOverflow",
                "link": "https://stackoverflow.com/users/5893639/codeoverflow"
            },
            "is_answered": false,
            "view_count": 27,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523525495,
            "creation_date": 1523522173,
            "last_edit_date": 1523525495,
            "question_id": 49791738,
            "body_markdown": "I have a question which hopefully someone can answer, I don&#39;t know if this is the right place to ask, but here it goes.\r\n\r\nI have a computer (which is not the server) with a .ldf file that is growing too big and is occupying a lot of disk space. I would like to know where does the file come from? \r\n\r\nI already know that it is a log file from SQL Server, but I don&#39;t know how it is ending in my disk, since the sql server is installed on the server and the file is on the computer disk and the file is growing automatically.\r\n\r\nSo to resume my questions are:\r\nWhat is placing the file on the disk?\r\nCan I shrink the file? If so how?\r\n\r\nAll help will be appreciated. Thanks in advance.\r\n\r\nEdit:\r\nIn the server the file has the same name, but doesn&#39;t occupy that much space.\r\nIn the server the size is about 500MB, on the disk is about 123GB. So I&#39;m thinking that it can be appending data periodically to the file on the local disk as a backup.\r\n[![sql server services][1]][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/l6PAc.png",
            "link": "https://stackoverflow.com/questions/49791738/ldf-file-growing-too-big",
            "title": "LDF file growing too big",
            "body": "<p>I have a question which hopefully someone can answer, I don't know if this is the right place to ask, but here it goes.</p>\n\n<p>I have a computer (which is not the server) with a .ldf file that is growing too big and is occupying a lot of disk space. I would like to know where does the file come from? </p>\n\n<p>I already know that it is a log file from SQL Server, but I don't know how it is ending in my disk, since the sql server is installed on the server and the file is on the computer disk and the file is growing automatically.</p>\n\n<p>So to resume my questions are:\nWhat is placing the file on the disk?\nCan I shrink the file? If so how?</p>\n\n<p>All help will be appreciated. Thanks in advance.</p>\n\n<p>Edit:\nIn the server the file has the same name, but doesn't occupy that much space.\nIn the server the size is about 500MB, on the disk is about 123GB. So I'm thinking that it can be appending data periodically to the file on the local disk as a backup.\n<a href=\"https://i.stack.imgur.com/l6PAc.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/l6PAc.png\" alt=\"sql server services\"></a></p>\n"
        },
        {
            "tags": [
                "python",
                "postgresql",
                "sorting",
                "skip",
                "is-empty"
            ],
            "owner": {
                "reputation": 2,
                "user_id": 1529965,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/a1bb3bd65ad960a8441088081eea598f?s=128&d=identicon&r=PG",
                "display_name": "Locust",
                "link": "https://stackoverflow.com/users/1529965/locust"
            },
            "is_answered": false,
            "view_count": 13,
            "answer_count": 0,
            "score": -2,
            "last_activity_date": 1523525493,
            "creation_date": 1523521816,
            "last_edit_date": 1523525493,
            "question_id": 49791606,
            "body_markdown": "i am trying to import a quite big csv file (21 columns / 125k rows) into Postgresql. Since you cannot insert an empty string into Postgres like with Sqlite. I am trying to sort through each row with a csvDictReader and filter the data in order to create an Insert statement for the columns/fileds with data. The sorting works well but when i try to create the insert statement it tries to insert the array instead of each value...  Please don&#180;t suggest other ways like Postgresql&#180;s copy etc. Thank you  \r\n\r\n    with codecs.open(filename, &#39;rb&#39;, encoding=&#39;utf-8&#39;) as csvfile:\r\n        reader = csv.DictReader(csvfile, delimiter=&#39;\\t&#39;)\r\n\t\ta=0\r\n\t\tcol=[]\r\n\t\tval=[]\r\n\t\tfor row in reader:\r\n\t\t    if a&gt;0:\r\n\t\t        for column, value in row.items():\r\n\t\t\t\t    if value != &#39;&#39;:\r\n\t\t\t\t\t    #print column, value\r\n\t\t\t\t\t\tcol.append(column)\r\n\t\t\t\t\t\tval.append(value)\r\n\t\t\t\t\t\t\ttry:\r\n\t\t\t\t\t\t\t\tc.execute(&#39;&#39;&#39;INSERT INTO AMA (%s) VALUES (%s) ON CONFLICT DO NOTHING&#39;&#39;&#39;,(col,val,))\r\n\t\t\t\t\t\t\texcept psycopg2.IntegrityError as e:\r\n\t\t\t\t\t\t\t\tprint e\r\n\t\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\tcol=[]\r\n\t\t\t\t\t\t\tval=[]\t\t\t\t\t\t\t\r\n\t\t\t\t\t\ta=a+1\r\n\r\npsycopg2.ProgrammingError: syntax error at or near &quot;ARRAY&quot;\r\nLINE 1: INSERT INTO AMA (ARRAY[&#39;fulfillment-id&#39;, &#39;sku&#39;, &#39;settleme...\r\n\r\n\r\nManaged to come this far, but now a different problem:\r\n\r\n    with codecs.open(filename, &#39;rb&#39;, encoding=&#39;utf-8&#39;) as csvfile:\r\n\t    reader = csv.DictReader(csvfile, delimiter=&#39;\\t&#39;)\r\n\t\ta=0\r\n\t\tcol=[]\r\n\t\tval=[]\r\n\t\tfor row in reader:\r\n\t\t    for column, value in row.items():\r\n\t\t\t    if value != &#39;&#39;:\r\n\t\t\t\t    col.append(column)\r\n\t\t\t\t\tval.append(value)\r\n\t\t\t\t\ttry:\r\n\t\t\t\t\t    query=&#39;&#39;&#39;INSERT INTO AMA %s VALUES %s ON CONFLICT DO NOTHING&#39;&#39;&#39;\r\n\t\t\t\t\t    print c.mogrify(query, (tuple(col), tuple(val)))\r\n\t\t\t\t\t    c.execute(query, (tuple(col), tuple(val),))\r\n\t\t\t\t\t\t\t\t\r\n\t\t\t\t    except psycopg2.IntegrityError as e:\r\n\t\t\t\t\t\tprint e\r\n\t\t\t\t\t\t\t\t\r\n\t\t\t\t\tcol=[]\r\n\t\t\t\t\tval=[]\t\t\t\t\t\t\t\r\n\t\t\ta=a+1\r\n\r\n\r\npsycopg2.ProgrammingError: syntax error at or near &quot;&#39;currency&#39;&quot;\r\nLINE 1: INSERT INTO AMA (&#39;currency&#39;, &#39;settlement-id&#39;, &#39;deposit-da\r\n\r\nIt looks like i need &quot; &quot; around the column names in Postregsql instead of &#39;&#39;. What i can do to change that?",
            "link": "https://stackoverflow.com/questions/49791606/python-postgresql-csv-import-empty-field-sorter",
            "title": "Python PostGreSql CSV Import empty field sorter",
            "body": "<p>i am trying to import a quite big csv file (21 columns / 125k rows) into Postgresql. Since you cannot insert an empty string into Postgres like with Sqlite. I am trying to sort through each row with a csvDictReader and filter the data in order to create an Insert statement for the columns/fileds with data. The sorting works well but when i try to create the insert statement it tries to insert the array instead of each value...  Please don´t suggest other ways like Postgresql´s copy etc. Thank you  </p>\n\n<pre><code>with codecs.open(filename, 'rb', encoding='utf-8') as csvfile:\n    reader = csv.DictReader(csvfile, delimiter='\\t')\n    a=0\n    col=[]\n    val=[]\n    for row in reader:\n        if a&gt;0:\n            for column, value in row.items():\n                if value != '':\n                    #print column, value\n                    col.append(column)\n                    val.append(value)\n                        try:\n                            c.execute('''INSERT INTO AMA (%s) VALUES (%s) ON CONFLICT DO NOTHING''',(col,val,))\n                        except psycopg2.IntegrityError as e:\n                            print e\n\n                        col=[]\n                        val=[]                          \n                    a=a+1\n</code></pre>\n\n<p>psycopg2.ProgrammingError: syntax error at or near \"ARRAY\"\nLINE 1: INSERT INTO AMA (ARRAY['fulfillment-id', 'sku', 'settleme...</p>\n\n<p>Managed to come this far, but now a different problem:</p>\n\n<pre><code>with codecs.open(filename, 'rb', encoding='utf-8') as csvfile:\n    reader = csv.DictReader(csvfile, delimiter='\\t')\n    a=0\n    col=[]\n    val=[]\n    for row in reader:\n        for column, value in row.items():\n            if value != '':\n                col.append(column)\n                val.append(value)\n                try:\n                    query='''INSERT INTO AMA %s VALUES %s ON CONFLICT DO NOTHING'''\n                    print c.mogrify(query, (tuple(col), tuple(val)))\n                    c.execute(query, (tuple(col), tuple(val),))\n\n                except psycopg2.IntegrityError as e:\n                    print e\n\n                col=[]\n                val=[]                          \n        a=a+1\n</code></pre>\n\n<p>psycopg2.ProgrammingError: syntax error at or near \"'currency'\"\nLINE 1: INSERT INTO AMA ('currency', 'settlement-id', 'deposit-da</p>\n\n<p>It looks like i need \" \" around the column names in Postregsql instead of ''. What i can do to change that?</p>\n"
        },
        {
            "tags": [
                "google-apps-script"
            ],
            "owner": {
                "reputation": 8,
                "user_id": 2112115,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/399dbec111c159e393f651105961f6e1?s=128&d=identicon&r=PG",
                "display_name": "Jonathan LeRoux",
                "link": "https://stackoverflow.com/users/2112115/jonathan-leroux"
            },
            "is_answered": true,
            "view_count": 788,
            "accepted_answer_id": 15124712,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1523525493,
            "creation_date": 1361906905,
            "question_id": 15097599,
            "body_markdown": "\r\n Despite fervent searches, I don&#39;t believe I have come up with quite the results I am needing. Within the Google Apps for Business Administrator console, under &quot;Reports&quot; there is the &quot;Audit Log.&quot; This audit log nearly displays everything I need; however, I am needing the actual name of the file instead of the fileID provided in the report.\r\n\r\n  The ultimate endgame is to generate, via Google Script, an automated daily report that gives  this same data to a few users daily. Is there any means by which I may access this data? I have been pouring over the APIs, and perhaps I&#39;ve missed something, but I don&#39;t feel that I&#39;ve found what I needed.\r\n\r\n  Any thoughts and help would be greatly appreciated. Thank you for your time and considerations.",
            "link": "https://stackoverflow.com/questions/15097599/accessing-the-administrator-audit-log-via-google-script",
            "title": "Accessing the Administrator Audit Log via Google Script",
            "body": "<p>Despite fervent searches, I don't believe I have come up with quite the results I am needing. Within the Google Apps for Business Administrator console, under \"Reports\" there is the \"Audit Log.\" This audit log nearly displays everything I need; however, I am needing the actual name of the file instead of the fileID provided in the report.</p>\n\n<p>The ultimate endgame is to generate, via Google Script, an automated daily report that gives  this same data to a few users daily. Is there any means by which I may access this data? I have been pouring over the APIs, and perhaps I've missed something, but I don't feel that I've found what I needed.</p>\n\n<p>Any thoughts and help would be greatly appreciated. Thank you for your time and considerations.</p>\n"
        },
        {
            "tags": [
                "wordpress",
                "custom-post-type"
            ],
            "owner": {
                "reputation": 5637,
                "user_id": 316408,
                "user_type": "registered",
                "accept_rate": 92,
                "profile_image": "https://www.gravatar.com/avatar/2d693c173f4a9e6cfec05c26c1df0d91?s=128&d=identicon&r=PG&f=1",
                "display_name": "mahatmanich",
                "link": "https://stackoverflow.com/users/316408/mahatmanich"
            },
            "is_answered": false,
            "view_count": 7,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523525492,
            "creation_date": 1523525492,
            "question_id": 49792983,
            "body_markdown": "I am using custom post types in combination with acf to build an api backend for three similar sites.\r\n\r\nNow I would like to have the ability to limit certain post types to allow only one single post. (sort of like a page)\r\n\r\nIs there a filter that allows for min/max number of posts of a given post type?\r\n\r\nOr can one deactivate the &quot;add new&quot; button after one post has been entered?\r\n\r\n**Basically I am trying to missuse CPTypes for pages with ACF and WPML**",
            "link": "https://stackoverflow.com/questions/49792983/restricting-a-custom-post-type-to-a-single-post-entry",
            "title": "Restricting a custom post type to a single post entry",
            "body": "<p>I am using custom post types in combination with acf to build an api backend for three similar sites.</p>\n\n<p>Now I would like to have the ability to limit certain post types to allow only one single post. (sort of like a page)</p>\n\n<p>Is there a filter that allows for min/max number of posts of a given post type?</p>\n\n<p>Or can one deactivate the \"add new\" button after one post has been entered?</p>\n\n<p><strong>Basically I am trying to missuse CPTypes for pages with ACF and WPML</strong></p>\n"
        },
        {
            "tags": [
                "python",
                "opencv",
                "tensorflow",
                "object-detection"
            ],
            "owner": {
                "reputation": 7,
                "user_id": 9619147,
                "user_type": "registered",
                "profile_image": "https://graph.facebook.com/1760046664016171/picture?type=large",
                "display_name": "Mae Baes",
                "link": "https://stackoverflow.com/users/9619147/mae-baes"
            },
            "is_answered": false,
            "view_count": 14,
            "answer_count": 0,
            "score": -1,
            "last_activity_date": 1523525491,
            "creation_date": 1523525491,
            "question_id": 49792982,
            "body_markdown": "I am currently using Tensorflow Object Detection API and I modified it so that it will capture frames from my webcam and the detection process will only run for every 2 seconds. However I want the bounding box to keep following the object in the 2 second span without the detection process but I don&#39;t know how to do it. Maybe someone can give me an insight to this. I also got this idea from a research and its says: \r\n\r\n&gt; Ideally, the detection algorithm is to be run on each input frame.\r\n&gt; However, this will inhibit the system from meeting its real time\r\n&gt; requirements. Instead, the detection algorithms in our implementation\r\n&gt; is invoked every two seconds. The location of the human targets in the\r\n&gt; remaining time is determined by tracking the detected humans using the\r\n&gt; tracking algorithm.\r\n\r\nThe process on how they did it is sadly not explained in the research and I&#39;m the type of person who only learns with examples.",
            "link": "https://stackoverflow.com/questions/49792982/executing-object-detection-only-for-every-2-seconds-python",
            "title": "Executing object detection only for every 2 seconds (python)",
            "body": "<p>I am currently using Tensorflow Object Detection API and I modified it so that it will capture frames from my webcam and the detection process will only run for every 2 seconds. However I want the bounding box to keep following the object in the 2 second span without the detection process but I don't know how to do it. Maybe someone can give me an insight to this. I also got this idea from a research and its says: </p>\n\n<blockquote>\n  <p>Ideally, the detection algorithm is to be run on each input frame.\n  However, this will inhibit the system from meeting its real time\n  requirements. Instead, the detection algorithms in our implementation\n  is invoked every two seconds. The location of the human targets in the\n  remaining time is determined by tracking the detected humans using the\n  tracking algorithm.</p>\n</blockquote>\n\n<p>The process on how they did it is sadly not explained in the research and I'm the type of person who only learns with examples.</p>\n"
        },
        {
            "tags": [
                "django",
                "docker"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 7394872,
                "user_type": "registered",
                "profile_image": "https://lh5.googleusercontent.com/-NMH7Mb0_iE0/AAAAAAAAAAI/AAAAAAAAAIk/3rYzidQcpoU/photo.jpg?sz=128",
                "display_name": "Rajat Sharma",
                "link": "https://stackoverflow.com/users/7394872/rajat-sharma"
            },
            "is_answered": false,
            "view_count": 19,
            "answer_count": 2,
            "score": 0,
            "last_activity_date": 1523525490,
            "creation_date": 1523474924,
            "question_id": 49783065,
            "body_markdown": "I am just a beginner at this \r\n\r\nSo when I run\r\n\r\n    sudo docker-compose run web python manage.py runserver    \r\nit shows\r\n\r\n    Starting thirddj_db_1 ... done  \r\n    usr/local/lib/python3.6/site-packages/psycopg2/__init__.py:144: \r\n    UserWarning: The psycopg2 wheel package will be renamed from release \r\n    2.8; in order to keep installing from binary please use &quot;pip install \r\n    psycopg2-binary&quot; instead. For details see: \r\n    &lt;http://initd.org/psycopg/docs/install.html#binary-install-from-pypi&gt;.\r\n    &quot;&quot;&quot;)\r\n    /usr/local/lib/python3.6/site-packages/psycopg2/__init__.py:144: \r\n    UserWarning: The psycopg2 wheel package will be renamed from release \r\n    2.8; in order to keep installing from binary please use &quot;pip install \r\n    psycopg2-binary&quot; instead. For details see: \r\n    &lt;http://initd.org/psycopg/docs/install.html#binary-install-from-pypi&gt;.\r\n    &quot;&quot;&quot;)\r\n    Performing system checks...\r\n\r\n    System check identified no issues (0 silenced).\r\n    April 11, 2018 - 19:15:59\r\n    Django version 1.11.12, using settings &#39;composeexample.settings&#39;\r\n    Starting development server at http://127.0.0.1:8000/\r\n    Quit the server with CONTROL-C.\r\n\r\nAnd then when I opened it in my browser, it shows the site cant be reached\r\n\r\nBut when i run\r\n\r\n    docker-compose up\r\n\r\nit shows \r\n\r\n    web_1  | /usr/local/lib/python3.6/site- \r\n    packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel \r\n    package will be renamed from release 2.8; in order to keep installing \r\n    from binary please use &quot;pip install psycopg2-binary&quot; instead. For \r\n    details see: &lt;http://initd.org/psycopg/docs/install.html#binary- \r\n    install-from-pypi&gt;.\r\n    web_1  |   &quot;&quot;&quot;)\r\n    web_1  | /usr/local/lib/python3.6/site- \r\n    packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel \r\n    package will be renamed from release 2.8; in order to keep installing \r\n    from binary please use &quot;pip install psycopg2-binary&quot; instead. For \r\n    details see: &lt;http://initd.org/psycopg/docs/install.html#binary- \r\n    install-from-pypi&gt;.\r\n    web_1  |   &quot;&quot;&quot;)\r\n    web_1  | Performing system checks...\r\n    web_1  | \r\n    web_1  | System check identified no issues (0 silenced).\r\n    web_1  | April 11, 2018 - 19:21:41\r\n    web_1  | Django version 1.11.12, using settings   \r\n    &#39;composeexample.settings&#39;\r\n    web_1  | Starting development server at http://0.0.0.0:8000/\r\n    web_1  | Quit the server with CONTROL-C.\r\n\r\n\r\nAnd then when I opened it in my browser, it works\r\n\r\n\r\nso i want to know Why sudo docker-compose run web python manage.py runserver is not working and what is the difference between both commands.\r\n \r\n\r\n ",
            "link": "https://stackoverflow.com/questions/49783065/django-runserver-on-docker-does-not-respond-when-opened-in-the-browser",
            "title": "Django runserver on Docker does not respond when opened in the browser",
            "body": "<p>I am just a beginner at this </p>\n\n<p>So when I run</p>\n\n<pre><code>sudo docker-compose run web python manage.py runserver    \n</code></pre>\n\n<p>it shows</p>\n\n<pre><code>Starting thirddj_db_1 ... done  \nusr/local/lib/python3.6/site-packages/psycopg2/__init__.py:144: \nUserWarning: The psycopg2 wheel package will be renamed from release \n2.8; in order to keep installing from binary please use \"pip install \npsycopg2-binary\" instead. For details see: \n&lt;http://initd.org/psycopg/docs/install.html#binary-install-from-pypi&gt;.\n\"\"\")\n/usr/local/lib/python3.6/site-packages/psycopg2/__init__.py:144: \nUserWarning: The psycopg2 wheel package will be renamed from release \n2.8; in order to keep installing from binary please use \"pip install \npsycopg2-binary\" instead. For details see: \n&lt;http://initd.org/psycopg/docs/install.html#binary-install-from-pypi&gt;.\n\"\"\")\nPerforming system checks...\n\nSystem check identified no issues (0 silenced).\nApril 11, 2018 - 19:15:59\nDjango version 1.11.12, using settings 'composeexample.settings'\nStarting development server at http://127.0.0.1:8000/\nQuit the server with CONTROL-C.\n</code></pre>\n\n<p>And then when I opened it in my browser, it shows the site cant be reached</p>\n\n<p>But when i run</p>\n\n<pre><code>docker-compose up\n</code></pre>\n\n<p>it shows </p>\n\n<pre><code>web_1  | /usr/local/lib/python3.6/site- \npackages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel \npackage will be renamed from release 2.8; in order to keep installing \nfrom binary please use \"pip install psycopg2-binary\" instead. For \ndetails see: &lt;http://initd.org/psycopg/docs/install.html#binary- \ninstall-from-pypi&gt;.\nweb_1  |   \"\"\")\nweb_1  | /usr/local/lib/python3.6/site- \npackages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel \npackage will be renamed from release 2.8; in order to keep installing \nfrom binary please use \"pip install psycopg2-binary\" instead. For \ndetails see: &lt;http://initd.org/psycopg/docs/install.html#binary- \ninstall-from-pypi&gt;.\nweb_1  |   \"\"\")\nweb_1  | Performing system checks...\nweb_1  | \nweb_1  | System check identified no issues (0 silenced).\nweb_1  | April 11, 2018 - 19:21:41\nweb_1  | Django version 1.11.12, using settings   \n'composeexample.settings'\nweb_1  | Starting development server at http://0.0.0.0:8000/\nweb_1  | Quit the server with CONTROL-C.\n</code></pre>\n\n<p>And then when I opened it in my browser, it works</p>\n\n<p>so i want to know Why sudo docker-compose run web python manage.py runserver is not working and what is the difference between both commands.</p>\n"
        },
        {
            "tags": [
                "scala",
                "apache-spark",
                "apache-spark-sql",
                "spark-dataframe"
            ],
            "owner": {
                "reputation": 30,
                "user_id": 7268207,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/4f59ac424a3a5b58865ca5e5baa3093f?s=128&d=identicon&r=PG&f=1",
                "display_name": "Sudheer Nulu",
                "link": "https://stackoverflow.com/users/7268207/sudheer-nulu"
            },
            "is_answered": true,
            "view_count": 27,
            "accepted_answer_id": 49792501,
            "answer_count": 1,
            "score": 2,
            "last_activity_date": 1523525490,
            "creation_date": 1523523620,
            "last_edit_date": 1523523702,
            "question_id": 49792292,
            "body_markdown": "I have two tables with columns table1 has id,name\r\nand table2 has only id\r\n\r\n    table 1\r\n    --------------\r\n    id     name\r\n    --------------\r\n    1\tsudheer\r\n    2\tsandeep\r\n    3\tsuresh\r\n    ----------------\r\n\r\ntable2\r\n\r\n    --------\r\n    id\r\n    -------- \r\n    1\r\n    2\r\n    -------\r\n\r\nrequired table should be if &quot;id&quot; column doesn&#39;t exist in the table2 my new column value should be &quot;N&quot; otherwise &quot;Y&quot;\r\n\r\n    table3\r\n    \r\n    id \tname \t\tIND\r\n    1\tsudheer\t\tY\r\n    2\tsandeep\t\tY\r\n    3\tsuresh \t\tN\r\n\r\n\r\n\r\nI have tried the below steps to approach:\r\n\r\n    val df = hc.sql(&quot;select * from table1&quot;)\r\n    val df1 = hc.sql(&quot;select * from table2&quot;)\r\n\r\nI tried to have a one more column (phone) in table2,as my join dataframe doesn&#39;t consist of id from table2,based on that null value I tried to set the value to Y/N\r\n\r\n    val df2 = df.join(df1,Seq(&quot;id&quot;),&quot;left_outer&quot;).withColumn(&quot;IND&quot;,exp(when(df1(&quot;phone&quot;)!= &quot;null&quot;,&quot;Y&quot;).otherwise(&quot;N&quot;)))\r\n\r\n\r\nBut this didn&#39;t worked out with error \r\nfound   : Boolean\r\n required: org.apache.spark.sql.Column\r\n\r\n\r\nCan anyone suggest any idea how to get the required result without adding a column to my table2?",
            "link": "https://stackoverflow.com/questions/49792292/to-create-a-new-column-based-on-the-joining-column-of-two-data-frames-using-scal",
            "title": "To create a new column based on the joining column of two data frames using scala",
            "body": "<p>I have two tables with columns table1 has id,name\nand table2 has only id</p>\n\n<pre><code>table 1\n--------------\nid     name\n--------------\n1   sudheer\n2   sandeep\n3   suresh\n----------------\n</code></pre>\n\n<p>table2</p>\n\n<pre><code>--------\nid\n-------- \n1\n2\n-------\n</code></pre>\n\n<p>required table should be if \"id\" column doesn't exist in the table2 my new column value should be \"N\" otherwise \"Y\"</p>\n\n<pre><code>table3\n\nid  name        IND\n1   sudheer     Y\n2   sandeep     Y\n3   suresh      N\n</code></pre>\n\n<p>I have tried the below steps to approach:</p>\n\n<pre><code>val df = hc.sql(\"select * from table1\")\nval df1 = hc.sql(\"select * from table2\")\n</code></pre>\n\n<p>I tried to have a one more column (phone) in table2,as my join dataframe doesn't consist of id from table2,based on that null value I tried to set the value to Y/N</p>\n\n<pre><code>val df2 = df.join(df1,Seq(\"id\"),\"left_outer\").withColumn(\"IND\",exp(when(df1(\"phone\")!= \"null\",\"Y\").otherwise(\"N\")))\n</code></pre>\n\n<p>But this didn't worked out with error \nfound   : Boolean\n required: org.apache.spark.sql.Column</p>\n\n<p>Can anyone suggest any idea how to get the required result without adding a column to my table2?</p>\n"
        },
        {
            "tags": [
                "python",
                "numpy",
                "matrix",
                "pagerank"
            ],
            "owner": {
                "reputation": 66,
                "user_id": 3085207,
                "user_type": "registered",
                "accept_rate": 64,
                "profile_image": "https://i.stack.imgur.com/7qezQ.jpg?s=128&g=1",
                "display_name": "Simon",
                "link": "https://stackoverflow.com/users/3085207/simon"
            },
            "is_answered": true,
            "view_count": 1481,
            "accepted_answer_id": 43644348,
            "answer_count": 2,
            "score": 3,
            "last_activity_date": 1523525489,
            "creation_date": 1493239978,
            "question_id": 43644320,
            "body_markdown": "I am working on building a transition matrix for implementing the PageRank algorithm. How could I use numpy to make sure that the columns add up to one.\r\n\r\nFor example:  \r\n\r\n    1 1 1   \r\n    1 1 1  \r\n    1 1 1\r\n\r\nshould be normalized to be \r\n\r\n    .33 .33 .33  \r\n    .33 .33 .33  \r\n    .33 .33 .33\r\n\r\n",
            "link": "https://stackoverflow.com/questions/43644320/how-to-make-numpy-array-column-sum-up-to-1",
            "title": "How to make numpy array column sum up to 1",
            "body": "<p>I am working on building a transition matrix for implementing the PageRank algorithm. How could I use numpy to make sure that the columns add up to one.</p>\n\n<p>For example:  </p>\n\n<pre><code>1 1 1   \n1 1 1  \n1 1 1\n</code></pre>\n\n<p>should be normalized to be </p>\n\n<pre><code>.33 .33 .33  \n.33 .33 .33  \n.33 .33 .33\n</code></pre>\n"
        },
        {
            "tags": [
                "snmp",
                "mib",
                "san",
                "opennms"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 1498679,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/500dadb060a787ce2d846edee2eba419?s=128&d=identicon&r=PG",
                "display_name": "user1498679",
                "link": "https://stackoverflow.com/users/1498679/user1498679"
            },
            "is_answered": false,
            "view_count": 17,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1523525488,
            "creation_date": 1523447931,
            "question_id": 49774543,
            "body_markdown": "I am using OpenNMS Horizon to monitor several nodes. For a given node it is  monitoring &quot;Storage (SNMP MIB-2 Host Resources) &quot; which tells about the Disk space (% of usage). While for local Disks of the node I am getting correct values. For the SAN File system disks wrong values (also negative values) are coming. However for few SAN volumes it is giving correct values. What are the possible reasons for this error?",
            "link": "https://stackoverflow.com/questions/49774543/opennms-storage-snmp-mib-2-host-resources-giving-incorrect-values",
            "title": "OpenNMS - Storage (SNMP MIB-2 Host Resources) giving incorrect values",
            "body": "<p>I am using OpenNMS Horizon to monitor several nodes. For a given node it is  monitoring \"Storage (SNMP MIB-2 Host Resources) \" which tells about the Disk space (% of usage). While for local Disks of the node I am getting correct values. For the SAN File system disks wrong values (also negative values) are coming. However for few SAN volumes it is giving correct values. What are the possible reasons for this error?</p>\n"
        },
        {
            "tags": [
                "c",
                "linux",
                "linux-kernel",
                "linux-device-driver",
                "block-device"
            ],
            "owner": {
                "reputation": 95,
                "user_id": 7991581,
                "user_type": "registered",
                "accept_rate": 62,
                "profile_image": "https://www.gravatar.com/avatar/f0ffccced69419766694f1774f275a3a?s=128&d=identicon&r=PG&f=1",
                "display_name": "Arkaik",
                "link": "https://stackoverflow.com/users/7991581/arkaik"
            },
            "is_answered": true,
            "view_count": 47,
            "accepted_answer_id": 49781937,
            "answer_count": 1,
            "score": 4,
            "last_activity_date": 1523525486,
            "creation_date": 1523467274,
            "last_edit_date": 1523525486,
            "question_id": 49781081,
            "body_markdown": "I&#39;m currently trying to implement a (not that ?) simple kernel block device driver.\r\n\r\nI inspired mainly from the book [Linux Device Drivers, 3rd Edition][1] which is not totally up-to-date anymore as it was published in 2005.\r\n\r\nAnyway the logic is still there and I learnt a lot from it. However examples are not really effective as many things have changed since 2005.\r\n\r\nI found a [github repository][2] where examples should be updated to work on recent kernels but I think there is still some things to update as I can&#39;t adapt examples to make it work on **kernel 4.9.0**\r\n\r\n***Here is how my module is made:***\r\n\r\nAt initialization :\r\n\r\n - Register the module as block device with `register_blkdev`\r\n - Allocate the device data buffer\r\n - Initialize spinlock\r\n - Initialize request queue\r\n - Configure request queue\r\n - Allocate `gendisk` structure\r\n - Fill `gendisk` structure\r\n - Create the disk with `add_disk`\r\n\r\nThen I implemented a function to handle request events from request queue and treat read and write events on the block device.\r\n\r\nHere is the function : (It&#39;s highly inspired from LLD-3rd with some modifications to match current kernel functions)\r\n\r\n    static void block_mod_request(struct request_queue *queue)\r\n    {\r\n        printk(KERN_NOTICE &quot;Entering request function\\n&quot;);\r\n        struct request *request;\r\n    \r\n        while(NULL != (request = blk_fetch_request(queue)))\r\n        {\r\n            blk_mod_t *self = request-&gt;rq_disk-&gt;private_data;\r\n            // Check if request is a filesystem request (i.e. moves block of data)\r\n            if(REQ_TYPE_FS != request-&gt;cmd_type)\r\n            {\r\n                // Close request with unsuccessful status\r\n                printk(KERN_WARNING &quot;Skip non-fs request\\n&quot;);\r\n                __blk_end_request_cur(request, -EIO);\r\n                continue;\r\n            }\r\n            // Treat request\r\n            block_mod_transfer(self, blk_rq_pos(request), blk_rq_cur_sectors(request), request-&gt;buffer, rq_data_dir(request));\r\n            // Close request with successful status\r\n            __blk_end_request_cur(request, 0);\r\n        }\r\n        return;\r\n    }\r\n\r\nHowever at compiling I got the following error:\r\n\r\n    block_mod.c:82:91: error: ‘struct request’ has no member named ‘buffer’\r\n             block_mod_transfer(self, blk_rq_pos(request), blk_rq_cur_sectors(request), request-&gt;buffer, rq_data_dir(request));\r\n\r\nAfter checking file `blkdev.h` into kernel v4.9.0 headers, it seems that the field `buffer` no longer exists into `struct request`.&lt;br/&gt;\r\nHowever I&#39;m not able to find any information on how things have evolved and how to modify the code to make it work.\r\n\r\nIf I understand well, the `buffer` field is supposed to be a pointer to a virtual kernel address. I suppose that once this buffer filled/read the kernel handles the transfer of data to/from userspace.\r\n\r\nI&#39;m kinda lost because I can&#39;t find where I should find the kernel virtual address if it&#39;s not given by the request anymore.\r\n\r\n***How am I supposed to know where to transfert data?***\r\n\r\n  [1]: https://www.safaribooksonline.com/library/view/linux-device-drivers/0596005903/\r\n  [2]: https://github.com/martinezjavier/ldd3",
            "link": "https://stackoverflow.com/questions/49781081/kernel-block-device",
            "title": "Kernel block device",
            "body": "<p>I'm currently trying to implement a (not that ?) simple kernel block device driver.</p>\n\n<p>I inspired mainly from the book <a href=\"https://www.safaribooksonline.com/library/view/linux-device-drivers/0596005903/\" rel=\"nofollow noreferrer\">Linux Device Drivers, 3rd Edition</a> which is not totally up-to-date anymore as it was published in 2005.</p>\n\n<p>Anyway the logic is still there and I learnt a lot from it. However examples are not really effective as many things have changed since 2005.</p>\n\n<p>I found a <a href=\"https://github.com/martinezjavier/ldd3\" rel=\"nofollow noreferrer\">github repository</a> where examples should be updated to work on recent kernels but I think there is still some things to update as I can't adapt examples to make it work on <strong>kernel 4.9.0</strong></p>\n\n<p><strong><em>Here is how my module is made:</em></strong></p>\n\n<p>At initialization :</p>\n\n<ul>\n<li>Register the module as block device with <code>register_blkdev</code></li>\n<li>Allocate the device data buffer</li>\n<li>Initialize spinlock</li>\n<li>Initialize request queue</li>\n<li>Configure request queue</li>\n<li>Allocate <code>gendisk</code> structure</li>\n<li>Fill <code>gendisk</code> structure</li>\n<li>Create the disk with <code>add_disk</code></li>\n</ul>\n\n<p>Then I implemented a function to handle request events from request queue and treat read and write events on the block device.</p>\n\n<p>Here is the function : (It's highly inspired from LLD-3rd with some modifications to match current kernel functions)</p>\n\n<pre><code>static void block_mod_request(struct request_queue *queue)\n{\n    printk(KERN_NOTICE \"Entering request function\\n\");\n    struct request *request;\n\n    while(NULL != (request = blk_fetch_request(queue)))\n    {\n        blk_mod_t *self = request-&gt;rq_disk-&gt;private_data;\n        // Check if request is a filesystem request (i.e. moves block of data)\n        if(REQ_TYPE_FS != request-&gt;cmd_type)\n        {\n            // Close request with unsuccessful status\n            printk(KERN_WARNING \"Skip non-fs request\\n\");\n            __blk_end_request_cur(request, -EIO);\n            continue;\n        }\n        // Treat request\n        block_mod_transfer(self, blk_rq_pos(request), blk_rq_cur_sectors(request), request-&gt;buffer, rq_data_dir(request));\n        // Close request with successful status\n        __blk_end_request_cur(request, 0);\n    }\n    return;\n}\n</code></pre>\n\n<p>However at compiling I got the following error:</p>\n\n<pre><code>block_mod.c:82:91: error: ‘struct request’ has no member named ‘buffer’\n         block_mod_transfer(self, blk_rq_pos(request), blk_rq_cur_sectors(request), request-&gt;buffer, rq_data_dir(request));\n</code></pre>\n\n<p>After checking file <code>blkdev.h</code> into kernel v4.9.0 headers, it seems that the field <code>buffer</code> no longer exists into <code>struct request</code>.<br/>\nHowever I'm not able to find any information on how things have evolved and how to modify the code to make it work.</p>\n\n<p>If I understand well, the <code>buffer</code> field is supposed to be a pointer to a virtual kernel address. I suppose that once this buffer filled/read the kernel handles the transfer of data to/from userspace.</p>\n\n<p>I'm kinda lost because I can't find where I should find the kernel virtual address if it's not given by the request anymore.</p>\n\n<p><strong><em>How am I supposed to know where to transfert data?</em></strong></p>\n"
        },
        {
            "tags": [
                "python",
                "tensorflow"
            ],
            "owner": {
                "reputation": 17,
                "user_id": 7811389,
                "user_type": "registered",
                "profile_image": "https://graph.facebook.com/406121886435828/picture?type=large",
                "display_name": "coma",
                "link": "https://stackoverflow.com/users/7811389/coma"
            },
            "is_answered": false,
            "view_count": 24,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1523525483,
            "creation_date": 1523522509,
            "question_id": 49791853,
            "body_markdown": "I&#39;m training a simple CNN model, here&#39;s the structure\r\n\r\nimage&#39;s size is 64X64\r\n\r\n**1st layer: convolution 5X5X8 average pooling 5X5 strides 2**\r\n\r\n**2nd layer: convolution 5X5X8 average pooling 5X5 strides 2**\r\n\r\n**3rd layer: convolution 1X1X32 average pooling global pooling**\r\n\r\n**4th layer: fully connected layer 32inputs 2outputs**\r\n\r\nwhen I use sparse_softmax_cross_entropy to calculate the loss, it raise an Error like this:\r\n\r\n[![enter image description here][1]][1]\r\n\r\nI thought the shape of Tensor output by 4th layer should be (?, 2) and it is, but I dont understand why the shape of logits is [**1280**, 2]\r\n\r\n    def stg_model_fn(features, labels, mode):\r\n    \t# Input Layer\r\n    \tx = tf.reshape(features, [-1, 64, 64, 1])\r\n    \t# print(x)\r\n    \tx = layer_module.conv_group(\r\n    \t\tinputs = x,\r\n    \t\tactivation = &quot;tanh&quot;,\r\n    \t\tfilters = 8,\r\n    \t\tkernel_size = [5, 5],\r\n    \t\tpool_size = 5,\r\n    \t\tstrides = 2,\r\n    \t\tabs_layer = True,\r\n    \t\tpool_padding = &quot;same&quot;)\r\n    \tprint(x)\r\n    \r\n    \tx = layer_module.conv_group(\r\n    \t\tinputs = x,\r\n    \t\tfilters = 16,\r\n    \t\tactivation = &quot;tanh&quot;,\r\n    \t\tkernel_size = [5, 5],\r\n    \t\tpool_size = 5,\r\n    \t\tstrides = 2,\r\n    \t\tabs_layer = False,\r\n    \t\tpool_padding = &quot;same&quot;)\r\n    \tprint(x)\r\n    \r\n    \tx = layer_module.conv_group(\r\n    \t\tinputs = x,\r\n    \t\tfilters = 32,\r\n    \t\tactivation = &quot;relu&quot;,\r\n    \t\tkernel_size = [1, 1],\r\n    \t\tpool_size = 16,\r\n    \t\tstrides = 1,\r\n    \t\tabs_layer = False,\r\n    \t\tpool_padding = &quot;valid&quot;)\r\n    \tprint(x)\r\n    \r\n    \r\n    \tx = tf.reshape(x, [-1, 32])\r\n    \tx = tf.layers.dense(inputs = x, units = 2)\r\n        \r\n\r\n    \r\n\r\n        predictions = {\r\n    \t    # Generate predictions (for PREDICT and EVAL mode)\r\n    \t    &quot;classes&quot;: tf.argmax(input=x, axis=1),\r\n    \t    # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\r\n    \t    # `logging_hook`.\r\n    \t    &quot;probabilities&quot;: tf.nn.softmax(x, name=&quot;softmax_tensor&quot;)\r\n    \t    }\r\n    \r\n        if mode == tf.estimator.ModeKeys.PREDICT:\r\n    \t    return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\r\n    \t    # Calculate Loss (for both TRAIN and EVAL modes)\r\n        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = labels, logits = x)\r\n\r\n    \r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n    \toptimizer = tf.train.GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\r\n    \ttrain_op = optimizer.minimize(\r\n    \t\tloss=loss,\r\n    \t\tglobal_step=tf.train.get_global_step())\r\n    \treturn tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n    \t# Add evaluation metrics (for EVAL mode)\r\n    eval_metric_ops = {\r\n    \t\t&quot;accuracy&quot;: tf.metrics.accuracy(\r\n    \t\t\tlabels=labels, predictions=predictions[&quot;classes&quot;])}\r\n    return tf.estimator.EstimatorSpec(\r\n    \t\tmode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\r\n\r\n  [1]: https://i.stack.imgur.com/woPZr.jpg",
            "link": "https://stackoverflow.com/questions/49791853/logits-and-labels-must-have-the-same-first-dimension-cnn",
            "title": "logits and labels must have the same first dimension CNN",
            "body": "<p>I'm training a simple CNN model, here's the structure</p>\n\n<p>image's size is 64X64</p>\n\n<p><strong>1st layer: convolution 5X5X8 average pooling 5X5 strides 2</strong></p>\n\n<p><strong>2nd layer: convolution 5X5X8 average pooling 5X5 strides 2</strong></p>\n\n<p><strong>3rd layer: convolution 1X1X32 average pooling global pooling</strong></p>\n\n<p><strong>4th layer: fully connected layer 32inputs 2outputs</strong></p>\n\n<p>when I use sparse_softmax_cross_entropy to calculate the loss, it raise an Error like this:</p>\n\n<p><a href=\"https://i.stack.imgur.com/woPZr.jpg\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/woPZr.jpg\" alt=\"enter image description here\"></a></p>\n\n<p>I thought the shape of Tensor output by 4th layer should be (?, 2) and it is, but I dont understand why the shape of logits is [<strong>1280</strong>, 2]</p>\n\n<pre><code>def stg_model_fn(features, labels, mode):\n    # Input Layer\n    x = tf.reshape(features, [-1, 64, 64, 1])\n    # print(x)\n    x = layer_module.conv_group(\n        inputs = x,\n        activation = \"tanh\",\n        filters = 8,\n        kernel_size = [5, 5],\n        pool_size = 5,\n        strides = 2,\n        abs_layer = True,\n        pool_padding = \"same\")\n    print(x)\n\n    x = layer_module.conv_group(\n        inputs = x,\n        filters = 16,\n        activation = \"tanh\",\n        kernel_size = [5, 5],\n        pool_size = 5,\n        strides = 2,\n        abs_layer = False,\n        pool_padding = \"same\")\n    print(x)\n\n    x = layer_module.conv_group(\n        inputs = x,\n        filters = 32,\n        activation = \"relu\",\n        kernel_size = [1, 1],\n        pool_size = 16,\n        strides = 1,\n        abs_layer = False,\n        pool_padding = \"valid\")\n    print(x)\n\n\n    x = tf.reshape(x, [-1, 32])\n    x = tf.layers.dense(inputs = x, units = 2)\n\n\n\n\n    predictions = {\n        # Generate predictions (for PREDICT and EVAL mode)\n        \"classes\": tf.argmax(input=x, axis=1),\n        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n        # `logging_hook`.\n        \"probabilities\": tf.nn.softmax(x, name=\"softmax_tensor\")\n        }\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n        # Calculate Loss (for both TRAIN and EVAL modes)\n    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = labels, logits = x)\n\n\n\nif mode == tf.estimator.ModeKeys.TRAIN:\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n    train_op = optimizer.minimize(\n        loss=loss,\n        global_step=tf.train.get_global_step())\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n    # Add evaluation metrics (for EVAL mode)\neval_metric_ops = {\n        \"accuracy\": tf.metrics.accuracy(\n            labels=labels, predictions=predictions[\"classes\"])}\nreturn tf.estimator.EstimatorSpec(\n        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n</code></pre>\n"
        },
        {
            "tags": [
                "caching",
                "cakephp",
                "cakephp-3.x"
            ],
            "owner": {
                "reputation": 26,
                "user_id": 7327885,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/28861d1a823bc630a4027531c4a40e51?s=128&d=identicon&r=PG&f=1",
                "display_name": "mbryja",
                "link": "https://stackoverflow.com/users/7327885/mbryja"
            },
            "is_answered": false,
            "view_count": 17,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1523525482,
            "creation_date": 1523475144,
            "last_edit_date": 1523525482,
            "question_id": 49783118,
            "body_markdown": "I have created a multi site application with Cakephp 3.5. So far I&#39;m very happy with it but I have one particular problem. I would like keep certain cache groups at the site scope. For example if I want to clear products group for site 1 it should not clear the products for site 2.\r\n\r\nI know I can have multiple config groups but this will not work for me because the number of sites is dynamic and new site can be created from admin panel on fly.\r\n\r\nIs there a way to do it be extending cache engine? ",
            "link": "https://stackoverflow.com/questions/49783118/dynamic-cache-groups-for-cakephp-3",
            "title": "Dynamic Cache Groups for CakePHP 3",
            "body": "<p>I have created a multi site application with Cakephp 3.5. So far I'm very happy with it but I have one particular problem. I would like keep certain cache groups at the site scope. For example if I want to clear products group for site 1 it should not clear the products for site 2.</p>\n\n<p>I know I can have multiple config groups but this will not work for me because the number of sites is dynamic and new site can be created from admin panel on fly.</p>\n\n<p>Is there a way to do it be extending cache engine? </p>\n"
        }
    ],
    "has_more": true,
    "quota_max": 300,
    "quota_remaining": 54
}