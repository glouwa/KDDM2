{
    "items": [
        {
            "tags": [
                "system-verilog",
                "modelsim",
                "vivado",
                "cadence",
                "system-verilog-dpi"
            ],
            "owner": {
                "reputation": 53,
                "user_id": 3716072,
                "user_type": "registered",
                "accept_rate": 100,
                "profile_image": "https://www.gravatar.com/avatar/bc8e205b8c65cd7f0ea0eecf466b9241?s=128&d=identicon&r=PG&f=1",
                "display_name": "user3716072",
                "link": "https://stackoverflow.com/users/3716072/user3716072"
            },
            "is_answered": true,
            "view_count": 32,
            "accepted_answer_id": 50353154,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1526427765,
            "creation_date": 1526391813,
            "question_id": 50351848,
            "body_markdown": "SystemVerilog LRM has some examples that show how to pass structs in SystemVerilog to\\from C through DPI-C layer. However when I try my own example it seems to not work at all in Incisive or Vivado simulator (it does work in ModelSim). I wanted to know if I am doing something wrong, or if it is an issue with the Simulators. My example is as follow:\r\n\r\n    #include &lt;stdio.h&gt;\r\n    \r\n    typedef struct {\r\n                   char f1;\r\n                   int f2;\r\n    } s1;\r\n    \r\n    void SimpleFcn(const s1 * in,s1 * out){\r\n        printf(&quot;In the C function the struct in has f1: %d\\n&quot;,in-&gt;f1);\r\n        printf(&quot;In the C function the struct in has f2: %d\\n&quot;,in-&gt;f2);\r\n        out-&gt;f1=!(in-&gt;f1);\r\n        out-&gt;f2=in-&gt;f2+1;    \r\n    }\r\n\r\nI compile the above code into a shared library:\r\n\r\n    gcc -c -fPIC -Wall -ansi -pedantic -Wno-long-long -fwrapv -O0 dpi_top.c -o dpi_top.o\r\n    gcc -shared -lm dpi_top.o -o dpi_top.so\r\n\r\nAnd the SystemVerilog code:\r\n\r\n    `timescale 1ns / 1ns\r\n    typedef struct {\r\n                   bit f1;\r\n                   int f2;\r\n                   } s1;\r\n     \r\n    import &quot;DPI-C&quot; function void SimpleFcn(input s1 in,output s1 out);\r\n    \r\n    module top();\r\n      s1 in,out;\r\n      initial\r\n        begin    \r\n        in.f1=1&#39;b0;  \r\n        in.f2 = 400;\r\n        $display(&quot;The input struct in SV has f1: %h and f2:%d&quot;,in.f1,in.f2);\r\n        SimpleFcn(in,out);\r\n        $display(&quot;The output struct in SV has f1: %h and f2:%d&quot;,out.f1,out.f2);\r\n     end\r\n    \r\n    endmodule \r\n\r\nIn Incisive I run it using irun:\r\n\r\n    irun -sv_lib ./dpi_top.so -sv ./top.sv\r\n\r\nBut it SegV&#39;s. \r\n\r\nIn Vivado I run it using \r\n\r\n    xvlog -sv ./top.sv \r\n    xelab top -sv_root ./ -sv_lib dpi_top.so -R\r\n\r\nIt runs fine until it exits simulation, then there is a memory corruption:\r\n\r\n    Vivado Simulator 2017.4\r\n    Time resolution is 1 ns\r\n    run -all\r\n    The input struct in SV has f1: 0 and f2:        400\r\n    In the C function the struct in has f1: 0\r\n    In the C function the struct in has f2: 400\r\n    The output struct in SV has f1: 1 and f2:        401\r\n    exit\r\n    *** Error in `xsim.dir/work.top/xsimk&#39;: double free or corruption (!prev): 0x00000000009da2c0 ***\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50351848/passing-c-structs-through-systemverilog-dpi-c-layer",
            "title": "Passing C structs through SystemVerilog DPI-C layer",
            "body": "<p>SystemVerilog LRM has some examples that show how to pass structs in SystemVerilog to\\from C through DPI-C layer. However when I try my own example it seems to not work at all in Incisive or Vivado simulator (it does work in ModelSim). I wanted to know if I am doing something wrong, or if it is an issue with the Simulators. My example is as follow:</p>\n\n<pre><code>#include &lt;stdio.h&gt;\n\ntypedef struct {\n               char f1;\n               int f2;\n} s1;\n\nvoid SimpleFcn(const s1 * in,s1 * out){\n    printf(\"In the C function the struct in has f1: %d\\n\",in-&gt;f1);\n    printf(\"In the C function the struct in has f2: %d\\n\",in-&gt;f2);\n    out-&gt;f1=!(in-&gt;f1);\n    out-&gt;f2=in-&gt;f2+1;    \n}\n</code></pre>\n\n<p>I compile the above code into a shared library:</p>\n\n<pre><code>gcc -c -fPIC -Wall -ansi -pedantic -Wno-long-long -fwrapv -O0 dpi_top.c -o dpi_top.o\ngcc -shared -lm dpi_top.o -o dpi_top.so\n</code></pre>\n\n<p>And the SystemVerilog code:</p>\n\n<pre><code>`timescale 1ns / 1ns\ntypedef struct {\n               bit f1;\n               int f2;\n               } s1;\n\nimport \"DPI-C\" function void SimpleFcn(input s1 in,output s1 out);\n\nmodule top();\n  s1 in,out;\n  initial\n    begin    \n    in.f1=1'b0;  \n    in.f2 = 400;\n    $display(\"The input struct in SV has f1: %h and f2:%d\",in.f1,in.f2);\n    SimpleFcn(in,out);\n    $display(\"The output struct in SV has f1: %h and f2:%d\",out.f1,out.f2);\n end\n\nendmodule \n</code></pre>\n\n<p>In Incisive I run it using irun:</p>\n\n<pre><code>irun -sv_lib ./dpi_top.so -sv ./top.sv\n</code></pre>\n\n<p>But it SegV's. </p>\n\n<p>In Vivado I run it using </p>\n\n<pre><code>xvlog -sv ./top.sv \nxelab top -sv_root ./ -sv_lib dpi_top.so -R\n</code></pre>\n\n<p>It runs fine until it exits simulation, then there is a memory corruption:</p>\n\n<pre><code>Vivado Simulator 2017.4\nTime resolution is 1 ns\nrun -all\nThe input struct in SV has f1: 0 and f2:        400\nIn the C function the struct in has f1: 0\nIn the C function the struct in has f2: 400\nThe output struct in SV has f1: 1 and f2:        401\nexit\n*** Error in `xsim.dir/work.top/xsimk': double free or corruption (!prev): 0x00000000009da2c0 ***\n</code></pre>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "amazon-kinesis-firehose"
            ],
            "owner": {
                "reputation": 45,
                "user_id": 5915500,
                "user_type": "registered",
                "accept_rate": 29,
                "profile_image": "https://www.gravatar.com/avatar/084b5d993d827e2d1fc51125ac845aee?s=128&d=identicon&r=PG&f=1",
                "display_name": "Dorian McAllister",
                "link": "https://stackoverflow.com/users/5915500/dorian-mcallister"
            },
            "is_answered": false,
            "view_count": 11,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526427749,
            "creation_date": 1526395376,
            "last_edit_date": 1526427749,
            "question_id": 50353086,
            "body_markdown": "I am trying to design a scalable solution that requires taking in an input (large amount of data) from an S3 bucket. I need to take the contents of each Object that is persisted in this bucket and call a custom ElasticSearch endpoint (not AWS ElasticSearch) via HTTP POST. \r\nCurrently we have an Autoscaling Fleet of EC2 instances listening to SQS events and doing the exact same thing. However company policies ask for a bimonthly upgradation of EC2 Instances to adopt the latest security patches etc and its becoming a headache to do this on our AutoScaling group. \r\n\r\nCan I use AWS FireHose to RECIEVE events from S3 buckets and parse and send the contents to our custom ElasticSearch endpoint instead? trying to understand if this is a possibility (or I just cannot send it to ANY ElasticSearch endpoint other than AWS ElasticSearch etc)?",
            "link": "https://stackoverflow.com/questions/50353086/using-aws-firehose-to-load-data-into-custom-elasticsearch-endpoint",
            "title": "Using AWS FireHose to Load data into Custom ElasticSearch Endpoint",
            "body": "<p>I am trying to design a scalable solution that requires taking in an input (large amount of data) from an S3 bucket. I need to take the contents of each Object that is persisted in this bucket and call a custom ElasticSearch endpoint (not AWS ElasticSearch) via HTTP POST. \nCurrently we have an Autoscaling Fleet of EC2 instances listening to SQS events and doing the exact same thing. However company policies ask for a bimonthly upgradation of EC2 Instances to adopt the latest security patches etc and its becoming a headache to do this on our AutoScaling group. </p>\n\n<p>Can I use AWS FireHose to RECIEVE events from S3 buckets and parse and send the contents to our custom ElasticSearch endpoint instead? trying to understand if this is a possibility (or I just cannot send it to ANY ElasticSearch endpoint other than AWS ElasticSearch etc)?</p>\n"
        },
        {
            "tags": [
                "android",
                "android-studio"
            ],
            "owner": {
                "reputation": 78,
                "user_id": 3857653,
                "user_type": "registered",
                "profile_image": "https://i.stack.imgur.com/oR1wI.jpg?s=128&g=1",
                "display_name": "Ripon Kumar Saha",
                "link": "https://stackoverflow.com/users/3857653/ripon-kumar-saha"
            },
            "is_answered": true,
            "view_count": 54913,
            "accepted_answer_id": 33383940,
            "answer_count": 6,
            "score": 14,
            "last_activity_date": 1526427746,
            "creation_date": 1423204284,
            "last_edit_date": 1526427746,
            "question_id": 28359851,
            "body_markdown": "I need to install android studio on many PCs. Is there any way to install android studio latest version offline by downloading all the offline files just once?\r\nMost of the PCs are running windows 8.1 / 8 / 7.\r\n",
            "link": "https://stackoverflow.com/questions/28359851/how-to-install-android-studio-full-offline",
            "title": "how to install android studio full offline",
            "body": "<p>I need to install android studio on many PCs. Is there any way to install android studio latest version offline by downloading all the offline files just once?\nMost of the PCs are running windows 8.1 / 8 / 7.</p>\n"
        },
        {
            "tags": [
                "apache-spark",
                "amazon-kinesis-firehose"
            ],
            "owner": {
                "reputation": 13,
                "user_id": 6431511,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-AorJmVSsn9g/AAAAAAAAAAI/AAAAAAAADDA/6JN0d1fwVr0/photo.jpg?sz=128",
                "display_name": "Martin Peng",
                "link": "https://stackoverflow.com/users/6431511/martin-peng"
            },
            "is_answered": false,
            "view_count": 18,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526427742,
            "creation_date": 1523587006,
            "last_edit_date": 1526427742,
            "question_id": 49808477,
            "body_markdown": "I wrote below listener to catch the event SparkListenerApplicationEnd. However the Spark Job still terminate in seconds when I am still persisting the data. Is there any other way to achieve this? \r\n\r\n      \r\n    class SparkJobEndListener(collectors: List[MetricCollector]) \r\n        extends SparkFirehoseListener {\r\n        override def onEvent(event: SparkListenerEvent): Unit = {\r\n          event match {\r\n            case jobEnd: SparkListenerApplicationEnd =&gt;\r\n              SparkLogger.log(&quot;[MetricsCollector] Job end event received...&quot;)\r\n              collectors.foreach(collector =&gt; collector.saveToBlob(true))\r\n            case _ =&gt;\r\n          }\r\n        }\r\n      }",
            "link": "https://stackoverflow.com/questions/49808477/how-to-persist-in-memory-data-into-file-system-when-spark-job-end",
            "title": "How to persist in memory data into file system when Spark Job end",
            "body": "<p>I wrote below listener to catch the event SparkListenerApplicationEnd. However the Spark Job still terminate in seconds when I am still persisting the data. Is there any other way to achieve this? </p>\n\n<pre><code>class SparkJobEndListener(collectors: List[MetricCollector]) \n    extends SparkFirehoseListener {\n    override def onEvent(event: SparkListenerEvent): Unit = {\n      event match {\n        case jobEnd: SparkListenerApplicationEnd =&gt;\n          SparkLogger.log(\"[MetricsCollector] Job end event received...\")\n          collectors.foreach(collector =&gt; collector.saveToBlob(true))\n        case _ =&gt;\n      }\n    }\n  }\n</code></pre>\n"
        },
        {
            "tags": [
                "git",
                "gnupg"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9781664,
                "user_type": "registered",
                "profile_image": "https://i.stack.imgur.com/kV8Zg.png?s=128&g=1",
                "display_name": "skd369174199",
                "link": "https://stackoverflow.com/users/9781664/skd369174199"
            },
            "is_answered": true,
            "view_count": 59,
            "accepted_answer_id": 50360580,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526427740,
            "creation_date": 1526151298,
            "last_edit_date": 1526155693,
            "question_id": 50309704,
            "body_markdown": "gpg4win-3.1.0 gpg:\r\n\r\n[enter image description here][1]\r\n\r\nI have tried many methods. But it didn&#39;t work.\r\nI have my key ID. \r\nI did this.\r\n\r\n`$ git config --global user.signingkey XXXXXXX`\r\n\r\nI wanted to change my gpg.program.\r\nBut I have no gpg2.exe.\r\n[enter image description here][2]\r\n\r\n\r\n[enter image description here][3]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/KKgWa.png\r\n  [2]: https://i.stack.imgur.com/zq05r.png\r\n  [3]: https://i.stack.imgur.com/OPs5z.png",
            "link": "https://stackoverflow.com/questions/50309704/skipped-xxx-secret-key-not-available",
            "title": "Skipped &quot;XXX&quot;: secret key not available",
            "body": "<p>gpg4win-3.1.0 gpg:</p>\n\n<p><img src=\"https://i.stack.imgur.com/KKgWa.png\" alt=\"enter image description here\"></p>\n\n<p>I have tried many methods. But it didn't work.\nI have my key ID. \nI did this.</p>\n\n<p><code>$ git config --global user.signingkey XXXXXXX</code></p>\n\n<p>I wanted to change my gpg.program.\nBut I have no gpg2.exe.\n<img src=\"https://i.stack.imgur.com/zq05r.png\" alt=\"enter image description here\"></p>\n\n<p><img src=\"https://i.stack.imgur.com/OPs5z.png\" alt=\"enter image description here\"></p>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "xamarin",
                "amazon-s3"
            ],
            "owner": {
                "reputation": 6,
                "user_id": 4770636,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-zDu2-Ov2PCA/AAAAAAAAAAI/AAAAAAAAADg/gPHC2cayRI8/photo.jpg?sz=128",
                "display_name": "Jhxnnxthxn kpl",
                "link": "https://stackoverflow.com/users/4770636/jhxnnxthxn-kpl"
            },
            "is_answered": false,
            "view_count": 7,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526427716,
            "creation_date": 1526427716,
            "question_id": 50360577,
            "body_markdown": "I can&#180;t set S3CannedACL.PublicRead in xamarin I don&#39;t know how to set this permissions.\r\n\r\n    AWSConfigsS3.UseSignatureVersion4 = true;\r\n\r\n    s3Client= new AmazonS3Client(AWS_ACCESS_KEY, AWS_SECRET_KEY,RegionEndpoint.USEast1);\r\n\r\n    public bool Subir_Fotos(){\r\n            var transferUtility = new TransferUtility(s3Client);\r\n            try\r\n            {\r\n                transferUtility.UploadAsync(\r\n                    Path.Combine(RutaFoto),AWS_BUCKET\r\n                );\r\n\r\n               \r\n            }catch(Exception e){\r\n                return false;\r\n            }\r\n\r\n            return true;\r\n        }\r\nI need to add  &quot;S3CannedACL.PublicRead&quot; to add this but i don&#39;t know where, cause when i execute this the image in the bucket display https://s3.amazonaws.com/test-content/downloads/test.pdf\r\n\r\nThanks for yours answer. \r\n",
            "link": "https://stackoverflow.com/questions/50360577/how-to-set-s3cannedacl-publicread-on-xamarin-awss3",
            "title": "how to set S3CannedACL.PublicRead on xamarin awsS3",
            "body": "<p>I can´t set S3CannedACL.PublicRead in xamarin I don't know how to set this permissions.</p>\n\n<pre><code>AWSConfigsS3.UseSignatureVersion4 = true;\n\ns3Client= new AmazonS3Client(AWS_ACCESS_KEY, AWS_SECRET_KEY,RegionEndpoint.USEast1);\n\npublic bool Subir_Fotos(){\n        var transferUtility = new TransferUtility(s3Client);\n        try\n        {\n            transferUtility.UploadAsync(\n                Path.Combine(RutaFoto),AWS_BUCKET\n            );\n\n\n        }catch(Exception e){\n            return false;\n        }\n\n        return true;\n    }\n</code></pre>\n\n<p>I need to add  \"S3CannedACL.PublicRead\" to add this but i don't know where, cause when i execute this the image in the bucket display <a href=\"https://s3.amazonaws.com/test-content/downloads/test.pdf\" rel=\"nofollow noreferrer\">https://s3.amazonaws.com/test-content/downloads/test.pdf</a></p>\n\n<p>Thanks for yours answer. </p>\n"
        },
        {
            "tags": [
                "python-2.7",
                "aws-lambda",
                "amazon-redshift",
                "amazon-vpc",
                "amazon-kinesis-firehose"
            ],
            "owner": {
                "reputation": 109,
                "user_id": 1825174,
                "user_type": "registered",
                "accept_rate": 89,
                "profile_image": "https://www.gravatar.com/avatar/b7963fe5a868014ed9ff43376de6e6fb?s=128&d=identicon&r=PG",
                "display_name": "kilomo",
                "link": "https://stackoverflow.com/users/1825174/kilomo"
            },
            "is_answered": false,
            "view_count": 86,
            "answer_count": 0,
            "score": 1,
            "last_activity_date": 1526427715,
            "creation_date": 1519484017,
            "last_edit_date": 1526427715,
            "question_id": 48964138,
            "body_markdown": "I am using Firehose with a lambda decorator to ingest vpc flow logs into Redshift.\r\n(VPC Flow Logs -&gt; Kinesis Data Stream -&gt; Kinesis Firehose -&gt; Lambda Decorator -&gt; Redshift) The volume of traffic is high which causes the lambda to error out with task timed out when reingesting unprocessed records back into firehose. The lambda has the max timeout and 3GB of memory.\r\n\r\nI believe the issue is related to lambda&#39;s 6mb payload size. Is there a way to batch or reduce the payload to ensure the function doesn&#39;t error out? Thanks in advance.\r\n\r\n\r\n        import base64\r\n        import json\r\n        import gzip\r\n        import StringIO\r\n        import boto3\r\n        import datetime\r\n\r\n        def transformLogEvent(log_event):\r\n            version     = log_event[&#39;extractedFields&#39;][&#39;version&#39;]\r\n            accountid   = log_event[&#39;extractedFields&#39;][&#39;account_id&#39;]\r\n            interfaceid = log_event[&#39;extractedFields&#39;][&#39;interface_id&#39;]\r\n            srcaddr     = log_event[&#39;extractedFields&#39;][&#39;srcaddr&#39;]\r\n            dstaddr     = log_event[&#39;extractedFields&#39;][&#39;dstaddr&#39;]\r\n            srcport     = log_event[&#39;extractedFields&#39;][&#39;srcport&#39;]\r\n            dstport     = log_event[&#39;extractedFields&#39;][&#39;dstport&#39;]\r\n            protocol    = log_event[&#39;extractedFields&#39;][&#39;protocol&#39;]\r\n            packets     = log_event[&#39;extractedFields&#39;][&#39;packets&#39;]\r\n            bytes       = log_event[&#39;extractedFields&#39;][&#39;bytes&#39;]\r\n            starttime   = datetime.datetime.fromtimestamp(int(log_event[&#39;extractedFields&#39;][&#39;start&#39;])).strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)\r\n            endtime     = datetime.datetime.fromtimestamp(int(log_event[&#39;extractedFields&#39;][&#39;end&#39;])).strftime(&#39;%Y-%m-%d %H:%M:%S&#39;)\r\n            action      = log_event[&#39;extractedFields&#39;][&#39;action&#39;]\r\n            logstatus   = log_event[&#39;extractedFields&#39;][&#39;log_status&#39;]\r\n\r\n            row = &#39;&quot;&#39; + str(version) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(accountid) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(interfaceid) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(srcaddr) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(dstaddr) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(srcport) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(dstport) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(protocol) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(packets) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(bytes) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(starttime) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(endtime) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(action) + &#39;&quot;&#39; + &quot;,&quot; + &#39;&quot;&#39; + str(logstatus) + &#39;&quot;&#39; + &quot;\\n&quot;\r\n            #print(row)\r\n            return row\r\n\r\n        def processRecords(records):\r\n            for r in records:\r\n                data = base64.b64decode(r[&#39;data&#39;])\r\n                striodata = StringIO.StringIO(data)\r\n                try:\r\n                    with gzip.GzipFile(fileobj=striodata, mode=&#39;r&#39;) as f:\r\n                        data = json.loads(f.read())\r\n                except IOError:\r\n                    # likely the data was re-ingested into firehose\r\n                    pass\r\n\r\n                recId = r[&#39;recordId&#39;]\r\n                # re-ingested data into firehose\r\n                if type(data) == str:\r\n                    yield {\r\n                        &#39;data&#39;: data,\r\n                        &#39;result&#39;: &#39;Ok&#39;,\r\n                        &#39;recordId&#39;: recId\r\n                    }\r\n                elif data[&#39;messageType&#39;] != &#39;DATA_MESSAGE&#39;:\r\n                    yield {\r\n                        &#39;result&#39;: &#39;ProcessingFailed&#39;,\r\n                        &#39;recordId&#39;: recId\r\n                    }\r\n                else:\r\n                    data = &#39;&#39;.join([transformLogEvent(e) for e in data[&#39;logEvents&#39;]])\r\n                    #print(data)\r\n                    data = base64.b64encode(data)\r\n                    yield {\r\n                        &#39;data&#39;: data,\r\n                        &#39;result&#39;: &#39;Ok&#39;,\r\n                        &#39;recordId&#39;: recId\r\n                    }\r\n\r\n\r\n        def putRecords(streamName, records, client, attemptsMade, maxAttempts):\r\n            failedRecords = []\r\n            codes = []\r\n            errMsg = &#39;&#39;\r\n            try:\r\n                response = client.put_record_batch(DeliveryStreamName=streamName, Records=records)\r\n            except Exception as e:\r\n                failedRecords = records\r\n                errMsg = str(e)\r\n\r\n            # if there are no failedRecords (put_record_batch succeeded), iterate over the response to gather results\r\n            if not failedRecords and response[&#39;FailedPutCount&#39;] &gt; 0:\r\n                for idx, res in enumerate(response[&#39;RequestResponses&#39;]):\r\n                    if not res[&#39;ErrorCode&#39;]:\r\n                        continue\r\n\r\n                    codes.append(res[&#39;ErrorCode&#39;])\r\n                    failedRecords.append(records[idx])\r\n\r\n                errMsg = &#39;Individual error codes: &#39; + &#39;,&#39;.join(codes)\r\n\r\n            if len(failedRecords) &gt; 0:\r\n                if attemptsMade + 1 &lt; maxAttempts:\r\n                    print(&#39;Some records failed while calling PutRecords, retrying. %s&#39; % (errMsg))\r\n                    putRecords(streamName, failedRecords, client, attemptsMade + 1, maxAttempts)\r\n                else:\r\n                    raise RuntimeError(&#39;Could not put records after %s attempts. %s&#39; % (str(maxAttempts), errMsg))\r\n\r\n\r\n        def handler(event, context):\r\n            streamARN = &#39;&#39;\r\n            region = &#39;&#39;\r\n            streamName = &#39;&#39;\r\n\r\n            records = list(processRecords(event[&#39;records&#39;]))\r\n            projectedSize = 0\r\n            recordsToReingest = []\r\n            for idx, rec in enumerate(records):\r\n                if rec[&#39;result&#39;] == &#39;ProcessingFailed&#39;:\r\n                    continue\r\n                projectedSize += len(rec[&#39;data&#39;]) + len(rec[&#39;recordId&#39;])\r\n                # 4000000 instead of 6291456 to leave ample headroom for the stuff we didn&#39;t account for\r\n                if projectedSize &gt; 4000000:\r\n                    recordsToReingest.append({\r\n                        &#39;Data&#39;: rec[&#39;data&#39;]\r\n                    })\r\n                    records[idx][&#39;result&#39;] = &#39;Dropped&#39;\r\n                    del(records[idx][&#39;data&#39;])\r\n\r\n            if len(recordsToReingest) &gt; 0:\r\n                client = boto3.client(&#39;firehose&#39;, region_name=region)\r\n                putRecords(streamName, recordsToReingest, client, attemptsMade=0, maxAttempts=20)\r\n                print(&#39;Reingested %d records out of %d&#39; % (len(recordsToReingest), len(event[&#39;records&#39;])))\r\n            else:\r\n                print(&#39;No records to be reingested&#39;)\r\n\r\n            return {&quot;records&quot;: records}\r\n",
            "link": "https://stackoverflow.com/questions/48964138/kinesis-firehose-with-lambda-decorator-getting-throttled",
            "title": "Kinesis Firehose with Lambda decorator getting throttled",
            "body": "<p>I am using Firehose with a lambda decorator to ingest vpc flow logs into Redshift.\n(VPC Flow Logs -> Kinesis Data Stream -> Kinesis Firehose -> Lambda Decorator -> Redshift) The volume of traffic is high which causes the lambda to error out with task timed out when reingesting unprocessed records back into firehose. The lambda has the max timeout and 3GB of memory.</p>\n\n<p>I believe the issue is related to lambda's 6mb payload size. Is there a way to batch or reduce the payload to ensure the function doesn't error out? Thanks in advance.</p>\n\n<pre><code>    import base64\n    import json\n    import gzip\n    import StringIO\n    import boto3\n    import datetime\n\n    def transformLogEvent(log_event):\n        version     = log_event['extractedFields']['version']\n        accountid   = log_event['extractedFields']['account_id']\n        interfaceid = log_event['extractedFields']['interface_id']\n        srcaddr     = log_event['extractedFields']['srcaddr']\n        dstaddr     = log_event['extractedFields']['dstaddr']\n        srcport     = log_event['extractedFields']['srcport']\n        dstport     = log_event['extractedFields']['dstport']\n        protocol    = log_event['extractedFields']['protocol']\n        packets     = log_event['extractedFields']['packets']\n        bytes       = log_event['extractedFields']['bytes']\n        starttime   = datetime.datetime.fromtimestamp(int(log_event['extractedFields']['start'])).strftime('%Y-%m-%d %H:%M:%S')\n        endtime     = datetime.datetime.fromtimestamp(int(log_event['extractedFields']['end'])).strftime('%Y-%m-%d %H:%M:%S')\n        action      = log_event['extractedFields']['action']\n        logstatus   = log_event['extractedFields']['log_status']\n\n        row = '\"' + str(version) + '\"' + \",\" + '\"' + str(accountid) + '\"' + \",\" + '\"' + str(interfaceid) + '\"' + \",\" + '\"' + str(srcaddr) + '\"' + \",\" + '\"' + str(dstaddr) + '\"' + \",\" + '\"' + str(srcport) + '\"' + \",\" + '\"' + str(dstport) + '\"' + \",\" + '\"' + str(protocol) + '\"' + \",\" + '\"' + str(packets) + '\"' + \",\" + '\"' + str(bytes) + '\"' + \",\" + '\"' + str(starttime) + '\"' + \",\" + '\"' + str(endtime) + '\"' + \",\" + '\"' + str(action) + '\"' + \",\" + '\"' + str(logstatus) + '\"' + \"\\n\"\n        #print(row)\n        return row\n\n    def processRecords(records):\n        for r in records:\n            data = base64.b64decode(r['data'])\n            striodata = StringIO.StringIO(data)\n            try:\n                with gzip.GzipFile(fileobj=striodata, mode='r') as f:\n                    data = json.loads(f.read())\n            except IOError:\n                # likely the data was re-ingested into firehose\n                pass\n\n            recId = r['recordId']\n            # re-ingested data into firehose\n            if type(data) == str:\n                yield {\n                    'data': data,\n                    'result': 'Ok',\n                    'recordId': recId\n                }\n            elif data['messageType'] != 'DATA_MESSAGE':\n                yield {\n                    'result': 'ProcessingFailed',\n                    'recordId': recId\n                }\n            else:\n                data = ''.join([transformLogEvent(e) for e in data['logEvents']])\n                #print(data)\n                data = base64.b64encode(data)\n                yield {\n                    'data': data,\n                    'result': 'Ok',\n                    'recordId': recId\n                }\n\n\n    def putRecords(streamName, records, client, attemptsMade, maxAttempts):\n        failedRecords = []\n        codes = []\n        errMsg = ''\n        try:\n            response = client.put_record_batch(DeliveryStreamName=streamName, Records=records)\n        except Exception as e:\n            failedRecords = records\n            errMsg = str(e)\n\n        # if there are no failedRecords (put_record_batch succeeded), iterate over the response to gather results\n        if not failedRecords and response['FailedPutCount'] &gt; 0:\n            for idx, res in enumerate(response['RequestResponses']):\n                if not res['ErrorCode']:\n                    continue\n\n                codes.append(res['ErrorCode'])\n                failedRecords.append(records[idx])\n\n            errMsg = 'Individual error codes: ' + ','.join(codes)\n\n        if len(failedRecords) &gt; 0:\n            if attemptsMade + 1 &lt; maxAttempts:\n                print('Some records failed while calling PutRecords, retrying. %s' % (errMsg))\n                putRecords(streamName, failedRecords, client, attemptsMade + 1, maxAttempts)\n            else:\n                raise RuntimeError('Could not put records after %s attempts. %s' % (str(maxAttempts), errMsg))\n\n\n    def handler(event, context):\n        streamARN = ''\n        region = ''\n        streamName = ''\n\n        records = list(processRecords(event['records']))\n        projectedSize = 0\n        recordsToReingest = []\n        for idx, rec in enumerate(records):\n            if rec['result'] == 'ProcessingFailed':\n                continue\n            projectedSize += len(rec['data']) + len(rec['recordId'])\n            # 4000000 instead of 6291456 to leave ample headroom for the stuff we didn't account for\n            if projectedSize &gt; 4000000:\n                recordsToReingest.append({\n                    'Data': rec['data']\n                })\n                records[idx]['result'] = 'Dropped'\n                del(records[idx]['data'])\n\n        if len(recordsToReingest) &gt; 0:\n            client = boto3.client('firehose', region_name=region)\n            putRecords(streamName, recordsToReingest, client, attemptsMade=0, maxAttempts=20)\n            print('Reingested %d records out of %d' % (len(recordsToReingest), len(event['records'])))\n        else:\n            print('No records to be reingested')\n\n        return {\"records\": records}\n</code></pre>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "amazon-kinesis-firehose"
            ],
            "owner": {
                "reputation": 166,
                "user_id": 1245257,
                "user_type": "registered",
                "accept_rate": 82,
                "profile_image": "https://www.gravatar.com/avatar/675d556813a7803c0c121d75bc75c1e3?s=128&d=identicon&r=PG",
                "display_name": "Giles Hunt",
                "link": "https://stackoverflow.com/users/1245257/giles-hunt"
            },
            "is_answered": false,
            "view_count": 51,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526427704,
            "creation_date": 1515429214,
            "last_edit_date": 1526427704,
            "question_id": 48154393,
            "body_markdown": "We&#39;re getting really slow PutRecordBatch writes to all our Kinesis firehose Streams in EU-West-1. This has been happenign on and off for several days. \r\n\r\nIt started as a slow running Lambda function so we&#39;ve looked at Lambda X-ray to see where the slow-down was occurring and its consistently write connections to put Firehose. Sometimes they succeed straightaway other times they take several seconds or even minutes to complete. We write to multiple Firehose streams and the slowdown seems to be random across the streams (see below).\r\n\r\n[![Lambda X-Ray][1]][1]\r\n\r\nAny help greatly appreciated. It feels like we&#39;re hitting some kind of limit that is rate throttling our requests???\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/vkMS7.png",
            "link": "https://stackoverflow.com/questions/48154393/slow-putrecordbatch-on-kinesis-firehose",
            "title": "Slow PutRecordBatch on Kinesis Firehose",
            "body": "<p>We're getting really slow PutRecordBatch writes to all our Kinesis firehose Streams in EU-West-1. This has been happenign on and off for several days. </p>\n\n<p>It started as a slow running Lambda function so we've looked at Lambda X-ray to see where the slow-down was occurring and its consistently write connections to put Firehose. Sometimes they succeed straightaway other times they take several seconds or even minutes to complete. We write to multiple Firehose streams and the slowdown seems to be random across the streams (see below).</p>\n\n<p><a href=\"https://i.stack.imgur.com/vkMS7.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/vkMS7.png\" alt=\"Lambda X-Ray\"></a></p>\n\n<p>Any help greatly appreciated. It feels like we're hitting some kind of limit that is rate throttling our requests???</p>\n"
        },
        {
            "tags": [
                "android",
                "android-espresso"
            ],
            "owner": {
                "reputation": 2636,
                "user_id": 1084749,
                "user_type": "registered",
                "accept_rate": 80,
                "profile_image": "https://i.stack.imgur.com/nJx3J.png?s=128&g=1",
                "display_name": "Eduard",
                "link": "https://stackoverflow.com/users/1084749/eduard"
            },
            "is_answered": true,
            "view_count": 10771,
            "accepted_answer_id": 20915473,
            "answer_count": 7,
            "score": 9,
            "last_activity_date": 1526427701,
            "creation_date": 1386384514,
            "last_edit_date": 1406670508,
            "question_id": 20436968,
            "body_markdown": "I&#39;m trying to type some text inside an EditText:\r\n\r\n&lt;pre&gt;&lt;code&gt;\r\n\tpublic void testSearch() {\r\n          onView(withId(R.id.titleInput)).perform(typeText(&quot;Engineer&quot;));\r\n          onView(withId(R.id.titleInput)).check(matches(withText(&quot;Engineer&quot;)));\r\n    }\r\n&lt;/pre&gt;&lt;/code&gt;\r\n\r\nI see the EditText is getting focus, but nothing happens. No text is typed.\r\n&lt;br&gt;\r\nI tried this on a phone and an emulator - same result.\r\n",
            "link": "https://stackoverflow.com/questions/20436968/espresso-typetext-not-working",
            "title": "Espresso - typeText not working",
            "body": "<p>I'm trying to type some text inside an EditText:</p>\n\n<p><pre><code>\n    public void testSearch() {\n          onView(withId(R.id.titleInput)).perform(typeText(\"Engineer\"));\n          onView(withId(R.id.titleInput)).check(matches(withText(\"Engineer\")));\n    }\n</pre></code></p>\n\n<p>I see the EditText is getting focus, but nothing happens. No text is typed.\n<br>\nI tried this on a phone and an emulator - same result.</p>\n"
        },
        {
            "tags": [
                "sql",
                "sql-server-2008"
            ],
            "owner": {
                "reputation": 485,
                "user_id": 222617,
                "user_type": "registered",
                "accept_rate": 84,
                "profile_image": "https://www.gravatar.com/avatar/8bec14ce7ba3f4b8384670b1729a10c4?s=128&d=identicon&r=PG",
                "display_name": "sajad",
                "link": "https://stackoverflow.com/users/222617/sajad"
            },
            "is_answered": true,
            "view_count": 86206,
            "accepted_answer_id": 2744357,
            "answer_count": 1,
            "score": 5,
            "last_activity_date": 1526427695,
            "creation_date": 1272628504,
            "last_edit_date": 1272637270,
            "question_id": 2744280,
            "body_markdown": "I am trying to query from a temp table and i keep getting this message:\r\n\r\n    Msg 102, Level 15, State 1, Line 1 Incorrect syntax near &#39; &#39;.\r\n\r\nCan somebody tell me what the problem is?  Is it due to convert?\r\n\r\nThe query is \r\n\r\n    select compid,2, convert(datetime, &#39;01/01/&#39; + CONVERT(char(4),cal_yr) ,101) ,0,&#160; Update_dt, th1, th2, th3_pc , Update_id, Update_dt,1\r\n    from  #tmp_CTF** ",
            "link": "https://stackoverflow.com/questions/2744280/msg-102-level-15-state-1-line-1-incorrect-syntax-near",
            "title": "Msg 102, Level 15, State 1, Line 1 Incorrect syntax near &#39;&#160;&#39;",
            "body": "<p>I am trying to query from a temp table and i keep getting this message:</p>\n\n<pre><code>Msg 102, Level 15, State 1, Line 1 Incorrect syntax near ' '.\n</code></pre>\n\n<p>Can somebody tell me what the problem is?  Is it due to convert?</p>\n\n<p>The query is </p>\n\n<pre><code>select compid,2, convert(datetime, '01/01/' + CONVERT(char(4),cal_yr) ,101) ,0,  Update_dt, th1, th2, th3_pc , Update_id, Update_dt,1\nfrom  #tmp_CTF** \n</code></pre>\n"
        },
        {
            "tags": [
                "sql",
                "cloudera"
            ],
            "owner": {
                "reputation": 36,
                "user_id": 6612915,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/97808d4b61bf9bc8affacbe2f5f80ddf?s=128&d=identicon&r=PG&f=1",
                "display_name": "Anna ",
                "link": "https://stackoverflow.com/users/6612915/anna"
            },
            "is_answered": false,
            "view_count": 7,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526427686,
            "creation_date": 1526427686,
            "question_id": 50360570,
            "body_markdown": "How can I write in sql cloudera \r\n\r\n    And \r\n    t1. segment_start_time\r\n\r\nis between / occurs within\r\n\r\n    (t2.begin_date_time and\r\n    t2.end_date_time)",
            "link": "https://stackoverflow.com/questions/50360570/within-a-specific-time-frame-in-sql-cloudera",
            "title": "within a specific time frame in sql cloudera",
            "body": "<p>How can I write in sql cloudera </p>\n\n<pre><code>And \nt1. segment_start_time\n</code></pre>\n\n<p>is between / occurs within</p>\n\n<pre><code>(t2.begin_date_time and\nt2.end_date_time)\n</code></pre>\n"
        },
        {
            "tags": [
                "android",
                "process"
            ],
            "owner": {
                "reputation": 43,
                "user_id": 881259,
                "user_type": "registered",
                "accept_rate": 20,
                "profile_image": "https://www.gravatar.com/avatar/74f7eae6f10c1f39bade016b6bc3ace4?s=128&d=identicon&r=PG&f=1",
                "display_name": "user881259",
                "link": "https://stackoverflow.com/users/881259/user881259"
            },
            "is_answered": true,
            "view_count": 27372,
            "accepted_answer_id": 9282254,
            "answer_count": 3,
            "score": 5,
            "last_activity_date": 1526427669,
            "creation_date": 1329240943,
            "question_id": 9281543,
            "body_markdown": "How to change process priority in Android? I have found process description and description of it&#39;s priorities in android docs\r\n\r\n[process and thread description][1]\r\n\r\n[process priorities][2]\r\n\r\nbut I have found nothing on changing process priority by some method.\r\n\r\nps: I need this to test only some feature of application which is connected with process priority and it won&#39;t be used in program.\r\n\r\nThe only way I see is to try to change the priority via **importance** field, but it&#39;s a bad idea I guess.\r\n\r\n\r\n  [1]: http://developer.android.com/guide/topics/fundamentals/processes-and-threads.html\r\n  [2]: http://developer.android.com/reference/android/app/ActivityManager.RunningAppProcessInfo.html#IMPORTANCE_BACKGROUND",
            "link": "https://stackoverflow.com/questions/9281543/change-process-priority-in-android",
            "title": "change process priority in android",
            "body": "<p>How to change process priority in Android? I have found process description and description of it's priorities in android docs</p>\n\n<p><a href=\"http://developer.android.com/guide/topics/fundamentals/processes-and-threads.html\" rel=\"noreferrer\">process and thread description</a></p>\n\n<p><a href=\"http://developer.android.com/reference/android/app/ActivityManager.RunningAppProcessInfo.html#IMPORTANCE_BACKGROUND\" rel=\"noreferrer\">process priorities</a></p>\n\n<p>but I have found nothing on changing process priority by some method.</p>\n\n<p>ps: I need this to test only some feature of application which is connected with process priority and it won't be used in program.</p>\n\n<p>The only way I see is to try to change the priority via <strong>importance</strong> field, but it's a bad idea I guess.</p>\n"
        },
        {
            "tags": [
                "amazon-kinesis-firehose"
            ],
            "owner": {
                "reputation": 169,
                "user_id": 1610140,
                "user_type": "registered",
                "accept_rate": 87,
                "profile_image": "https://www.gravatar.com/avatar/5dd32729f769758003a5e77256c7b324?s=128&d=identicon&r=PG",
                "display_name": "Joseph McCarthy",
                "link": "https://stackoverflow.com/users/1610140/joseph-mccarthy"
            },
            "is_answered": false,
            "view_count": 63,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526427655,
            "creation_date": 1522158301,
            "last_edit_date": 1526427655,
            "question_id": 49514613,
            "body_markdown": "I&#39;m processing an XML file added to S3 and writing the results to a firehose, and storing the results on the same S3 bucket, but the destination filename has to be in a specific format. I&#39;ve examing the documentation and I can&#39;t see any way to set the format of the filename.\r\nThe closest I can find is in the [firehose FAQ][1]\r\n\r\n&gt; **Q: What is the naming pattern of the Amazon S3 objects delivered by Amazon Kinesis Data Firehose?**\r\n\r\n&gt; The Amazon S3 object name follows the pattern DeliveryStreamName-DeliveryStreamVersion-YYYY-MM-DD-HH-MM-SS-RandomString, where DeliveryStreamVersion begins with 1 and increases by 1 for every configuration change of the delivery stream. You can change delivery stream configurations (for example, the name of the S3 bucket, buffering hints, compression, and encryption) with the Firehose Console or the UpdateDestination operation.\r\n\r\n\r\n  [1]: https://aws.amazon.com/kinesis/data-firehose/faqs/",
            "link": "https://stackoverflow.com/questions/49514613/how-can-i-set-the-destination-filename-for-aws-firehose-on-s3",
            "title": "How can I set the destination filename for AWS Firehose on S3?",
            "body": "<p>I'm processing an XML file added to S3 and writing the results to a firehose, and storing the results on the same S3 bucket, but the destination filename has to be in a specific format. I've examing the documentation and I can't see any way to set the format of the filename.\nThe closest I can find is in the <a href=\"https://aws.amazon.com/kinesis/data-firehose/faqs/\" rel=\"nofollow noreferrer\">firehose FAQ</a></p>\n\n<blockquote>\n  <p><strong>Q: What is the naming pattern of the Amazon S3 objects delivered by Amazon Kinesis Data Firehose?</strong></p>\n  \n  <p>The Amazon S3 object name follows the pattern DeliveryStreamName-DeliveryStreamVersion-YYYY-MM-DD-HH-MM-SS-RandomString, where DeliveryStreamVersion begins with 1 and increases by 1 for every configuration change of the delivery stream. You can change delivery stream configurations (for example, the name of the S3 bucket, buffering hints, compression, and encryption) with the Firehose Console or the UpdateDestination operation.</p>\n</blockquote>\n"
        },
        {
            "tags": [
                "python",
                "c",
                "gtk",
                "pygobject"
            ],
            "owner": {
                "reputation": 54,
                "user_id": 4010066,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/3fac3a8d1a7f936df8164c41bb9aebdd?s=128&d=identicon&r=PG&f=1",
                "display_name": "Adam Solchenberger",
                "link": "https://stackoverflow.com/users/4010066/adam-solchenberger"
            },
            "is_answered": false,
            "view_count": 12,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526427645,
            "creation_date": 1526427645,
            "question_id": 50360564,
            "body_markdown": "I&#39;m interested in being able to mix python and C code with GTK+. I&#39;m capable of creating shared object libs in C and accessing them from python. I&#39;m wondering how to gain access to the GtkWidget pointer from a pygobject class and pass that into the C function. Another possibility is to create the GtkWidget in a C function, but I would still need a pointer to the parent widget I want to pack_start to etc. I would like to do openGl code in C for the GLArea widget and pack it into a pygobject Gtk.Box(). I know it&#39;s possible to use the python wrapper for openGl. ",
            "link": "https://stackoverflow.com/questions/50360564/mixing-pygobject-python-wrapper-and-c-gtk",
            "title": "mixing pygobject (python wrapper) and C GTK+",
            "body": "<p>I'm interested in being able to mix python and C code with GTK+. I'm capable of creating shared object libs in C and accessing them from python. I'm wondering how to gain access to the GtkWidget pointer from a pygobject class and pass that into the C function. Another possibility is to create the GtkWidget in a C function, but I would still need a pointer to the parent widget I want to pack_start to etc. I would like to do openGl code in C for the GLArea widget and pack it into a pygobject Gtk.Box(). I know it's possible to use the python wrapper for openGl. </p>\n"
        },
        {
            "tags": [
                "ubuntu",
                "tkinter",
                "ubuntu-18.04"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 8799703,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-ttf9QTBfizQ/AAAAAAAAAAI/AAAAAAAAEAw/LyK8iH4WzS8/photo.jpg?sz=128",
                "display_name": "Midhun S",
                "link": "https://stackoverflow.com/users/8799703/midhun-s"
            },
            "is_answered": false,
            "view_count": 21,
            "answer_count": 1,
            "score": -1,
            "last_activity_date": 1526427636,
            "creation_date": 1526401388,
            "question_id": 50355026,
            "body_markdown": "I&#39;m using Ubuntu 18.04 lts. It keep saying me that tk is not defined . Is there a way to install tkinter in Ubuntu . Pls help me",
            "link": "https://stackoverflow.com/questions/50355026/tk-not-defined-in-tkinter-ubuntu",
            "title": "Tk not defined in tkinter ubuntu",
            "body": "<p>I'm using Ubuntu 18.04 lts. It keep saying me that tk is not defined . Is there a way to install tkinter in Ubuntu . Pls help me</p>\n"
        },
        {
            "tags": [
                "python",
                "django",
                "csv",
                "django-models"
            ],
            "owner": {
                "reputation": 6,
                "user_id": 9796791,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/917bdac456be1f34552b2585b8f13716?s=128&d=identicon&r=PG&f=1",
                "display_name": "Martin",
                "link": "https://stackoverflow.com/users/9796791/martin"
            },
            "is_answered": false,
            "view_count": 23,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1526427632,
            "creation_date": 1526420723,
            "last_edit_date": 1526427632,
            "question_id": 50359589,
            "body_markdown": "I&#39;m currently working on a web app, and I&#39;m using config files to upload the names for certain models.\r\n\r\nHow would I go about associating these created models to a `ForeignKey` associated to a parent model class that already exists in a different app?\r\n\r\nFor example, I have the following models:\r\n    \r\n    # Base models.py\r\n    class Parent(models.Model):\r\n        name = models.CharField()\r\n    \r\n    # Child models.py\r\n    class child(models.Model):\r\n        name = models.CharField()\r\n        parent = models.foreignKey(&#39;Parent&#39;, on_delete = models.CASCADE)\r\n\r\nThe first base models will already exist within the database and will be used in a previous view. Then in the current views I&#39;ll call a function that will get or create a certain amount of child models, based on the number of rows in the csv file, and pass these generated models into the html template.\r\n\r\nWhat I&#39;m not sure how to do is making sure that the models created in the function will have the appropriate foreign key that associates it to the right specific model used in the previous view.\r\n",
            "link": "https://stackoverflow.com/questions/50359589/django-how-to-create-a-new-model-with-a-foreign-key-associated-to-a-previously",
            "title": "Django: How to create a new model with a foreign key associated to a previously visited model",
            "body": "<p>I'm currently working on a web app, and I'm using config files to upload the names for certain models.</p>\n\n<p>How would I go about associating these created models to a <code>ForeignKey</code> associated to a parent model class that already exists in a different app?</p>\n\n<p>For example, I have the following models:</p>\n\n<pre><code># Base models.py\nclass Parent(models.Model):\n    name = models.CharField()\n\n# Child models.py\nclass child(models.Model):\n    name = models.CharField()\n    parent = models.foreignKey('Parent', on_delete = models.CASCADE)\n</code></pre>\n\n<p>The first base models will already exist within the database and will be used in a previous view. Then in the current views I'll call a function that will get or create a certain amount of child models, based on the number of rows in the csv file, and pass these generated models into the html template.</p>\n\n<p>What I'm not sure how to do is making sure that the models created in the function will have the appropriate foreign key that associates it to the right specific model used in the previous view.</p>\n"
        },
        {
            "tags": [
                "c#",
                "override",
                "new-operator"
            ],
            "owner": {
                "reputation": 1512,
                "user_id": 472875,
                "user_type": "registered",
                "accept_rate": 74,
                "profile_image": "https://www.gravatar.com/avatar/eeb995ee434a1d4769286a5198a86e74?s=128&d=identicon&r=PG",
                "display_name": "user472875",
                "link": "https://stackoverflow.com/users/472875/user472875"
            },
            "is_answered": true,
            "view_count": 1937,
            "closed_date": 1331061787,
            "accepted_answer_id": 9590275,
            "answer_count": 3,
            "score": 13,
            "last_activity_date": 1526427622,
            "creation_date": 1331061030,
            "last_edit_date": 1495540934,
            "question_id": 9590243,
            "body_markdown": "&gt; **Possible Duplicate:**  \n&gt; [C# keyword usage virtual+override vs. new](https://stackoverflow.com/questions/159978/c-sharp-keyword-usage-virtualoverride-vs-new)  \n&gt; [Difference between new and override?](https://stackoverflow.com/questions/7542238/difference-between-new-and-override)  \n\n&lt;!-- End of automatically inserted text --&gt;\n\nSo I&#39;ve been working on a project and decided to do some reading about the difference between the `new` and `override` keywords in C#.\r\n\r\nFrom what I saw, it seems that using the `new` keyword functionality is a great way to create bugs in the code. Apart from that I don&#39;t really see when it would actually make sense to use it. \r\n\r\nMore out of curiosity than anything, is there any pattern where the `new` keyword is the right way to go?",
            "link": "https://stackoverflow.com/questions/9590243/when-to-use-new-instead-of-override-c",
            "closed_reason": "exact duplicate",
            "title": "When to use new instead of override C#",
            "body": "<blockquote>\n  <p><strong>Possible Duplicate:</strong><br>\n  <a href=\"https://stackoverflow.com/questions/159978/c-sharp-keyword-usage-virtualoverride-vs-new\">C# keyword usage virtual+override vs. new</a><br>\n  <a href=\"https://stackoverflow.com/questions/7542238/difference-between-new-and-override\">Difference between new and override?</a>  </p>\n</blockquote>\n\n\n\n<p>So I've been working on a project and decided to do some reading about the difference between the <code>new</code> and <code>override</code> keywords in C#.</p>\n\n<p>From what I saw, it seems that using the <code>new</code> keyword functionality is a great way to create bugs in the code. Apart from that I don't really see when it would actually make sense to use it. </p>\n\n<p>More out of curiosity than anything, is there any pattern where the <code>new</code> keyword is the right way to go?</p>\n"
        },
        {
            "tags": [
                "git",
                "github",
                "push",
                "branch"
            ],
            "owner": {
                "reputation": 23,
                "user_id": 7180157,
                "user_type": "registered",
                "accept_rate": 0,
                "profile_image": "https://www.gravatar.com/avatar/373548eaa41b2898529ee403ba8d31ff?s=128&d=identicon&r=PG&f=1",
                "display_name": "Person_Object",
                "link": "https://stackoverflow.com/users/7180157/person-object"
            },
            "is_answered": true,
            "view_count": 29,
            "closed_date": 1526594626,
            "accepted_answer_id": 50360556,
            "answer_count": 4,
            "score": 0,
            "last_activity_date": 1526427544,
            "creation_date": 1526426797,
            "question_id": 50360470,
            "body_markdown": "Super nooby question but....\r\n\r\n\r\n - So I&#39;m currently in the master branch and have made some changes locally.\r\n - I don&#39;t want my changes to affect master branch but only my branch.\r\n - How can I commit and push changes made to a branch without ever affecting the master branch?",
            "link": "https://stackoverflow.com/questions/50360470/how-to-git-push-to-branch-when-work-is-done-in-master",
            "closed_reason": "duplicate",
            "title": "How to git push to branch when work is done in master?",
            "body": "<p>Super nooby question but....</p>\n\n<ul>\n<li>So I'm currently in the master branch and have made some changes locally.</li>\n<li>I don't want my changes to affect master branch but only my branch.</li>\n<li>How can I commit and push changes made to a branch without ever affecting the master branch?</li>\n</ul>\n"
        },
        {
            "tags": [
                "google-chrome",
                "hyperlink",
                "menu"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9797104,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/ca0634b5ec7a1da37565a4592def7925?s=128&d=identicon&r=PG&f=1",
                "display_name": "Giovanni HexaNet",
                "link": "https://stackoverflow.com/users/9797104/giovanni-hexanet"
            },
            "is_answered": false,
            "view_count": 10,
            "answer_count": 0,
            "score": -1,
            "last_activity_date": 1526427539,
            "creation_date": 1526427539,
            "question_id": 50360555,
            "body_markdown": "I’m new here :)&lt;br&gt;\r\nI have a problem with my code.&lt;br&gt;\r\n\r\n    &lt;div class=&quot;collapse navbar-collapse navbar-ex1-collapse&quot;&gt;\r\n\t\t&lt;ul class=&quot;nav navbar-nav navbar-right&quot;&gt;\r\n\t\t\t&lt;li class=&quot;active&quot;&gt;&lt;a href=&quot;#top-section&quot;&gt;Home&lt;/a&gt;&lt;/li&gt;\r\n\t\t\t&lt;li&gt;&lt;a href=&quot;#Section-1&quot;&gt;Assortiment&lt;/a&gt;&lt;/li&gt;\r\n\t\t\t&lt;li&gt;&lt;a href=&quot;#Section-2&quot;&gt;Over ons&lt;/a&gt;&lt;/li&gt;\r\n\t\t\t&lt;li&gt;&lt;a href=&quot;#Section-3&quot;&gt;Contact&lt;/a&gt;&lt;/li&gt;\r\n\t\t\t&lt;li&gt;&lt;a href=&quot;#Section-4&quot;&gt;Partners&lt;/a&gt;&lt;/li&gt;\r\n\t\t&lt;/ul&gt;\r\n\t&lt;/div&gt;\r\n\r\nThese menu links don’t open when you click on them in Google Chrome. I also tested it on Internet Explorer and it had no problem at all. Can anyone help me with this? Thanks!",
            "link": "https://stackoverflow.com/questions/50360555/menu-links-don-t-work-in-google-chrome",
            "title": "Menu links don’t work in Google Chrome",
            "body": "<p>I’m new here :)<br>\nI have a problem with my code.<br></p>\n\n<pre><code>&lt;div class=\"collapse navbar-collapse navbar-ex1-collapse\"&gt;\n    &lt;ul class=\"nav navbar-nav navbar-right\"&gt;\n        &lt;li class=\"active\"&gt;&lt;a href=\"#top-section\"&gt;Home&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"#Section-1\"&gt;Assortiment&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"#Section-2\"&gt;Over ons&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"#Section-3\"&gt;Contact&lt;/a&gt;&lt;/li&gt;\n        &lt;li&gt;&lt;a href=\"#Section-4\"&gt;Partners&lt;/a&gt;&lt;/li&gt;\n    &lt;/ul&gt;\n&lt;/div&gt;\n</code></pre>\n\n<p>These menu links don’t open when you click on them in Google Chrome. I also tested it on Internet Explorer and it had no problem at all. Can anyone help me with this? Thanks!</p>\n"
        },
        {
            "tags": [
                "angular",
                "debugging",
                "visual-studio-code",
                "vscode-debugger"
            ],
            "owner": {
                "reputation": 1436,
                "user_id": 460143,
                "user_type": "registered",
                "accept_rate": 0,
                "profile_image": "https://www.gravatar.com/avatar/45816cfef501bf8acf7942abef7e4d24?s=128&d=identicon&r=PG",
                "display_name": "Maxxx",
                "link": "https://stackoverflow.com/users/460143/maxxx"
            },
            "is_answered": false,
            "view_count": 41,
            "answer_count": 0,
            "score": 2,
            "last_activity_date": 1526427533,
            "creation_date": 1526427533,
            "question_id": 50360554,
            "body_markdown": "I Created a new Angular 6 CLI project\r\n\r\n    ng new myProjects\r\n\r\nCreated &#39;sub project&#39;\r\n\r\n    ng g mySubProject\r\n\r\nIf I ng serve mySubProject then try to debug from VS Code using my normal launch.json, then breakpoints are not hit.\r\n\r\n    {\r\n      &quot;name&quot;: &quot;Launch Chrome (test)&quot;,\r\n      &quot;type&quot;: &quot;chrome&quot;,\r\n      &quot;request&quot;: &quot;launch&quot;,\r\n      &quot;url&quot;: &quot;http://localhost:4200/&quot;,\r\n      &quot;webRoot&quot;: &quot;${workspaceFolder}&quot;\r\n    },\r\n\r\nCan someone guide me how to set up my launch.json to debug sub-projects like this?\r\n\r\n(for details on how I have my sub projects set up, it&#39;s based on a post\r\n[here][1] )\r\n\r\nIf I just ng serve, then this launch.json debigs the &#39;main&#39; project OK - so I am guessing I need to set up somewhere in the launch.json to tell it where the child project is?\r\n\r\n  [1]: https://medium.com/@SirMaxxx/angular-6-creating-a-shareable-control-library-6a27f0ebe5c2",
            "link": "https://stackoverflow.com/questions/50360554/how-to-debug-angular-6-project-from-vs-code",
            "title": "How to debug Angular 6 project from VS Code",
            "body": "<p>I Created a new Angular 6 CLI project</p>\n\n<pre><code>ng new myProjects\n</code></pre>\n\n<p>Created 'sub project'</p>\n\n<pre><code>ng g mySubProject\n</code></pre>\n\n<p>If I ng serve mySubProject then try to debug from VS Code using my normal launch.json, then breakpoints are not hit.</p>\n\n<pre><code>{\n  \"name\": \"Launch Chrome (test)\",\n  \"type\": \"chrome\",\n  \"request\": \"launch\",\n  \"url\": \"http://localhost:4200/\",\n  \"webRoot\": \"${workspaceFolder}\"\n},\n</code></pre>\n\n<p>Can someone guide me how to set up my launch.json to debug sub-projects like this?</p>\n\n<p>(for details on how I have my sub projects set up, it's based on a post\n<a href=\"https://medium.com/@SirMaxxx/angular-6-creating-a-shareable-control-library-6a27f0ebe5c2\" rel=\"nofollow noreferrer\">here</a> )</p>\n\n<p>If I just ng serve, then this launch.json debigs the 'main' project OK - so I am guessing I need to set up somewhere in the launch.json to tell it where the child project is?</p>\n"
        },
        {
            "tags": [
                "firebase",
                "dart",
                "google-cloud-firestore",
                "flutter"
            ],
            "owner": {
                "reputation": 85,
                "user_id": 4945902,
                "user_type": "registered",
                "accept_rate": 40,
                "profile_image": "https://www.gravatar.com/avatar/20cd2b4b26e837585feedf43153069da?s=128&d=identicon&r=PG&f=1",
                "display_name": "Johann Feser",
                "link": "https://stackoverflow.com/users/4945902/johann-feser"
            },
            "is_answered": false,
            "view_count": 87,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526427507,
            "creation_date": 1525364605,
            "last_edit_date": 1525438710,
            "question_id": 50159766,
            "body_markdown": "I want to build an App using Flutter and Firebase (Firestore version cloud_firestore: ^0.6.3).\r\nWhen loading a Collection from Firestore using StreamBuilder to display it in a ListView \r\nI get an error saying:\r\n\r\nThe following assertion was thrown building StreamBuilder&lt;QuerySnapshot&gt;(dirty, state:\r\nI/flutter (26287): _StreamBuilderBaseState&lt;QuerySnapshot, AsyncSnapshot&lt;QuerySnapshot&gt;&gt;#d5638):\r\n\r\nI/flutter (26287): type &#39;_InternalLinkedHashMap&lt;dynamic, dynamic&gt;&#39; is not a subtype of type &#39;Map&lt;String, dynamic&gt;&#39;\r\n\r\nI/flutter (26287): where\r\n\r\nI/flutter (26287):   _InternalLinkedHashMap is from dart:collection\r\n\r\nI/flutter (26287):   Map is from dart:core\r\n\r\nI/flutter (26287):   String is from dart:core\r\n\r\nI/flutter (26287): \r\n\r\nAnd I can&#39;t get my head around because even when removing the lines the use the \r\nDocumentSnapshots it throws the error.\r\n\r\nThis is how I want to acces the filed in the Snapshots.\r\n\r\n    Creator.fromDoc(DocumentSnapshot doc) {\r\n        this.creatorId = doc[&#39;creatorId&#39;];\r\n        this.name = doc[&#39;name&#39;];\r\n     }\r\n\r\nOr\r\n\r\n    Creator.fromMap(doc.data);\r\n    \r\n    Creator.fromMap(Map&lt;String, dynamic&gt; map) {\r\n        if (map.containsKey(&#39;creatorId&#39;)) {\r\n          this.creatorId = map[&#39;creatorId&#39;];\r\n        }\r\n        if (map.containsKey(&#39;name&#39;)) {\r\n          this.name = map[&#39;name&#39;];\r\n        }\r\n      }\r\n\r\nThe Layout (Without the calls from above but even this throws the error).\r\nThis is partly copied from the firestore example.\r\n   \r\n\r\n     return new Scaffold(\r\n                appBar: new AppBar(title: new Text(&#39;Creator&#39;)),\r\n                body: new StreamBuilder&lt;QuerySnapshot&gt;(\r\n                    stream: CreatorRepo.getAllCreator(),\r\n                    builder:\r\n                        (BuildContext context, AsyncSnapshot&lt;QuerySnapshot&gt; snapshot) {\r\n                      if (!snapshot.hasData) return new Text(&#39;Loading...&#39;);\r\n                      return new ListView(\r\n                        children: snapshot.data.documents.map&lt;Widget&gt;((DocumentSnapshot doc){\r\n                          return new ExpansionTile(\r\n                            title: new Text(doc[&#39;name&#39;]),\r\n                            children: &lt;Widget&gt;[\r\n                              new Text(doc[&#39;creatorId&#39;])\r\n                            ],\r\n                          );\r\n                        }).toList(),\r\n                      );\r\n                    }));\r\n\r\nEdit:\r\nDependencies:\r\n\r\n    dependencies:\r\n      flutter:\r\n        sdk: flutter\r\n      cloud_firestore: ^0.6.3\r\n      firebase_messaging: ^0.2.4\r\n      cupertino_icons: ^0.1.0\r\n    \r\n    dev_dependencies:\r\n      flutter_test:\r\n        sdk: flutter\r\n    flutter:\r\n      uses-material-design: true\r\n\r\n\r\nflutter clean helped but resulted in another error:\r\n\r\n    Note: C:\\Program Files\\flutter\\.pub-cache\\hosted\\pub.dartlang.org\\firebase_core-0.2.3\\android\\src\\main\\java\\io\\flutter\\plugins\\firebase\\core\\FirebaseCorePlugin.java uses unchecked or unsafe operations.\r\n    Note: Recompile with -Xlint:unchecked for details.\r\n\r\nThis error appears to have no effect on the app though.\r\n\r\nWhen I started this project I checked the Firestore code and the data object inside the DocumentSnapshot was of type Map &lt; String, dynamic &gt; so I thought that every map inside an object would be of that type.\r\nBut to acces these sub Maps of an object you need to assume that that Map is of type Map &lt; dynamic,dynamic &gt;. Everytime I tried to acces a Map with \r\nMap &lt; String,dynamic &gt; it would crash changing this to o acces these sub Maps of an object you need to assume that that Map is of type Map &lt; dynamic,dynamic &gt; stoped the crashes for now.",
            "link": "https://stackoverflow.com/questions/50159766/flutter-mapstring-dynamic-error",
            "title": "Flutter Map&lt;String, dynamic&gt; error",
            "body": "<p>I want to build an App using Flutter and Firebase (Firestore version cloud_firestore: ^0.6.3).\nWhen loading a Collection from Firestore using StreamBuilder to display it in a ListView \nI get an error saying:</p>\n\n<p>The following assertion was thrown building StreamBuilder(dirty, state:\nI/flutter (26287): _StreamBuilderBaseState>#d5638):</p>\n\n<p>I/flutter (26287): type '_InternalLinkedHashMap' is not a subtype of type 'Map'</p>\n\n<p>I/flutter (26287): where</p>\n\n<p>I/flutter (26287):   _InternalLinkedHashMap is from dart:collection</p>\n\n<p>I/flutter (26287):   Map is from dart:core</p>\n\n<p>I/flutter (26287):   String is from dart:core</p>\n\n<p>I/flutter (26287): </p>\n\n<p>And I can't get my head around because even when removing the lines the use the \nDocumentSnapshots it throws the error.</p>\n\n<p>This is how I want to acces the filed in the Snapshots.</p>\n\n<pre><code>Creator.fromDoc(DocumentSnapshot doc) {\n    this.creatorId = doc['creatorId'];\n    this.name = doc['name'];\n }\n</code></pre>\n\n<p>Or</p>\n\n<pre><code>Creator.fromMap(doc.data);\n\nCreator.fromMap(Map&lt;String, dynamic&gt; map) {\n    if (map.containsKey('creatorId')) {\n      this.creatorId = map['creatorId'];\n    }\n    if (map.containsKey('name')) {\n      this.name = map['name'];\n    }\n  }\n</code></pre>\n\n<p>The Layout (Without the calls from above but even this throws the error).\nThis is partly copied from the firestore example.</p>\n\n<pre><code> return new Scaffold(\n            appBar: new AppBar(title: new Text('Creator')),\n            body: new StreamBuilder&lt;QuerySnapshot&gt;(\n                stream: CreatorRepo.getAllCreator(),\n                builder:\n                    (BuildContext context, AsyncSnapshot&lt;QuerySnapshot&gt; snapshot) {\n                  if (!snapshot.hasData) return new Text('Loading...');\n                  return new ListView(\n                    children: snapshot.data.documents.map&lt;Widget&gt;((DocumentSnapshot doc){\n                      return new ExpansionTile(\n                        title: new Text(doc['name']),\n                        children: &lt;Widget&gt;[\n                          new Text(doc['creatorId'])\n                        ],\n                      );\n                    }).toList(),\n                  );\n                }));\n</code></pre>\n\n<p>Edit:\nDependencies:</p>\n\n<pre><code>dependencies:\n  flutter:\n    sdk: flutter\n  cloud_firestore: ^0.6.3\n  firebase_messaging: ^0.2.4\n  cupertino_icons: ^0.1.0\n\ndev_dependencies:\n  flutter_test:\n    sdk: flutter\nflutter:\n  uses-material-design: true\n</code></pre>\n\n<p>flutter clean helped but resulted in another error:</p>\n\n<pre><code>Note: C:\\Program Files\\flutter\\.pub-cache\\hosted\\pub.dartlang.org\\firebase_core-0.2.3\\android\\src\\main\\java\\io\\flutter\\plugins\\firebase\\core\\FirebaseCorePlugin.java uses unchecked or unsafe operations.\nNote: Recompile with -Xlint:unchecked for details.\n</code></pre>\n\n<p>This error appears to have no effect on the app though.</p>\n\n<p>When I started this project I checked the Firestore code and the data object inside the DocumentSnapshot was of type Map &lt; String, dynamic > so I thought that every map inside an object would be of that type.\nBut to acces these sub Maps of an object you need to assume that that Map is of type Map &lt; dynamic,dynamic >. Everytime I tried to acces a Map with \nMap &lt; String,dynamic > it would crash changing this to o acces these sub Maps of an object you need to assume that that Map is of type Map &lt; dynamic,dynamic > stoped the crashes for now.</p>\n"
        },
        {
            "tags": [
                "amazon-redshift",
                "amazon-kinesis",
                "mysqlbinlog",
                "amazon-kinesis-firehose"
            ],
            "owner": {
                "reputation": 16,
                "user_id": 8598021,
                "user_type": "registered",
                "profile_image": "https://lh6.googleusercontent.com/-F2A1wT42AUY/AAAAAAAAAAI/AAAAAAAAAJM/4Y38hS_xpkk/photo.jpg?sz=128",
                "display_name": "Luka Krstev",
                "link": "https://stackoverflow.com/users/8598021/luka-krstev"
            },
            "is_answered": true,
            "view_count": 488,
            "answer_count": 1,
            "score": 3,
            "last_activity_date": 1526427494,
            "creation_date": 1511164531,
            "last_edit_date": 1526427494,
            "question_id": 47387341,
            "body_markdown": "I&#39;m trying to figure out most efficient way to extract data from MySQL, transform it and load to Redshift in near real-time.\r\nCurrently we have overnight ETL process (using Pentaho) which last ~ 40min and we want to replace it with near real-time (mini batch 1-5 minutes).\r\n\r\nI found couple of tools for data extraction from MySQL binlog (i.e.http://maxwells-daemon.io) with connector to AWS Kineses and I plan to transform and join data with Apache Spark or AWS Lambda and write it to S3 and from there `COPY` command writing it to Redshift.\r\n\r\nDoes anybody have any suggestion or recommendation regarding this or similar solution?\r\nThanks in advance!\r\n  ",
            "link": "https://stackoverflow.com/questions/47387341/near-real-time-etl-from-mysql-to-redshift",
            "title": "Near real-time ETL from MySQL to Redshift",
            "body": "<p>I'm trying to figure out most efficient way to extract data from MySQL, transform it and load to Redshift in near real-time.\nCurrently we have overnight ETL process (using Pentaho) which last ~ 40min and we want to replace it with near real-time (mini batch 1-5 minutes).</p>\n\n<p>I found couple of tools for data extraction from MySQL binlog (i.e.<a href=\"http://maxwells-daemon.io\" rel=\"nofollow noreferrer\">http://maxwells-daemon.io</a>) with connector to AWS Kineses and I plan to transform and join data with Apache Spark or AWS Lambda and write it to S3 and from there <code>COPY</code> command writing it to Redshift.</p>\n\n<p>Does anybody have any suggestion or recommendation regarding this or similar solution?\nThanks in advance!</p>\n"
        },
        {
            "tags": [
                "angular",
                "angular4-router"
            ],
            "owner": {
                "reputation": 20,
                "user_id": 6810459,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/9b88fd03fa8e0edacb04ae6b7a3f613b?s=128&d=identicon&r=PG&f=1",
                "display_name": "Tamer Hussien",
                "link": "https://stackoverflow.com/users/6810459/tamer-hussien"
            },
            "is_answered": false,
            "view_count": 10,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526427493,
            "creation_date": 1526405357,
            "question_id": 50356126,
            "body_markdown": "I have angular tab \r\n\r\n    &lt;tab (select)=&quot;goTo(&#39;materials&#39;)&quot; id=&quot;materials&quot;&gt;&lt;/tab&gt;\r\n\r\ncomponent `goTo(url) {\r\n    this._router.navigate([url], { relativeTo: this.route });\r\n  }`\r\n\r\nin another tab &quot;general&quot; i have a link redirect to this &quot;material&quot; tab but when i click it it redirect to the right link and load the corresponding view but the tab name &quot;general&quot; hasn&#39;t  changed\r\n\r\n    &lt;i (click)=&quot;goto(&#39;../general&#39;)&quot;&gt;&lt;/i&gt;\r\n\r\ncomponent : `goto(tabname){\r\n      this.router.navigate([tabname], { relativeTo: this.route });\r\n    }`",
            "link": "https://stackoverflow.com/questions/50356126/tab-route-change",
            "title": "Tab Route Change",
            "body": "<p>I have angular tab </p>\n\n<pre><code>&lt;tab (select)=\"goTo('materials')\" id=\"materials\"&gt;&lt;/tab&gt;\n</code></pre>\n\n<p>component <code>goTo(url) {\n    this._router.navigate([url], { relativeTo: this.route });\n  }</code></p>\n\n<p>in another tab \"general\" i have a link redirect to this \"material\" tab but when i click it it redirect to the right link and load the corresponding view but the tab name \"general\" hasn't  changed</p>\n\n<pre><code>&lt;i (click)=\"goto('../general')\"&gt;&lt;/i&gt;\n</code></pre>\n\n<p>component : <code>goto(tabname){\n      this.router.navigate([tabname], { relativeTo: this.route });\n    }</code></p>\n"
        },
        {
            "tags": [
                "python-3.x",
                "matplotlib"
            ],
            "owner": {
                "reputation": 1009,
                "user_id": 1708550,
                "user_type": "registered",
                "profile_image": "https://i.stack.imgur.com/cfz1B.png?s=128&g=1",
                "display_name": "Blitzkoder",
                "link": "https://stackoverflow.com/users/1708550/blitzkoder"
            },
            "is_answered": true,
            "view_count": 29,
            "accepted_answer_id": 50360254,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526427486,
            "creation_date": 1526355765,
            "last_edit_date": 1526427486,
            "question_id": 50342003,
            "body_markdown": "I need to create a sequence of .pdf files where each .pdf contains a figure with five plots.\r\nAs I am going to include them in a LaTeX article, I wanted them all to be the same width and height so that each figure&#39;s corners are vertically aligned on both left and right sides.\r\n\r\nI thought this would be enough, but apparently not:\r\n\r\n    common_figsize=(6,5)\r\n\r\n    fig, ax = plt.subplots(figsize = common_figsize)\r\n    # five plots in a loop for the first figure.\r\n    # my_code()...\r\n    plt.savefig(&quot;Figure-1.pdf&quot;, transparent=True)\r\n    plt.close(fig)\r\n\r\n\r\n    fig, ax = plt.subplots(figsize = common_figsize)\r\n    # five plots in a loop for the new figure.\r\n    # my_code()...\r\n    plt.savefig(&quot;Figure-2.pdf&quot;, transparent=True)\r\n    plt.close(fig)\r\n\r\nIf I understand correctly, this does not do exactly what I want because of different scales originating from different yticks resolutions.\r\n\r\nFor both figures, pyplot is fed the same list for xticks.\r\nIn this case, it is a list of 50 values, from 1 to 50.\r\n\r\n    CHUNK_COUNT = 50\r\n    x_step = CHUNK_COUNT / 10\r\n    new_xticks = list(range(x_step, CHUNK_COUNT + x_step, x_step)) + [1]\r\n    plt.xticks(new_xticks)\r\n    ax.set_xlim(left=1, right=CHUNK_COUNT)\r\n\r\nThis creates both figures with an X-axis that goes from 1 to 50.\r\nSo far so good.\r\n\r\nHowever, I haven&#39;t figured out how to deal with the problem of yticks resolution.\r\nOne of the figures had less `yticks` than the other, so I overrode it to have as many ticks as the other:\r\n\r\n    # Add yticks to Figure 1.\r\n    y_divisor = 6\r\n    y_step = (100 - min_y_tick) / y_divisor\r\n    new_yticks = [min_y_tick + y_step * i for i in range(0, y_divisor + 1)]\r\n    plt.yticks(new_yticks)\r\n\r\nThis resulted in the following images:\r\n**(click on each to open in new tab to see that in fact the bounding square of each figure is different)**\r\n\r\nFigure 1:\r\n\r\n[![Figure 1 - edited to have as many yticks as Figure 2][1]][1]\r\n\r\nFigure 2:\r\n\r\n[![Figure 2 - unchanged yticks][2]][2]\r\n\r\nIn summary, I believe matplotlib is accepting the `figsize` parameter, but then rearranges plot elements to accommodate for different tick values and text lengths.\r\n\r\nIs it possible for it to operate in reverse? To change label and text rotations *automagically* so that the squares are absolutely the same length and height?\r\n\r\nApologies if this is a duplicate and thanks for the help.\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/fV8yI.png\r\n  [2]: https://i.stack.imgur.com/As8F4.png\r\n\r\n**EDIT:**\r\n\r\nFinally able to provide a minimal, complete and verifiable example.\r\nAmong the tests, I removed the custom `yticks` code and the problem still persists:\r\n\r\n\tfrom matplotlib.lines import Line2D\r\n\timport matplotlib.ticker as mtick\r\n\timport matplotlib.pyplot as plt\r\n\tfrom matplotlib import rc\r\n\t# activate latex text rendering\r\n\trc(&#39;text&#39;, usetex=True)\r\n\r\n\tfrom matplotlib import rcParams\r\n\trcParams.update({&#39;figure.autolayout&#39;: True})\r\n\r\n    CHUNK_COUNT = 50\r\n    common_figsize=(6,5)\r\n    plot_counter = 5\r\n\r\n    x_step = int(int(CHUNK_COUNT) / 10)\r\n    new_xticks = list(range(x_step, int(CHUNK_COUNT) + x_step, x_step)) + [1]\r\n\r\n    ##### Plot Figure 1\r\n    fig, ax = plt.subplots(figsize = common_figsize)\r\n\r\n    plt.ylabel(&quot;Summary of a simple YY axis&quot;)\r\n    plt.yticks(rotation=45)\r\n\r\n    ax.yaxis.set_major_formatter(mtick.PercentFormatter(is_latex=False))\r\n\r\n    for i in range(0, plot_counter):\r\n\r\n        xvals = range(1, CHUNK_COUNT + 1)\r\n\r\n        yvals = []\r\n        for j in xvals:\r\n            yvals.append(j + i)\r\n\r\n        plt.plot(xvals, yvals)\r\n\r\n    plt.xticks(new_xticks)\r\n    ax.set_xlim(left=1, right=int(CHUNK_COUNT))\r\n\r\n    plt.savefig(&quot;Figure_1.png&quot;, transparent=True)\r\n    plt.close(fig)\r\n\r\n    ##### Plot Figure 2\r\n    fig, ax = plt.subplots(figsize = common_figsize)\r\n\r\n    plt.ylabel(&quot;Summary of another YY axis&quot;)\r\n    plt.yticks(rotation=45)\r\n\r\n    ax.yaxis.set_major_formatter(mtick.PercentFormatter(is_latex=False))\r\n\r\n    for i in range(0, plot_counter):\r\n\r\n        xvals = range(1, CHUNK_COUNT + 1)\r\n\r\n        yvals = []\r\n        for j in xvals:\r\n            yvals.append((j + i) / 100)\r\n\r\n\r\n        plt.plot(xvals, yvals)\r\n\r\n    plt.xticks(new_xticks)\r\n    ax.set_xlim(left=1, right=int(CHUNK_COUNT))\r\n\r\n    plt.savefig(&quot;Figure_2.png&quot;, transparent=True)\r\n    plt.close(fig)",
            "link": "https://stackoverflow.com/questions/50342003/matplotlib-sequentially-creating-figures-with-the-same-size",
            "title": "Matplotlib - sequentially creating figures with the same size",
            "body": "<p>I need to create a sequence of .pdf files where each .pdf contains a figure with five plots.\nAs I am going to include them in a LaTeX article, I wanted them all to be the same width and height so that each figure's corners are vertically aligned on both left and right sides.</p>\n\n<p>I thought this would be enough, but apparently not:</p>\n\n<pre><code>common_figsize=(6,5)\n\nfig, ax = plt.subplots(figsize = common_figsize)\n# five plots in a loop for the first figure.\n# my_code()...\nplt.savefig(\"Figure-1.pdf\", transparent=True)\nplt.close(fig)\n\n\nfig, ax = plt.subplots(figsize = common_figsize)\n# five plots in a loop for the new figure.\n# my_code()...\nplt.savefig(\"Figure-2.pdf\", transparent=True)\nplt.close(fig)\n</code></pre>\n\n<p>If I understand correctly, this does not do exactly what I want because of different scales originating from different yticks resolutions.</p>\n\n<p>For both figures, pyplot is fed the same list for xticks.\nIn this case, it is a list of 50 values, from 1 to 50.</p>\n\n<pre><code>CHUNK_COUNT = 50\nx_step = CHUNK_COUNT / 10\nnew_xticks = list(range(x_step, CHUNK_COUNT + x_step, x_step)) + [1]\nplt.xticks(new_xticks)\nax.set_xlim(left=1, right=CHUNK_COUNT)\n</code></pre>\n\n<p>This creates both figures with an X-axis that goes from 1 to 50.\nSo far so good.</p>\n\n<p>However, I haven't figured out how to deal with the problem of yticks resolution.\nOne of the figures had less <code>yticks</code> than the other, so I overrode it to have as many ticks as the other:</p>\n\n<pre><code># Add yticks to Figure 1.\ny_divisor = 6\ny_step = (100 - min_y_tick) / y_divisor\nnew_yticks = [min_y_tick + y_step * i for i in range(0, y_divisor + 1)]\nplt.yticks(new_yticks)\n</code></pre>\n\n<p>This resulted in the following images:\n<strong>(click on each to open in new tab to see that in fact the bounding square of each figure is different)</strong></p>\n\n<p>Figure 1:</p>\n\n<p><a href=\"https://i.stack.imgur.com/fV8yI.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/fV8yI.png\" alt=\"Figure 1 - edited to have as many yticks as Figure 2\"></a></p>\n\n<p>Figure 2:</p>\n\n<p><a href=\"https://i.stack.imgur.com/As8F4.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/As8F4.png\" alt=\"Figure 2 - unchanged yticks\"></a></p>\n\n<p>In summary, I believe matplotlib is accepting the <code>figsize</code> parameter, but then rearranges plot elements to accommodate for different tick values and text lengths.</p>\n\n<p>Is it possible for it to operate in reverse? To change label and text rotations <em>automagically</em> so that the squares are absolutely the same length and height?</p>\n\n<p>Apologies if this is a duplicate and thanks for the help.</p>\n\n<p><strong>EDIT:</strong></p>\n\n<p>Finally able to provide a minimal, complete and verifiable example.\nAmong the tests, I removed the custom <code>yticks</code> code and the problem still persists:</p>\n\n<pre><code>from matplotlib.lines import Line2D\nimport matplotlib.ticker as mtick\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\n# activate latex text rendering\nrc('text', usetex=True)\n\nfrom matplotlib import rcParams\nrcParams.update({'figure.autolayout': True})\n\nCHUNK_COUNT = 50\ncommon_figsize=(6,5)\nplot_counter = 5\n\nx_step = int(int(CHUNK_COUNT) / 10)\nnew_xticks = list(range(x_step, int(CHUNK_COUNT) + x_step, x_step)) + [1]\n\n##### Plot Figure 1\nfig, ax = plt.subplots(figsize = common_figsize)\n\nplt.ylabel(\"Summary of a simple YY axis\")\nplt.yticks(rotation=45)\n\nax.yaxis.set_major_formatter(mtick.PercentFormatter(is_latex=False))\n\nfor i in range(0, plot_counter):\n\n    xvals = range(1, CHUNK_COUNT + 1)\n\n    yvals = []\n    for j in xvals:\n        yvals.append(j + i)\n\n    plt.plot(xvals, yvals)\n\nplt.xticks(new_xticks)\nax.set_xlim(left=1, right=int(CHUNK_COUNT))\n\nplt.savefig(\"Figure_1.png\", transparent=True)\nplt.close(fig)\n\n##### Plot Figure 2\nfig, ax = plt.subplots(figsize = common_figsize)\n\nplt.ylabel(\"Summary of another YY axis\")\nplt.yticks(rotation=45)\n\nax.yaxis.set_major_formatter(mtick.PercentFormatter(is_latex=False))\n\nfor i in range(0, plot_counter):\n\n    xvals = range(1, CHUNK_COUNT + 1)\n\n    yvals = []\n    for j in xvals:\n        yvals.append((j + i) / 100)\n\n\n    plt.plot(xvals, yvals)\n\nplt.xticks(new_xticks)\nax.set_xlim(left=1, right=int(CHUNK_COUNT))\n\nplt.savefig(\"Figure_2.png\", transparent=True)\nplt.close(fig)\n</code></pre>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "lambda",
                "elasticsearch-5",
                "aws-kinesis-firehose"
            ],
            "owner": {
                "reputation": 429,
                "user_id": 1216965,
                "user_type": "registered",
                "accept_rate": 60,
                "profile_image": "https://www.gravatar.com/avatar/83546302f158b9216ad4f4d92eb82ded?s=128&d=identicon&r=PG",
                "display_name": "neowulf33",
                "link": "https://stackoverflow.com/users/1216965/neowulf33"
            },
            "is_answered": false,
            "view_count": 234,
            "answer_count": 1,
            "score": 2,
            "last_activity_date": 1526427483,
            "creation_date": 1508777804,
            "last_edit_date": 1526427483,
            "question_id": 46894664,
            "body_markdown": "## TLDR\r\n\r\nThe lambda function is not able to index the firehose logs into the AWS managed ES due to an &quot;encoding problem&quot;.\r\n\r\n## Actual Error Response\r\n\r\nI do not get any error when I base64 encode a single `logEvent` from a firehose `record` and send the collected records to the AWS managed ES.\r\n\r\nSee the next section for more details.\r\n\r\nThe base 64 encoded compressed payload is being sent to ES as the resulting json transformation is too big for ES to index - [see this ES link](https://github.com/elastic/elasticsearch-net/issues/1209).\r\n\r\nI get the following error from the AWS managed ES:\r\n\r\n    {\r\n        &quot;deliveryStreamARN&quot;: &quot;arn:aws:firehose:us-west-2:*:deliverystream/*&quot;,\r\n        &quot;destination&quot;: &quot;arn:aws:es:us-west-2:*:domain/*&quot;,\r\n        &quot;deliveryStreamVersionId&quot;: 1,\r\n        &quot;message&quot;: &quot;The data could not be decoded as UTF-8&quot;,\r\n        &quot;errorCode&quot;: &quot;InvalidEncodingException&quot;,\r\n        &quot;processor&quot;: &quot;arn:aws:lambda:us-west-2:*:function:*&quot;\r\n      }\r\n\r\nIf the output record is not compressed, `the body size is too long` (as small as 14MB). Without compression and a simple base64 encoded payload, I get the following error in the Lambda logs:\r\n\r\n    {\r\n      &quot;type&quot;: &quot;mapper_parsing_exception&quot;,\r\n      &quot;reason&quot;: &quot;failed to parse&quot;,\r\n      &quot;caused_by&quot;: {\r\n        &quot;type&quot;: &quot;not_x_content_exception&quot;,\r\n        &quot;reason&quot;: &quot;Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes&quot;\r\n      }\r\n    }\r\n\r\n## Description\r\n\r\nI have Cloudwatch logs that are getting buffered by size / interval which gets fed into a Kinesis Firehose. The firehose transports the logs into a lambda function which transforms the log into a json record which should then send it over to the AWS managed Elasticsearch cluster.\r\n\r\nThe lambda function gets the following JSON structure:\r\n\r\n    {\r\n    \t&quot;invocationId&quot;: &quot;cf1306b5-2d3c-4886-b7be-b5bcf0a66ef3&quot;,\r\n    \t&quot;deliveryStreamArn&quot;: &quot;arn:aws:firehose:...&quot;,\r\n    \t&quot;region&quot;: &quot;us-west-2&quot;,\r\n    \t&quot;records&quot;: [{\r\n    \t\t&quot;recordId&quot;: &quot;49577998431243709525183749876652374166077260049460232194000000&quot;,\r\n    \t\t&quot;approximateArrivalTimestamp&quot;: 1508197563377,\r\n    \t\t&quot;data&quot;: &quot;some_compressed_data_in_base_64_encoding&quot;\r\n    \t}]\r\n    }\r\n\r\n\r\nThe lambda function then extracts `.records[].data` and decodes the data as base64 and decompresses the data which results in the following JSON:\r\n\r\n\r\n    {\r\n      &quot;messageType&quot;: &quot;DATA_MESSAGE&quot;,\r\n      &quot;owner&quot;: &quot;aws_account_number&quot;,\r\n      &quot;logGroup&quot;: &quot;some_cloudwatch_log_group_name&quot;,\r\n      &quot;logStream&quot;: &quot;i-0221b6ec01af47bfb&quot;,\r\n      &quot;subscriptionFilters&quot;: [\r\n        &quot;cloudwatch_log_subscription_filter_name&quot;\r\n      ],\r\n      &quot;logEvents&quot;: [\r\n        {\r\n          &quot;id&quot;: &quot;33633929427703365813575134502195362621356131219229245440&quot;,\r\n          &quot;timestamp&quot;: 1508197557000,\r\n          &quot;message&quot;: &quot;Oct 16 23:45:57 some_log_entry_1&quot;\r\n        },\r\n        {\r\n          &quot;id&quot;: &quot;33633929427703365813575134502195362621356131219229245441&quot;,\r\n          &quot;timestamp&quot;: 1508197557000,\r\n          &quot;message&quot;: &quot;Oct 16 23:45:57 some_log_entry_2&quot;\r\n        },\r\n        {\r\n          &quot;id&quot;: &quot;33633929427703365813575134502195362621356131219229245442&quot;,\r\n          &quot;timestamp&quot;: 1508197557000,\r\n          &quot;message&quot;: &quot;Oct 16 23:45:57 some_log_entry_3&quot;\r\n        }\r\n      ]\r\n    }\r\n\r\nIndividual item from `.logEvents[]` gets transformed into a json structure where the keys are the desired columns when searching logs within Kibana - something like this:\r\n\r\n    {\r\n    \t&#39;journalctl_host&#39;: &#39;ip-172-11-11-111&#39;,\r\n    \t&#39;process&#39;: &#39;haproxy&#39;,\r\n    \t&#39;pid&#39;: 15507,\r\n    \t&#39;client_ip&#39;: &#39;172.11.11.111&#39;,\r\n    \t&#39;client_port&#39;: 3924,\r\n    \t&#39;frontend_name&#39;: &#39;http-web&#39;,\r\n    \t&#39;backend_name&#39;: &#39;server&#39;,\r\n    \t&#39;server_name&#39;: &#39;server-3&#39;,\r\n    \t&#39;time_duration&#39;: 10,\r\n    \t&#39;status_code&#39;: 200,\r\n    \t&#39;bytes_read&#39;: 79,\r\n    \t&#39;@timestamp&#39;: &#39;1900-10-16T23:46:01.0Z&#39;,\r\n    \t&#39;tags&#39;: [&#39;haproxy&#39;],\r\n    \t&#39;message&#39;: &#39;HEAD / HTTP/1.1&#39;\r\n    }\r\n\r\nThe transformed json gets collected into an array which gets zlib compressed and base64 encoded string which is then transformed into a new json payload as the final lambda result:\r\n\r\n\r\n    {\r\n    &quot;records&quot;: [\r\n        {\r\n            &quot;recordId&quot;: &quot;49577998431243709525183749876652374166077260049460232194000000&quot;,\r\n            &quot;result&quot;: &quot;Ok&quot;,\r\n            &quot;data&quot;: &quot;base64_encoded_zlib_compressed_array_of_transformed_logs&quot;\r\n        }\r\n    ]}\r\n\r\n### Cloudwatch configuration\r\n\r\n13 log entries (~4kb) can get transformed to about 635kb.\r\n\r\nI have also decreased the thresholds for the awslogs, hoping that the size of the logs that are being sent to Lambda function is going to small:\r\n\r\n    buffer_duration = 10\r\n    batch_count = 10\r\n    batch_size = 500\r\n\r\nUnfortunately, when there is a burst - the spike can be upwards of 2800 lines where the size is upwards of 1MB.\r\n\r\nWhen the resulting payload from the lambda function is &quot;too big&quot; (~13mb of transformed logs), an error is logged in the lambda cloudwatch logs - &quot;body size is too long&quot;. There doesn&#39;t seem to be any indication where this error is coming from or whether there is a size limit on the lambda fn&#39;s response payload.",
            "link": "https://stackoverflow.com/questions/46894664/how-to-i-index-the-transformed-log-records-into-aws-elasticsearch",
            "title": "How to I index the transformed log records into AWS Elasticsearch?",
            "body": "<h2>TLDR</h2>\n\n<p>The lambda function is not able to index the firehose logs into the AWS managed ES due to an \"encoding problem\".</p>\n\n<h2>Actual Error Response</h2>\n\n<p>I do not get any error when I base64 encode a single <code>logEvent</code> from a firehose <code>record</code> and send the collected records to the AWS managed ES.</p>\n\n<p>See the next section for more details.</p>\n\n<p>The base 64 encoded compressed payload is being sent to ES as the resulting json transformation is too big for ES to index - <a href=\"https://github.com/elastic/elasticsearch-net/issues/1209\" rel=\"nofollow noreferrer\">see this ES link</a>.</p>\n\n<p>I get the following error from the AWS managed ES:</p>\n\n<pre><code>{\n    \"deliveryStreamARN\": \"arn:aws:firehose:us-west-2:*:deliverystream/*\",\n    \"destination\": \"arn:aws:es:us-west-2:*:domain/*\",\n    \"deliveryStreamVersionId\": 1,\n    \"message\": \"The data could not be decoded as UTF-8\",\n    \"errorCode\": \"InvalidEncodingException\",\n    \"processor\": \"arn:aws:lambda:us-west-2:*:function:*\"\n  }\n</code></pre>\n\n<p>If the output record is not compressed, <code>the body size is too long</code> (as small as 14MB). Without compression and a simple base64 encoded payload, I get the following error in the Lambda logs:</p>\n\n<pre><code>{\n  \"type\": \"mapper_parsing_exception\",\n  \"reason\": \"failed to parse\",\n  \"caused_by\": {\n    \"type\": \"not_x_content_exception\",\n    \"reason\": \"Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes\"\n  }\n}\n</code></pre>\n\n<h2>Description</h2>\n\n<p>I have Cloudwatch logs that are getting buffered by size / interval which gets fed into a Kinesis Firehose. The firehose transports the logs into a lambda function which transforms the log into a json record which should then send it over to the AWS managed Elasticsearch cluster.</p>\n\n<p>The lambda function gets the following JSON structure:</p>\n\n<pre><code>{\n    \"invocationId\": \"cf1306b5-2d3c-4886-b7be-b5bcf0a66ef3\",\n    \"deliveryStreamArn\": \"arn:aws:firehose:...\",\n    \"region\": \"us-west-2\",\n    \"records\": [{\n        \"recordId\": \"49577998431243709525183749876652374166077260049460232194000000\",\n        \"approximateArrivalTimestamp\": 1508197563377,\n        \"data\": \"some_compressed_data_in_base_64_encoding\"\n    }]\n}\n</code></pre>\n\n<p>The lambda function then extracts <code>.records[].data</code> and decodes the data as base64 and decompresses the data which results in the following JSON:</p>\n\n<pre><code>{\n  \"messageType\": \"DATA_MESSAGE\",\n  \"owner\": \"aws_account_number\",\n  \"logGroup\": \"some_cloudwatch_log_group_name\",\n  \"logStream\": \"i-0221b6ec01af47bfb\",\n  \"subscriptionFilters\": [\n    \"cloudwatch_log_subscription_filter_name\"\n  ],\n  \"logEvents\": [\n    {\n      \"id\": \"33633929427703365813575134502195362621356131219229245440\",\n      \"timestamp\": 1508197557000,\n      \"message\": \"Oct 16 23:45:57 some_log_entry_1\"\n    },\n    {\n      \"id\": \"33633929427703365813575134502195362621356131219229245441\",\n      \"timestamp\": 1508197557000,\n      \"message\": \"Oct 16 23:45:57 some_log_entry_2\"\n    },\n    {\n      \"id\": \"33633929427703365813575134502195362621356131219229245442\",\n      \"timestamp\": 1508197557000,\n      \"message\": \"Oct 16 23:45:57 some_log_entry_3\"\n    }\n  ]\n}\n</code></pre>\n\n<p>Individual item from <code>.logEvents[]</code> gets transformed into a json structure where the keys are the desired columns when searching logs within Kibana - something like this:</p>\n\n<pre><code>{\n    'journalctl_host': 'ip-172-11-11-111',\n    'process': 'haproxy',\n    'pid': 15507,\n    'client_ip': '172.11.11.111',\n    'client_port': 3924,\n    'frontend_name': 'http-web',\n    'backend_name': 'server',\n    'server_name': 'server-3',\n    'time_duration': 10,\n    'status_code': 200,\n    'bytes_read': 79,\n    '@timestamp': '1900-10-16T23:46:01.0Z',\n    'tags': ['haproxy'],\n    'message': 'HEAD / HTTP/1.1'\n}\n</code></pre>\n\n<p>The transformed json gets collected into an array which gets zlib compressed and base64 encoded string which is then transformed into a new json payload as the final lambda result:</p>\n\n<pre><code>{\n\"records\": [\n    {\n        \"recordId\": \"49577998431243709525183749876652374166077260049460232194000000\",\n        \"result\": \"Ok\",\n        \"data\": \"base64_encoded_zlib_compressed_array_of_transformed_logs\"\n    }\n]}\n</code></pre>\n\n<h3>Cloudwatch configuration</h3>\n\n<p>13 log entries (~4kb) can get transformed to about 635kb.</p>\n\n<p>I have also decreased the thresholds for the awslogs, hoping that the size of the logs that are being sent to Lambda function is going to small:</p>\n\n<pre><code>buffer_duration = 10\nbatch_count = 10\nbatch_size = 500\n</code></pre>\n\n<p>Unfortunately, when there is a burst - the spike can be upwards of 2800 lines where the size is upwards of 1MB.</p>\n\n<p>When the resulting payload from the lambda function is \"too big\" (~13mb of transformed logs), an error is logged in the lambda cloudwatch logs - \"body size is too long\". There doesn't seem to be any indication where this error is coming from or whether there is a size limit on the lambda fn's response payload.</p>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "elasticsearch",
                "amazon-s3",
                "aws-kinesis-firehose"
            ],
            "owner": {
                "reputation": 259,
                "user_id": 2534949,
                "user_type": "registered",
                "accept_rate": 0,
                "profile_image": "https://www.gravatar.com/avatar/a78b4d180154073aed212c03bc001e67?s=128&d=identicon&r=PG",
                "display_name": "einav",
                "link": "https://stackoverflow.com/users/2534949/einav"
            },
            "is_answered": true,
            "view_count": 213,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526427470,
            "creation_date": 1501577642,
            "last_edit_date": 1526427470,
            "question_id": 45433243,
            "body_markdown": "What I am trying to do is define cross delivery between a Firehose stream that is in account A to Elasticsearch that is in account B.\r\n\r\nI get &quot;Cross-account destination roles are not allowed&quot; when creating the stream through the c#.\r\n\r\nAny one knows why?\r\n\r\nThanks!",
            "link": "https://stackoverflow.com/questions/45433243/aws-cross-account-destination-roles-are-not-allowed",
            "title": "AWS - Cross-account destination roles are not allowed",
            "body": "<p>What I am trying to do is define cross delivery between a Firehose stream that is in account A to Elasticsearch that is in account B.</p>\n\n<p>I get \"Cross-account destination roles are not allowed\" when creating the stream through the c#.</p>\n\n<p>Any one knows why?</p>\n\n<p>Thanks!</p>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "amazon-kinesis",
                "aws-kinesis-firehose"
            ],
            "owner": {
                "reputation": 28,
                "user_id": 3251128,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/cd6b3124ce714da48379c4b6715a2e16?s=128&d=identicon&r=PG&f=1",
                "display_name": "lorilew",
                "link": "https://stackoverflow.com/users/3251128/lorilew"
            },
            "is_answered": false,
            "view_count": 243,
            "answer_count": 0,
            "score": 4,
            "last_activity_date": 1526427464,
            "creation_date": 1499182708,
            "last_edit_date": 1526427464,
            "question_id": 44909875,
            "body_markdown": "I&#39;m currently sending a series of xml messages to aws kinesis stream, I&#39;ve been using this on different projects, so I&#39;m pretty confident that this bit works. Then I&#39;ve written a lambda to process events from kinesis stream to kinesis firehose:\r\n\r\n    import os\r\n    import boto3\r\n    import base64\r\n\r\n    firehose = boto3.client(&#39;firehose&#39;)\r\n\r\n\r\n    def lambda_handler(event, context):\r\n        deliveryStreamName = os.environ[&#39;FIREHOSE_STREAM_NAME&#39;]\r\n\r\n        # Send record directly to firehose\r\n        for record in event[&#39;Records&#39;]:\r\n            data = record[&#39;kinesis&#39;][&#39;data&#39;]\r\n\r\n            response = firehose.put_record(\r\n                DeliveryStreamName=deliveryStreamName,\r\n                Record={&#39;Data&#39;: data}\r\n            )\r\n            print(response)\r\n\r\nI&#39;ve set the kinesis stream as the lamdba trigger, and set the batch size as 1, and starting position LATEST. \r\n\r\nFor the kinesis firehose I have the following config:\r\n\r\n    Data transformation*: Disabled\r\n    Source record backup*: Disabled\r\n    S3 buffer size (MB)*: 10\r\n    S3 buffer interval (sec)*: 60\r\n    S3 Compression: UNCOMPRESSED\r\n    S3 Encryption: No Encryption\r\n    Status: ACTIVE\r\n    Error logging: Enabled\r\n\r\nI sent 162 events, and I read them from s3, and the most I&#39;ve managed to get it 160, and usually it&#39;s less. I&#39;ve even tried to wait a few hours incase something strange was happening with retries. \r\n\r\nAnyone had any experience using kinesis-&gt; lamdba -&gt; firehose, and seen issues of lost data? ",
            "link": "https://stackoverflow.com/questions/44909875/anyone-experienced-data-lost-when-using-aws-kinesis-streams-lambda-and-firehose",
            "title": "Anyone experienced data lost when using AWS kinesis streams, lambda and firehose?",
            "body": "<p>I'm currently sending a series of xml messages to aws kinesis stream, I've been using this on different projects, so I'm pretty confident that this bit works. Then I've written a lambda to process events from kinesis stream to kinesis firehose:</p>\n\n<pre><code>import os\nimport boto3\nimport base64\n\nfirehose = boto3.client('firehose')\n\n\ndef lambda_handler(event, context):\n    deliveryStreamName = os.environ['FIREHOSE_STREAM_NAME']\n\n    # Send record directly to firehose\n    for record in event['Records']:\n        data = record['kinesis']['data']\n\n        response = firehose.put_record(\n            DeliveryStreamName=deliveryStreamName,\n            Record={'Data': data}\n        )\n        print(response)\n</code></pre>\n\n<p>I've set the kinesis stream as the lamdba trigger, and set the batch size as 1, and starting position LATEST. </p>\n\n<p>For the kinesis firehose I have the following config:</p>\n\n<pre><code>Data transformation*: Disabled\nSource record backup*: Disabled\nS3 buffer size (MB)*: 10\nS3 buffer interval (sec)*: 60\nS3 Compression: UNCOMPRESSED\nS3 Encryption: No Encryption\nStatus: ACTIVE\nError logging: Enabled\n</code></pre>\n\n<p>I sent 162 events, and I read them from s3, and the most I've managed to get it 160, and usually it's less. I've even tried to wait a few hours incase something strange was happening with retries. </p>\n\n<p>Anyone had any experience using kinesis-> lamdba -> firehose, and seen issues of lost data? </p>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "lambda",
                "aws-lambda",
                "aws-kinesis-firehose"
            ],
            "owner": {
                "reputation": 15,
                "user_id": 2736527,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/7fd77e7ee20c25373aef8b318531d8a8?s=128&d=identicon&r=PG&f=1",
                "display_name": "user2736527",
                "link": "https://stackoverflow.com/users/2736527/user2736527"
            },
            "is_answered": true,
            "view_count": 373,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1526427455,
            "creation_date": 1496155585,
            "last_edit_date": 1526427455,
            "question_id": 44265425,
            "body_markdown": "I have a use case in which I have to collect thousands of records per second from different producers and push them to elastic search using AWS firehose. I am also using a data transformation lambda on the firehose which does a fair amount of computation before passing the records back to the firehose.\r\n\r\nFirehose is supposed to invoke lambda with each buffered batch asynchronously before the data is buffered again to be delivered to the destination.\r\n\r\nI ran a basic test with ingress rate of 4k records per second for 15 minutes and here is how the system responded.\r\n\r\n\r\n![Firehose CloudWatch Metrics][1]\r\n[Firehose CloudWatch Metrics][1]\r\n\r\n![Lambda CloudWatch Metrics][2]\r\n[Lambda CloudWatch Metrics][2]\r\n\r\nLooking at the firehose metrics, it is obvious that it took the firehose more than an hour to process all the incoming events. As there isn&#39;t any lambda throttles (figure 2), So I am wondering why Firehose didn&#39;t run as many lambdas as possible to be able to keep up with the input rate?\r\n\r\nAs shown in the second figure, I have around 30 lambda invocations per minute and the average processing duration is 8000ms.\r\n\r\nSo I am wondering if firehose runs lambda concurrently? and is there any firehose-lambda concurrency limits that I am missing?\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/5PBJ5.jpg\r\n  [2]: https://i.stack.imgur.com/yGyDC.jpg",
            "link": "https://stackoverflow.com/questions/44265425/aws-firehose-data-transformation-concurrency-limits",
            "title": "AWS Firehose data transformation concurrency limits",
            "body": "<p>I have a use case in which I have to collect thousands of records per second from different producers and push them to elastic search using AWS firehose. I am also using a data transformation lambda on the firehose which does a fair amount of computation before passing the records back to the firehose.</p>\n\n<p>Firehose is supposed to invoke lambda with each buffered batch asynchronously before the data is buffered again to be delivered to the destination.</p>\n\n<p>I ran a basic test with ingress rate of 4k records per second for 15 minutes and here is how the system responded.</p>\n\n<p><img src=\"https://i.stack.imgur.com/5PBJ5.jpg\" alt=\"Firehose CloudWatch Metrics\">\n<a href=\"https://i.stack.imgur.com/5PBJ5.jpg\" rel=\"nofollow noreferrer\">Firehose CloudWatch Metrics</a></p>\n\n<p><img src=\"https://i.stack.imgur.com/yGyDC.jpg\" alt=\"Lambda CloudWatch Metrics\">\n<a href=\"https://i.stack.imgur.com/yGyDC.jpg\" rel=\"nofollow noreferrer\">Lambda CloudWatch Metrics</a></p>\n\n<p>Looking at the firehose metrics, it is obvious that it took the firehose more than an hour to process all the incoming events. As there isn't any lambda throttles (figure 2), So I am wondering why Firehose didn't run as many lambdas as possible to be able to keep up with the input rate?</p>\n\n<p>As shown in the second figure, I have around 30 lambda invocations per minute and the average processing duration is 8000ms.</p>\n\n<p>So I am wondering if firehose runs lambda concurrently? and is there any firehose-lambda concurrency limits that I am missing?</p>\n"
        },
        {
            "tags": [
                "python",
                "django",
                "django-forms",
                "django-views"
            ],
            "owner": {
                "reputation": 26,
                "user_id": 8981018,
                "user_type": "registered",
                "accept_rate": 83,
                "profile_image": "https://graph.facebook.com/1884572654904647/picture?type=large",
                "display_name": "Adam Dalton",
                "link": "https://stackoverflow.com/users/8981018/adam-dalton"
            },
            "is_answered": true,
            "view_count": 22,
            "accepted_answer_id": 50360544,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526427454,
            "creation_date": 1526426241,
            "last_edit_date": 1526427404,
            "question_id": 50360404,
            "body_markdown": "I am working on a Django project and essentially I have written a model which stores user details. They complete this profile after they have signed up and I have a Boolean in the User model which states whether they have completed this custom profile yet so I can make changes in the template.\r\n\r\nWhen the form for the second profile page gets submitted I would like it to update the Bool from False to True but I am getting the error:\r\n\r\n    &#39;bool&#39; object has no attribute &#39;has_created_artist_profile&#39;\r\n\r\nSee code below:\r\n\r\n\r\n**views.py**\r\n\r\n    def ArtistEditView(request):\r\n        artist = Artist.objects.get(user=request.user)\r\n        current_artist = request.user\r\n        artist_status = current_artist.has_created_artist_profile\r\n    \r\n        if request.method == &#39;POST&#39;:\r\n            form = ArtistForm(request.POST, request.FILES, instance=artist)\r\n            if form.is_valid():\r\n                artist_status.has_created_artist_profile = True\r\n                artist_status.save()\r\n                form.save()\r\n                return redirect(reverse(&#39;artist_home&#39;))\r\n        else:\r\n            artist_dict = model_to_dict(artist)\r\n            form = ArtistForm(artist_dict)\r\n        return render(request, &#39;artist/artist_edit.html&#39;, {&#39;form&#39;: form})\r\n\r\n\r\n**forms.py**\r\n\r\n    class ArtistForm(forms.ModelForm):\r\n        class Meta:\r\n            model = Artist\r\n            exclude =[&#39;user&#39;, ]\r\n\r\n\r\nAny one able to suggest a better way to update this / to get rid of the error?\r\n",
            "link": "https://stackoverflow.com/questions/50360404/attributeerror-bool-object-has-no-attribute",
            "title": "AttributeError: &#39;bool&#39; object has no attribute",
            "body": "<p>I am working on a Django project and essentially I have written a model which stores user details. They complete this profile after they have signed up and I have a Boolean in the User model which states whether they have completed this custom profile yet so I can make changes in the template.</p>\n\n<p>When the form for the second profile page gets submitted I would like it to update the Bool from False to True but I am getting the error:</p>\n\n<pre><code>'bool' object has no attribute 'has_created_artist_profile'\n</code></pre>\n\n<p>See code below:</p>\n\n<p><strong>views.py</strong></p>\n\n<pre><code>def ArtistEditView(request):\n    artist = Artist.objects.get(user=request.user)\n    current_artist = request.user\n    artist_status = current_artist.has_created_artist_profile\n\n    if request.method == 'POST':\n        form = ArtistForm(request.POST, request.FILES, instance=artist)\n        if form.is_valid():\n            artist_status.has_created_artist_profile = True\n            artist_status.save()\n            form.save()\n            return redirect(reverse('artist_home'))\n    else:\n        artist_dict = model_to_dict(artist)\n        form = ArtistForm(artist_dict)\n    return render(request, 'artist/artist_edit.html', {'form': form})\n</code></pre>\n\n<p><strong>forms.py</strong></p>\n\n<pre><code>class ArtistForm(forms.ModelForm):\n    class Meta:\n        model = Artist\n        exclude =['user', ]\n</code></pre>\n\n<p>Any one able to suggest a better way to update this / to get rid of the error?</p>\n"
        },
        {
            "tags": [
                "android",
                "google-play-services",
                "safetynet"
            ],
            "owner": {
                "reputation": 1378,
                "user_id": 1649615,
                "user_type": "registered",
                "accept_rate": 67,
                "profile_image": "https://i.stack.imgur.com/11yOm.jpg?s=128&g=1",
                "display_name": "Mickey Tin",
                "link": "https://stackoverflow.com/users/1649615/mickey-tin"
            },
            "is_answered": true,
            "view_count": 194,
            "accepted_answer_id": 50360510,
            "answer_count": 1,
            "score": 6,
            "last_activity_date": 1526427434,
            "creation_date": 1524733176,
            "last_edit_date": 1526372791,
            "question_id": 50038897,
            "body_markdown": "When setting restriction to an API key the Attestation API stops working:\r\n`OnFailureListener` gets fired with the [`CANCELLED`][1](16) status code\r\n\r\nThe restrictions are the android package name and the certificate signature(SHA-1)\r\n\r\n[![enter image description here][2]][2]\r\n\r\nDoes the SafetyNet Attestation API support restricted API keys ?\r\n\r\nNote: the same API key with the additional restriction works fine with Google Maps API\r\n\r\n\r\n  [1]: https://developers.google.com/android/reference/com/google/android/gms/common/api/CommonStatusCodes.html#CANCELED\r\n[2]: https://i.stack.imgur.com/2LE9q.png",
            "link": "https://stackoverflow.com/questions/50038897/android-safetynet-api-fails-when-using-api-key-restriction",
            "title": "Android SafetyNet API fails when using API key restriction",
            "body": "<p>When setting restriction to an API key the Attestation API stops working:\n<code>OnFailureListener</code> gets fired with the <a href=\"https://developers.google.com/android/reference/com/google/android/gms/common/api/CommonStatusCodes.html#CANCELED\" rel=\"nofollow noreferrer\"><code>CANCELLED</code></a>(16) status code</p>\n\n<p>The restrictions are the android package name and the certificate signature(SHA-1)</p>\n\n<p><a href=\"https://i.stack.imgur.com/2LE9q.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/2LE9q.png\" alt=\"enter image description here\"></a></p>\n\n<p>Does the SafetyNet Attestation API support restricted API keys ?</p>\n\n<p>Note: the same API key with the additional restriction works fine with Google Maps API</p>\n"
        }
    ],
    "has_more": true,
    "quota_max": 300,
    "quota_remaining": 295
}