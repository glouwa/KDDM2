{
    "items": [
        {
            "tags": [
                "amazon-s3",
                "parquet",
                "amazon-athena",
                "aws-glue"
            ],
            "owner": {
                "reputation": 269,
                "user_id": 5504459,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/5433245ceb1d5feb805dbcbd9f462bc5?s=128&d=identicon&r=PG&f=1",
                "display_name": "mark s.",
                "link": "https://stackoverflow.com/users/5504459/mark-s"
            },
            "is_answered": false,
            "view_count": 131,
            "answer_count": 2,
            "score": 0,
            "last_activity_date": 1526428210,
            "creation_date": 1524502444,
            "last_edit_date": 1526428210,
            "question_id": 49986235,
            "body_markdown": "I&#39;m using AWS S3, Glue, and Athena with the following setup:\r\n\r\nS3 --&gt; Glue --&gt; Athena\r\n\r\nMy raw data is stored on S3 as CSV files. I&#39;m using Glue for ETL, and I&#39;m using Athena to query the data. \r\n\r\nSince I&#39;m using Athena, I&#39;d like to convert the CSV files to Parquet. I&#39;m using AWS Glue to do this right now. This is the current process I&#39;m using:\r\n\r\n1. Run Crawler to read CSV files and populate Data Catalog.\r\n2. Run ETL job to create Parquet file from Data Catalog.\r\n3. Run a Crawler to populate Data Catalog using Parquet file.\r\n\r\nThe Glue job only allows me to convert one table at a time. If I have many CSV files, this process quickly becomes unmanageable. Is there a better way, perhaps a &quot;correct&quot; way, of converting **many** CSV files to Parquet using AWS Glue or some other AWS service?\r\n",
            "link": "https://stackoverflow.com/questions/49986235/how-to-convert-many-csv-files-to-parquet-using-aws-glue",
            "title": "How to Convert Many CSV files to Parquet using AWS Glue",
            "body": "<p>I'm using AWS S3, Glue, and Athena with the following setup:</p>\n\n<p>S3 --> Glue --> Athena</p>\n\n<p>My raw data is stored on S3 as CSV files. I'm using Glue for ETL, and I'm using Athena to query the data. </p>\n\n<p>Since I'm using Athena, I'd like to convert the CSV files to Parquet. I'm using AWS Glue to do this right now. This is the current process I'm using:</p>\n\n<ol>\n<li>Run Crawler to read CSV files and populate Data Catalog.</li>\n<li>Run ETL job to create Parquet file from Data Catalog.</li>\n<li>Run a Crawler to populate Data Catalog using Parquet file.</li>\n</ol>\n\n<p>The Glue job only allows me to convert one table at a time. If I have many CSV files, this process quickly becomes unmanageable. Is there a better way, perhaps a \"correct\" way, of converting <strong>many</strong> CSV files to Parquet using AWS Glue or some other AWS service?</p>\n"
        },
        {
            "tags": [
                "arrays",
                "json",
                "azure",
                "azure-resource-manager"
            ],
            "owner": {
                "reputation": 93,
                "user_id": 5161238,
                "user_type": "registered",
                "accept_rate": 11,
                "profile_image": "https://lh3.googleusercontent.com/-hzjTk9Q79HM/AAAAAAAAAAI/AAAAAAAAABo/sbxYr1fdn0g/photo.jpg?sz=128",
                "display_name": "craig Rickett",
                "link": "https://stackoverflow.com/users/5161238/craig-rickett"
            },
            "is_answered": true,
            "view_count": 19,
            "answer_count": 2,
            "score": 0,
            "last_activity_date": 1526428206,
            "creation_date": 1526399390,
            "last_edit_date": 1526414695,
            "question_id": 50354410,
            "body_markdown": "I want to define a Azure policy where by deployments can only be made to &#39;West US&#39; and East US at subscription level.\r\n\r\nI understand that I&#39;m trying to populate a array of locations but I&#39;m going wrong some where;\r\n\r\n&lt;!-- begin snippet: js hide: false console: true babel: false --&gt;\r\n\r\n&lt;!-- language: lang-html --&gt;\r\n\r\n    {\r\n      &quot;policyRule&quot;: {\r\n        &quot;if&quot;: {\r\n          &quot;not&quot;: {\r\n            &quot;field&quot;: &quot;location&quot;,\r\n            &quot;in&quot;: &quot;[parameters(&#39;allowedLocations&#39;)]&quot;\r\n          }\r\n        },\r\n        &quot;then&quot;: {\r\n          &quot;effect&quot;: &quot;deny&quot;\r\n        }\r\n      },\r\n      &quot;parameters&quot;: {\r\n        &quot;allowedLocations&quot;: {\r\n          &quot;type&quot;: &quot;Array&quot;,\r\n          &quot;metadata&quot;: {\r\n            &quot;description&quot;: &quot;The list of allowed locations for resources.&quot;,\r\n            &quot;displayName&quot;: &quot;Allowed locations&quot;,\r\n            &quot;strongType&quot;: &quot;location&quot;,\r\n            &quot;value&quot;: [ &quot;West US&quot;, &quot;East US&quot; ], \r\n          }\r\n        }\r\n      }\r\n    }\r\n\r\n&lt;!-- end snippet --&gt;\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50354410/set-azure-location-policy-restriction",
            "title": "Set Azure Location Policy Restriction",
            "body": "<p>I want to define a Azure policy where by deployments can only be made to 'West US' and East US at subscription level.</p>\n\n<p>I understand that I'm trying to populate a array of locations but I'm going wrong some where;</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>{\r\n  \"policyRule\": {\r\n    \"if\": {\r\n      \"not\": {\r\n        \"field\": \"location\",\r\n        \"in\": \"[parameters('allowedLocations')]\"\r\n      }\r\n    },\r\n    \"then\": {\r\n      \"effect\": \"deny\"\r\n    }\r\n  },\r\n  \"parameters\": {\r\n    \"allowedLocations\": {\r\n      \"type\": \"Array\",\r\n      \"metadata\": {\r\n        \"description\": \"The list of allowed locations for resources.\",\r\n        \"displayName\": \"Allowed locations\",\r\n        \"strongType\": \"location\",\r\n        \"value\": [ \"West US\", \"East US\" ], \r\n      }\r\n    }\r\n  }\r\n}</code></pre>\r\n</div>\r\n</div>\r\n</p>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "prestodb",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 100,
                "user_id": 5005586,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/c102d821f8a576e20324ca64b13e5283?s=128&d=identicon&r=PG&f=1",
                "display_name": "Sunil Kumbhar",
                "link": "https://stackoverflow.com/users/5005586/sunil-kumbhar"
            },
            "is_answered": true,
            "view_count": 95,
            "accepted_answer_id": 49969673,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1526428205,
            "creation_date": 1524421239,
            "last_edit_date": 1526428205,
            "question_id": 49969274,
            "body_markdown": "Does Athena have a gigantic cluster of machines ready to take queries from users and run them against their data? Are they using a specific open-source cluster management software for this?",
            "link": "https://stackoverflow.com/questions/49969274/how-does-aws-athena-manage-to-execute-queries-immediately",
            "title": "How does AWS Athena manage to execute queries immediately?",
            "body": "<p>Does Athena have a gigantic cluster of machines ready to take queries from users and run them against their data? Are they using a specific open-source cluster management software for this?</p>\n"
        },
        {
            "tags": [
                "python",
                "amazon-web-services",
                "boto3",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 48,
                "user_id": 2228145,
                "user_type": "registered",
                "accept_rate": 88,
                "profile_image": "https://i.stack.imgur.com/Uua4y.jpg?s=128&g=1",
                "display_name": "Carlos P Ceballos",
                "link": "https://stackoverflow.com/users/2228145/carlos-p-ceballos"
            },
            "is_answered": true,
            "view_count": 34,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428194,
            "creation_date": 1524252656,
            "last_edit_date": 1526428194,
            "question_id": 49948563,
            "body_markdown": "Im making a script that creates a database in AWS Athena and then creates tables for that database, today the DB creation was taking ages, so the tables being created referred to a db that doesn&#39;t exists, is there a way to check if a DB is already created in Athena using boto3?\r\n\r\nThis is the part that created the db:\r\n\r\n\r\n    client = boto3.client(&#39;athena&#39;)\r\n    client.start_query_execution(\r\n        QueryString=&#39;create database {}&#39;.format(&#39;db_name&#39;),\r\n        ResultConfiguration=config\r\n    )",
            "link": "https://stackoverflow.com/questions/49948563/boto3-check-if-athena-database-exists",
            "title": "boto3 check if Athena database exists",
            "body": "<p>Im making a script that creates a database in AWS Athena and then creates tables for that database, today the DB creation was taking ages, so the tables being created referred to a db that doesn't exists, is there a way to check if a DB is already created in Athena using boto3?</p>\n\n<p>This is the part that created the db:</p>\n\n<pre><code>client = boto3.client('athena')\nclient.start_query_execution(\n    QueryString='create database {}'.format('db_name'),\n    ResultConfiguration=config\n)\n</code></pre>\n"
        },
        {
            "tags": [
                "tsql"
            ],
            "owner": {
                "reputation": 29,
                "user_id": 7369720,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/caa1cfd5e5e2c3a2f5c87f43f08d6c0f?s=128&d=identicon&r=PG&f=1",
                "display_name": "Peter",
                "link": "https://stackoverflow.com/users/7369720/peter"
            },
            "is_answered": false,
            "view_count": 29,
            "answer_count": 1,
            "score": -1,
            "last_activity_date": 1526428191,
            "creation_date": 1526333670,
            "question_id": 50339353,
            "body_markdown": "I see this example in Microsofts documentation\r\n\r\n    USE AdventureWorks2012;  \r\n    GO  \r\n    DECLARE contact_cursor CURSOR FOR  \r\n    SELECT LastName FROM Person.Person  \r\n    WHERE LastName LIKE &#39;B%&#39;  \r\n    ORDER BY LastName;  \r\n    \r\n    OPEN contact_cursor;  \r\n    \r\n    -- Perform the first fetch.  \r\n    FETCH NEXT FROM contact_cursor;  \r\n    \r\n    -- Check @@FETCH_STATUS to see if there are any more rows to fetch.  \r\n    WHILE @@FETCH_STATUS = 0  \r\n    BEGIN  \r\n       -- **This is executed as long as the previous fetch succeeds.**  \r\n       FETCH NEXT FROM contact_cursor;  \r\n    END  \r\n    \r\n    CLOSE contact_cursor;  \r\n    DEALLOCATE contact_cursor;  \r\n    GO  \r\n\r\nI&#39;m wondering and asking\r\n \r\na) what happens inside the FETCH-WHILE loop? It seems to do just nothing, except skipping over the comment. Can one possibly do something on the data, without using `&#39;INTO&#39;`?\r\n\r\nb) how to access the column data from the current row, inside the loop, in T-SQL? Preferrably just `contact_cursor.LastName` .\r\n\r\nYes I read that I should work &#39;set-based&#39;, and cursors are not efficient. But I&#39;m allowed to do processing (string manips, testing them, and in the end one UPDATE per row) in T-SQL and not in the outside procedural environment, and do it carefully in a robust way, and it is just a couple of records, once+testing. And me perfectly new to T-SQL, but old to SQL and many other things. T-SQL is 2016 when it matters.\r\n\r\nPlz I can&#39;t reply as comments, because my level is too low. I&#39;m grateful for help, for the explanation of this. Thank you!",
            "link": "https://stackoverflow.com/questions/50339353/purpose-of-microsofts-fetch-loop-example",
            "title": "purpose of Microsofts FETCH loop example?",
            "body": "<p>I see this example in Microsofts documentation</p>\n\n<pre><code>USE AdventureWorks2012;  \nGO  \nDECLARE contact_cursor CURSOR FOR  \nSELECT LastName FROM Person.Person  \nWHERE LastName LIKE 'B%'  \nORDER BY LastName;  \n\nOPEN contact_cursor;  \n\n-- Perform the first fetch.  \nFETCH NEXT FROM contact_cursor;  \n\n-- Check @@FETCH_STATUS to see if there are any more rows to fetch.  \nWHILE @@FETCH_STATUS = 0  \nBEGIN  \n   -- **This is executed as long as the previous fetch succeeds.**  \n   FETCH NEXT FROM contact_cursor;  \nEND  \n\nCLOSE contact_cursor;  \nDEALLOCATE contact_cursor;  \nGO  \n</code></pre>\n\n<p>I'm wondering and asking</p>\n\n<p>a) what happens inside the FETCH-WHILE loop? It seems to do just nothing, except skipping over the comment. Can one possibly do something on the data, without using <code>'INTO'</code>?</p>\n\n<p>b) how to access the column data from the current row, inside the loop, in T-SQL? Preferrably just <code>contact_cursor.LastName</code> .</p>\n\n<p>Yes I read that I should work 'set-based', and cursors are not efficient. But I'm allowed to do processing (string manips, testing them, and in the end one UPDATE per row) in T-SQL and not in the outside procedural environment, and do it carefully in a robust way, and it is just a couple of records, once+testing. And me perfectly new to T-SQL, but old to SQL and many other things. T-SQL is 2016 when it matters.</p>\n\n<p>Plz I can't reply as comments, because my level is too low. I'm grateful for help, for the explanation of this. Thank you!</p>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "aws-sdk",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 31,
                "user_id": 1413199,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/ede80e6e434a261d1f58629d7f33eb67?s=128&d=identicon&r=PG",
                "display_name": "JoshBleggi",
                "link": "https://stackoverflow.com/users/1413199/joshbleggi"
            },
            "is_answered": true,
            "view_count": 27,
            "accepted_answer_id": 49957756,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428187,
            "creation_date": 1524153289,
            "last_edit_date": 1526428187,
            "question_id": 49925347,
            "body_markdown": "I&#39;ve got a set of files with way more columns than we actually need. Of which, the columns included and order may be variable. Using this Table create:\r\n\r\n    CREATE EXTERNAL TABLE `test1column`(\r\n    `column3` string)\r\n    ROW FORMAT DELIMITED \r\n      FIELDS TERMINATED BY &#39;,&#39; \r\n    STORED AS INPUTFORMAT \r\n      &#39;org.apache.hadoop.mapred.TextInputFormat&#39; \r\n    OUTPUTFORMAT \r\n      &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;\r\n    LOCATION\r\n      &#39;s3://bucketpath/folder&#39;\r\n    TBLPROPERTIES (\r\n      &#39;has_encrypted_data&#39;=&#39;false&#39;, \r\n      &#39;transient_lastDdlTime&#39;=&#39;1524150460&#39;)\r\n\r\nAthena just pulls the first column in so the output ends up being: \r\n\r\n    column3\r\n    ---------\r\n    column1\r\n    val1\r\n    val2\r\n    val3\r\n\r\nI&#39;m creating these tables programmatically so I&#39;d like to not have to read through every column name and create a table with more data than I need. If it&#39;s not possible to map only certain columns into a table with Athena yet then I suppose I&#39;ll have to.\r\n",
            "link": "https://stackoverflow.com/questions/49925347/can-an-athena-table-with-only-certain-columns-be-created",
            "title": "Can an Athena table with only certain columns be created?",
            "body": "<p>I've got a set of files with way more columns than we actually need. Of which, the columns included and order may be variable. Using this Table create:</p>\n\n<pre><code>CREATE EXTERNAL TABLE `test1column`(\n`column3` string)\nROW FORMAT DELIMITED \n  FIELDS TERMINATED BY ',' \nSTORED AS INPUTFORMAT \n  'org.apache.hadoop.mapred.TextInputFormat' \nOUTPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n  's3://bucketpath/folder'\nTBLPROPERTIES (\n  'has_encrypted_data'='false', \n  'transient_lastDdlTime'='1524150460')\n</code></pre>\n\n<p>Athena just pulls the first column in so the output ends up being: </p>\n\n<pre><code>column3\n---------\ncolumn1\nval1\nval2\nval3\n</code></pre>\n\n<p>I'm creating these tables programmatically so I'd like to not have to read through every column name and create a table with more data than I need. If it's not possible to map only certain columns into a table with Athena yet then I suppose I'll have to.</p>\n"
        },
        {
            "tags": [
                "node.js",
                "lambda",
                "compression",
                "amazon-athena",
                "snappy"
            ],
            "owner": {
                "reputation": 6,
                "user_id": 3042194,
                "user_type": "registered",
                "accept_rate": 0,
                "profile_image": "https://graph.facebook.com/692813217/picture?type=large",
                "display_name": "justMiLa",
                "link": "https://stackoverflow.com/users/3042194/justmila"
            },
            "is_answered": false,
            "view_count": 37,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526428182,
            "creation_date": 1524089147,
            "last_edit_date": 1526428182,
            "question_id": 49909708,
            "body_markdown": "i made an AWS lambda in nodejs, so a i&#39;m using a library to convert json to parquet files. \r\n\r\nThis library doesn&#39;t have an own compress method, so after create my parquet file, i&#39;m trying to compress it with the google snappy library:\r\n\r\n    next =&gt; {\r\n          snappy.compress(fs.readFileSync(parquetFile.name), (err, compressed) =&gt; {\r\n            next(err, compressed)\r\n          })\r\n        },\r\n        (compressed, next) =&gt; {\r\n          const parquetS3File = new S3File(config.parquet.bucket, Tools.createPutKey(config.parquet.prefix, s3ObjectKey))\r\n          parquetS3File.body = compressed\r\n          parquetS3File.upload(err =&gt; {\r\n            parquetFile.removeCallback()\r\n            next(err)\r\n          })\r\n        }\r\n\r\nBut, when i try to do some query in athena, i have the next error \r\n\r\n    HIVE_CANNOT_OPEN_SPLIT: Error opening Hive split s3://bucket-example/data/parquet_node/year=2018/month=04/day=18/hour=18/minute=47/file.snappy.parquet (offset=0, length=11716266): can not read class parquet.format.FileMetaData: don&#39;t know what type: 15\r\n    \r\n    This query ran against the &quot;test&quot; database, unless qualified by the query. Please post the error message on our forum or contact customer support with Query Id: 3b3a2df1-b202.\r\n\r\nOn the other hand, when i don&#39;t compress the data, Athena can read the files.\r\n\r\nSomething like that:\r\n\r\n    next =&gt; {\r\n              const parquetS3File = new S3File(config.parquet.bucket, Tools.createPutKey(config.parquet.prefix, s3ObjectKey))\r\n              parquetS3File.body = fs.readFileSync(parquetFile.name)\r\n              parquetS3File.upload(err =&gt; {\r\n                parquetFile.removeCallback()\r\n                next(err)\r\n              })\r\n            }\r\n\r\nSo, the question is: Is there any way to compress a parquet file (with snappy) after it has been created?\r\n\r\nThanks!",
            "link": "https://stackoverflow.com/questions/49909708/aws-athena-cant-read-parquet-files-with-snappy-compression",
            "title": "AWS Athena can&#39;t read parquet files with snappy compression",
            "body": "<p>i made an AWS lambda in nodejs, so a i'm using a library to convert json to parquet files. </p>\n\n<p>This library doesn't have an own compress method, so after create my parquet file, i'm trying to compress it with the google snappy library:</p>\n\n<pre><code>next =&gt; {\n      snappy.compress(fs.readFileSync(parquetFile.name), (err, compressed) =&gt; {\n        next(err, compressed)\n      })\n    },\n    (compressed, next) =&gt; {\n      const parquetS3File = new S3File(config.parquet.bucket, Tools.createPutKey(config.parquet.prefix, s3ObjectKey))\n      parquetS3File.body = compressed\n      parquetS3File.upload(err =&gt; {\n        parquetFile.removeCallback()\n        next(err)\n      })\n    }\n</code></pre>\n\n<p>But, when i try to do some query in athena, i have the next error </p>\n\n<pre><code>HIVE_CANNOT_OPEN_SPLIT: Error opening Hive split s3://bucket-example/data/parquet_node/year=2018/month=04/day=18/hour=18/minute=47/file.snappy.parquet (offset=0, length=11716266): can not read class parquet.format.FileMetaData: don't know what type: 15\n\nThis query ran against the \"test\" database, unless qualified by the query. Please post the error message on our forum or contact customer support with Query Id: 3b3a2df1-b202.\n</code></pre>\n\n<p>On the other hand, when i don't compress the data, Athena can read the files.</p>\n\n<p>Something like that:</p>\n\n<pre><code>next =&gt; {\n          const parquetS3File = new S3File(config.parquet.bucket, Tools.createPutKey(config.parquet.prefix, s3ObjectKey))\n          parquetS3File.body = fs.readFileSync(parquetFile.name)\n          parquetS3File.upload(err =&gt; {\n            parquetFile.removeCallback()\n            next(err)\n          })\n        }\n</code></pre>\n\n<p>So, the question is: Is there any way to compress a parquet file (with snappy) after it has been created?</p>\n\n<p>Thanks!</p>\n"
        },
        {
            "tags": [
                "javascript",
                "angular"
            ],
            "owner": {
                "reputation": 116,
                "user_id": 1566939,
                "user_type": "registered",
                "accept_rate": 57,
                "profile_image": "https://i.stack.imgur.com/Lw1CZ.png?s=128&g=1",
                "display_name": "amigo21",
                "link": "https://stackoverflow.com/users/1566939/amigo21"
            },
            "is_answered": true,
            "view_count": 18,
            "accepted_answer_id": 50360630,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428175,
            "creation_date": 1526422698,
            "last_edit_date": 1526424479,
            "question_id": 50359921,
            "body_markdown": "I have some states in my Angular app, both having `myParent` as parent state:\r\n\r\n    var myChild1 = {\r\n      name: &#39;myChild1&#39;,\r\n      parent: &#39;myParent&#39;,\r\n      url: &#39;/mychild1&#39;\r\n      ...\r\n    };\r\n\r\n    var myChild2 = {\r\n      name: &#39;myChild2&#39;,\r\n      parent: &#39;myParent&#39;,\r\n      url: &#39;/mychild2&#39;\r\n      ...\r\n    };\r\n\r\nand here&#39;s `myParent`\r\n\r\n    var myParent = {\r\n      name: &#39;myParent&#39;,\r\n      abstract: true,\r\n      url: &#39;/:username/:password&#39;,\r\n      resolve: {\r\n        authorized: [\r\n          myService,\r\n          &#39;$stateParams&#39;,\r\n          function (myService, $stateParams) {\r\n            console.log($stateParams);\r\n         \r\n            myService.doSomething() // unrelated\r\n          }\r\n        ]\r\n      }\r\n    };\r\n\r\nI want to access it like the following:\r\n\r\n`/mychild1/superman/superpassword`\r\n\r\nHowever, it&#39;s only letting me access it like so:\r\n\r\n`/superman/superpassword/mychild1`\r\n\r\nIf I do it the second way, I can&#39;t make the parameters optional. I don&#39;t think my solution&#39;s correct in the sense. How can I make the parameters optional for all children? (I know I could define the params in the `url` for every child, but that&#39;s not good practice)\r\n\r\n\r\n** Btw, I know this is not proper authentication.. it&#39;s just an example I am using at the moment ;)\r\n",
            "link": "https://stackoverflow.com/questions/50359921/having-optional-parameters-as-parent-state-in-ui-router",
            "title": "Having optional parameters as parent state in ui-router",
            "body": "<p>I have some states in my Angular app, both having <code>myParent</code> as parent state:</p>\n\n<pre><code>var myChild1 = {\n  name: 'myChild1',\n  parent: 'myParent',\n  url: '/mychild1'\n  ...\n};\n\nvar myChild2 = {\n  name: 'myChild2',\n  parent: 'myParent',\n  url: '/mychild2'\n  ...\n};\n</code></pre>\n\n<p>and here's <code>myParent</code></p>\n\n<pre><code>var myParent = {\n  name: 'myParent',\n  abstract: true,\n  url: '/:username/:password',\n  resolve: {\n    authorized: [\n      myService,\n      '$stateParams',\n      function (myService, $stateParams) {\n        console.log($stateParams);\n\n        myService.doSomething() // unrelated\n      }\n    ]\n  }\n};\n</code></pre>\n\n<p>I want to access it like the following:</p>\n\n<p><code>/mychild1/superman/superpassword</code></p>\n\n<p>However, it's only letting me access it like so:</p>\n\n<p><code>/superman/superpassword/mychild1</code></p>\n\n<p>If I do it the second way, I can't make the parameters optional. I don't think my solution's correct in the sense. How can I make the parameters optional for all children? (I know I could define the params in the <code>url</code> for every child, but that's not good practice)</p>\n\n<p>** Btw, I know this is not proper authentication.. it's just an example I am using at the moment ;)</p>\n"
        },
        {
            "tags": [
                "acumatica"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9773408,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/d242f6be1fbf2b58f0748abd81bc5693?s=128&d=identicon&r=PG&f=1",
                "display_name": "Bob Moby",
                "link": "https://stackoverflow.com/users/9773408/bob-moby"
            },
            "is_answered": false,
            "view_count": 23,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526428170,
            "creation_date": 1525989545,
            "question_id": 50282092,
            "body_markdown": "I was playing with Acumatica Business Process and stuck in finding a way how to Create Purshase Order for Sales Order partially.\r\n\r\nI need an opportunity to create Purshase Order for 50 items from Sales Order with 100 items and then somehow to Create another Purshase Order for 50 items.\r\n\r\nIf someone has any ideas how it can be implemented I would be highly appresiate for any help according to my problem.\r\n\r\nThanks,\r\n\r\nHave a nice day or night.",
            "link": "https://stackoverflow.com/questions/50282092/create-acumatica-purchase-order-partially",
            "title": "Create Acumatica Purchase Order Partially",
            "body": "<p>I was playing with Acumatica Business Process and stuck in finding a way how to Create Purshase Order for Sales Order partially.</p>\n\n<p>I need an opportunity to create Purshase Order for 50 items from Sales Order with 100 items and then somehow to Create another Purshase Order for 50 items.</p>\n\n<p>If someone has any ideas how it can be implemented I would be highly appresiate for any help according to my problem.</p>\n\n<p>Thanks,</p>\n\n<p>Have a nice day or night.</p>\n"
        },
        {
            "tags": [
                "amazon-s3",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 2175,
                "user_id": 287455,
                "user_type": "registered",
                "accept_rate": 76,
                "profile_image": "https://www.gravatar.com/avatar/d3e050b96cc7344a7c14257f2e89892f?s=128&d=identicon&r=PG",
                "display_name": "ekeren",
                "link": "https://stackoverflow.com/users/287455/ekeren"
            },
            "is_answered": true,
            "view_count": 41,
            "accepted_answer_id": 49933693,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428170,
            "creation_date": 1524037903,
            "last_edit_date": 1526428170,
            "question_id": 49894114,
            "body_markdown": "Not like ELB logs, S3 access logs files are not structured with folders \r\n\r\n\r\nHere is an examle:\r\n\r\n`BUCKET_NAME/ACCESS_LOGS_DEST/2018-03-15-03-05-46-2E5105C8E00951B32018-03-15-03-05-46-2E5105C8E00951B3`\r\n\r\nIs there a simple way to partition them in Athena based on day? ",
            "link": "https://stackoverflow.com/questions/49894114/athena-partition-s3-access-logs",
            "title": "Athena Partition s3 access logs",
            "body": "<p>Not like ELB logs, S3 access logs files are not structured with folders </p>\n\n<p>Here is an examle:</p>\n\n<p><code>BUCKET_NAME/ACCESS_LOGS_DEST/2018-03-15-03-05-46-2E5105C8E00951B32018-03-15-03-05-46-2E5105C8E00951B3</code></p>\n\n<p>Is there a simple way to partition them in Athena based on day? </p>\n"
        },
        {
            "tags": [
                "amazon-cloudformation",
                "amazon-athena",
                "aws-glue"
            ],
            "owner": {
                "reputation": 1738,
                "user_id": 3002273,
                "user_type": "registered",
                "accept_rate": 86,
                "profile_image": "https://www.gravatar.com/avatar/14663c07f99bd2fdc191b05db9d3e900?s=128&d=identicon&r=PG&f=1",
                "display_name": "aidan.plenert.macdonald",
                "link": "https://stackoverflow.com/users/3002273/aidan-plenert-macdonald"
            },
            "is_answered": true,
            "view_count": 45,
            "accepted_answer_id": 49866520,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428164,
            "creation_date": 1523912663,
            "last_edit_date": 1526428164,
            "question_id": 49866197,
            "body_markdown": "As of [January 19, 2018 updates](https://docs.aws.amazon.com/athena/latest/ug/release-note-2018-01-19.html), Athena can skip the header row of files,\r\n\r\n&gt; Support for ignoring headers. You can use the `skip.header.line.count` property when defining tables, to allow Athena to ignore headers.\r\n\r\nI use [AWS Glue in Cloudformation](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-glue-table.html) to manage my Athena tables. Using the [Glue Table Input](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-glue-table-tableinput.html), how can I tell Athena to skip the header row?",
            "link": "https://stackoverflow.com/questions/49866197/aws-glueathena-skip-header-row",
            "title": "AWS Glue+Athena skip header row",
            "body": "<p>As of <a href=\"https://docs.aws.amazon.com/athena/latest/ug/release-note-2018-01-19.html\" rel=\"nofollow noreferrer\">January 19, 2018 updates</a>, Athena can skip the header row of files,</p>\n\n<blockquote>\n  <p>Support for ignoring headers. You can use the <code>skip.header.line.count</code> property when defining tables, to allow Athena to ignore headers.</p>\n</blockquote>\n\n<p>I use <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-glue-table.html\" rel=\"nofollow noreferrer\">AWS Glue in Cloudformation</a> to manage my Athena tables. Using the <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-glue-table-tableinput.html\" rel=\"nofollow noreferrer\">Glue Table Input</a>, how can I tell Athena to skip the header row?</p>\n"
        },
        {
            "tags": [
                "spring",
                "transactions"
            ],
            "owner": {
                "reputation": 6,
                "user_id": 9101889,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/0eeb4ebbc6758476e95e429e57168c01?s=128&d=identicon&r=PG&f=1",
                "display_name": "Gaurav Sinha",
                "link": "https://stackoverflow.com/users/9101889/gaurav-sinha"
            },
            "is_answered": false,
            "view_count": 9,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526428138,
            "creation_date": 1526428138,
            "question_id": 50360626,
            "body_markdown": "I have a spring  trsaction metod bar() which calls one more  transactional method foo which inserts to database.Now I want to roll back   entry in the table which foo has commited if there are any exceptions(checked/unchecked)   in writetoAfile() method.I tried using below way but it did not work .Can you please let me know how to handle this.\r\n\r\n\r\n\r\n    @Transaction(Proporation.REQUIRES_NEW,rollbackFor = Exception.class)\r\n        bar(){\r\n         foo()\r\n         writetoAfile()\r\n        \r\n        }\r\n        \r\n        @Transaction(Proporation.REQUIRED)\r\n         foo{\r\n          // insert into foo table\r\n        \r\n        }\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50360626/spring-nested-transaction-roll-back",
            "title": "Spring nested transaction roll back",
            "body": "<p>I have a spring  trsaction metod bar() which calls one more  transactional method foo which inserts to database.Now I want to roll back   entry in the table which foo has commited if there are any exceptions(checked/unchecked)   in writetoAfile() method.I tried using below way but it did not work .Can you please let me know how to handle this.</p>\n\n<pre><code>@Transaction(Proporation.REQUIRES_NEW,rollbackFor = Exception.class)\n    bar(){\n     foo()\n     writetoAfile()\n\n    }\n\n    @Transaction(Proporation.REQUIRED)\n     foo{\n      // insert into foo table\n\n    }\n</code></pre>\n"
        },
        {
            "tags": [
                "excel",
                "excel-vba"
            ],
            "owner": {
                "reputation": 14,
                "user_id": 4477300,
                "user_type": "registered",
                "accept_rate": 50,
                "profile_image": "https://graph.facebook.com/1540523858/picture?type=large",
                "display_name": "Patrick Handcock",
                "link": "https://stackoverflow.com/users/4477300/patrick-handcock"
            },
            "is_answered": true,
            "view_count": 5563,
            "accepted_answer_id": 28065567,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428134,
            "creation_date": 1421837256,
            "question_id": 28065252,
            "body_markdown": "I&#39;m not an excel whiz but really stuck on this one and hoping a guru out there will be able to help me out as it&#39;s really important for some research I am doing - please!! I think it will be quite a simple fix (see attached example excel sheet for context below). If you are even able to modify the formula in question and re-attach that would be awesome too!!\r\n\r\nThere are 2 datasets in sheet 1 side-by-side. The data on left (rows A to K) displays data in 10 sec time epochs, the data on right (rows N to X) in 1 min time epochs. I would like to be able to drag the formula in P2 all the way down the column P based on data in column C (as per colour coding in red and blue).\r\n\r\nYou&#39;ll note that the P2 formula is taking a SUM of C2-C7 and P3 is taking sum of C8-C13. I would like to be able to continue this pattern down the column, perhaps with a better drag-down formula (or more efficiently – as there is loads of data!) if possible. Essentially I want each single data row on the right to move onto the next block of 6 rows from data on the left. \r\n\r\nI hope I explained that well enough. Really hoping someone can help! Really important to me!\r\n\r\nPatrick\r\n\r\nSee attached excel example - thanks so much!! I will be ever grateful!\r\n\r\nhttps://www.dropbox.com/s/72r7ty9v15vzyyv/drag%20formula%20quick%20way%20-%20help.xlsx?dl=0 \r\n",
            "link": "https://stackoverflow.com/questions/28065252/repeat-a-specific-excel-formula-pattern-down-column",
            "title": "repeat a specific excel formula pattern down column",
            "body": "<p>I'm not an excel whiz but really stuck on this one and hoping a guru out there will be able to help me out as it's really important for some research I am doing - please!! I think it will be quite a simple fix (see attached example excel sheet for context below). If you are even able to modify the formula in question and re-attach that would be awesome too!!</p>\n\n<p>There are 2 datasets in sheet 1 side-by-side. The data on left (rows A to K) displays data in 10 sec time epochs, the data on right (rows N to X) in 1 min time epochs. I would like to be able to drag the formula in P2 all the way down the column P based on data in column C (as per colour coding in red and blue).</p>\n\n<p>You'll note that the P2 formula is taking a SUM of C2-C7 and P3 is taking sum of C8-C13. I would like to be able to continue this pattern down the column, perhaps with a better drag-down formula (or more efficiently – as there is loads of data!) if possible. Essentially I want each single data row on the right to move onto the next block of 6 rows from data on the left. </p>\n\n<p>I hope I explained that well enough. Really hoping someone can help! Really important to me!</p>\n\n<p>Patrick</p>\n\n<p>See attached excel example - thanks so much!! I will be ever grateful!</p>\n\n<p><a href=\"https://www.dropbox.com/s/72r7ty9v15vzyyv/drag%20formula%20quick%20way%20-%20help.xlsx?dl=0\" rel=\"nofollow\">https://www.dropbox.com/s/72r7ty9v15vzyyv/drag%20formula%20quick%20way%20-%20help.xlsx?dl=0</a> </p>\n"
        },
        {
            "tags": [
                "javascript",
                "node.js",
                "actions-on-google"
            ],
            "owner": {
                "reputation": 96,
                "user_id": 8599469,
                "user_type": "registered",
                "accept_rate": 100,
                "profile_image": "https://www.gravatar.com/avatar/483a7ca4de59ee50a2e920fb25fcda69?s=128&d=identicon&r=PG&f=1",
                "display_name": "chan3600",
                "link": "https://stackoverflow.com/users/8599469/chan3600"
            },
            "is_answered": true,
            "view_count": 53,
            "accepted_answer_id": 50360624,
            "answer_count": 2,
            "score": 0,
            "last_activity_date": 1526428129,
            "creation_date": 1526347512,
            "last_edit_date": 1526407531,
            "question_id": 50341054,
            "body_markdown": "I am trying to pass parameter value from google actions to my fulfillment.\r\nHowever, I am not able to get only the parameter value. How should I able to get only &quot;Asda&quot; under `newName` field from the argument?  Do I have to extract it from `conv` (like `conv.inputs.arguments[1].rawText`)? if like this, then what is the purpose of having a name for the parameter?\r\n\r\nRequest JSON from Google Actions:\r\n\r\n    {\r\n      &quot;user&quot;: {\r\n        &quot;userId&quot;: &quot;ABwppHEAPgcgb2yFUFURYFEJGg4VdAVcL9UKO9cS7a7rVfAMr9ht67LzgrmMseTvb5mmJjbjj7UV&quot;,\r\n        &quot;locale&quot;: &quot;en-US&quot;,\r\n        &quot;lastSeen&quot;: &quot;2018-05-15T01:08:55Z&quot;,\r\n        &quot;userStorage&quot;: &quot;{\\&quot;data\\&quot;:{}}&quot;\r\n      },\r\n      &quot;conversation&quot;: {\r\n        &quot;conversationId&quot;: &quot;1526346570079&quot;,\r\n        &quot;type&quot;: &quot;NEW&quot;\r\n      },\r\n      &quot;inputs&quot;: [\r\n        {\r\n          &quot;intent&quot;: &quot;com.example.test.RENAME&quot;,\r\n          &quot;rawInputs&quot;: [\r\n            {\r\n              &quot;inputType&quot;: &quot;KEYBOARD&quot;,\r\n              &quot;query&quot;: &quot;Talk to GoTestApp to rename Asda&quot;\r\n            }\r\n          ],\r\n          &quot;arguments&quot;: [\r\n            {\r\n              &quot;name&quot;: &quot;trigger_query&quot;,\r\n              &quot;rawText&quot;: &quot;rename Asda&quot;,\r\n              &quot;textValue&quot;: &quot;rename Asda&quot;\r\n            },\r\n            {\r\n              &quot;name&quot;: &quot;newName&quot;,\r\n              &quot;rawText&quot;: &quot;Asda&quot;,\r\n              &quot;textValue&quot;: &quot;Asda&quot;\r\n            }\r\n          ]\r\n        }\r\n      ],\r\n      &quot;surface&quot;: {\r\n        &quot;capabilities&quot;: [\r\n          {\r\n            &quot;name&quot;: &quot;actions.capability.MEDIA_RESPONSE_AUDIO&quot;\r\n          },\r\n          {\r\n            &quot;name&quot;: &quot;actions.capability.SCREEN_OUTPUT&quot;\r\n          },\r\n          {\r\n            &quot;name&quot;: &quot;actions.capability.AUDIO_OUTPUT&quot;\r\n          },\r\n          {\r\n            &quot;name&quot;: &quot;actions.capability.WEB_BROWSER&quot;\r\n          }\r\n        ]\r\n      },\r\n      &quot;isInSandbox&quot;: true,\r\n      &quot;availableSurfaces&quot;: [\r\n        {\r\n          &quot;capabilities&quot;: [\r\n            {\r\n              &quot;name&quot;: &quot;actions.capability.SCREEN_OUTPUT&quot;\r\n            },\r\n            {\r\n              &quot;name&quot;: &quot;actions.capability.AUDIO_OUTPUT&quot;\r\n            }\r\n          ]\r\n        }\r\n      ]\r\n    }\r\n\r\nMy code of Fulfillment side:\r\n\r\n    app.intent(&#39;com.example.test.RENAME&#39;, (conv, input, arg) =&gt; {\r\n      console.log(input); //print Talk to GoTestApp to rename Asda\r\n      console.log(arg); //only print &quot;rename Asda&quot;\r\n      console.log(arg[1]) //only print &quot;e&quot;\r\n    }\r\n\r\nAction Package action:\r\n\r\n      &quot;name&quot;: &quot;RENAME&quot;,\r\n      &quot;intent&quot;: {\r\n        &quot;name&quot;: &quot;com.example.test.RENAME&quot;,\r\n        &quot;parameters&quot;: [{\r\n          &quot;name&quot;: &quot;newName&quot;,\r\n          &quot;type&quot;: &quot;SchemaOrg_Text&quot;\r\n        }],\r\n        &quot;trigger&quot;: {\r\n          &quot;queryPatterns&quot;: [\r\n            &quot;rename $SchemaOrg_Text:newName&quot;\r\n          ]\r\n        }\r\n      },\r\n      &quot;fulfillment&quot;: {\r\n        &quot;conversationName&quot;: &quot;example&quot;\r\n      }\r\n    }",
            "link": "https://stackoverflow.com/questions/50341054/how-do-i-get-parameter-value-from-arguments",
            "title": "How do I get parameter value from arguments?",
            "body": "<p>I am trying to pass parameter value from google actions to my fulfillment.\nHowever, I am not able to get only the parameter value. How should I able to get only \"Asda\" under <code>newName</code> field from the argument?  Do I have to extract it from <code>conv</code> (like <code>conv.inputs.arguments[1].rawText</code>)? if like this, then what is the purpose of having a name for the parameter?</p>\n\n<p>Request JSON from Google Actions:</p>\n\n<pre><code>{\n  \"user\": {\n    \"userId\": \"ABwppHEAPgcgb2yFUFURYFEJGg4VdAVcL9UKO9cS7a7rVfAMr9ht67LzgrmMseTvb5mmJjbjj7UV\",\n    \"locale\": \"en-US\",\n    \"lastSeen\": \"2018-05-15T01:08:55Z\",\n    \"userStorage\": \"{\\\"data\\\":{}}\"\n  },\n  \"conversation\": {\n    \"conversationId\": \"1526346570079\",\n    \"type\": \"NEW\"\n  },\n  \"inputs\": [\n    {\n      \"intent\": \"com.example.test.RENAME\",\n      \"rawInputs\": [\n        {\n          \"inputType\": \"KEYBOARD\",\n          \"query\": \"Talk to GoTestApp to rename Asda\"\n        }\n      ],\n      \"arguments\": [\n        {\n          \"name\": \"trigger_query\",\n          \"rawText\": \"rename Asda\",\n          \"textValue\": \"rename Asda\"\n        },\n        {\n          \"name\": \"newName\",\n          \"rawText\": \"Asda\",\n          \"textValue\": \"Asda\"\n        }\n      ]\n    }\n  ],\n  \"surface\": {\n    \"capabilities\": [\n      {\n        \"name\": \"actions.capability.MEDIA_RESPONSE_AUDIO\"\n      },\n      {\n        \"name\": \"actions.capability.SCREEN_OUTPUT\"\n      },\n      {\n        \"name\": \"actions.capability.AUDIO_OUTPUT\"\n      },\n      {\n        \"name\": \"actions.capability.WEB_BROWSER\"\n      }\n    ]\n  },\n  \"isInSandbox\": true,\n  \"availableSurfaces\": [\n    {\n      \"capabilities\": [\n        {\n          \"name\": \"actions.capability.SCREEN_OUTPUT\"\n        },\n        {\n          \"name\": \"actions.capability.AUDIO_OUTPUT\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>\n\n<p>My code of Fulfillment side:</p>\n\n<pre><code>app.intent('com.example.test.RENAME', (conv, input, arg) =&gt; {\n  console.log(input); //print Talk to GoTestApp to rename Asda\n  console.log(arg); //only print \"rename Asda\"\n  console.log(arg[1]) //only print \"e\"\n}\n</code></pre>\n\n<p>Action Package action:</p>\n\n<pre><code>  \"name\": \"RENAME\",\n  \"intent\": {\n    \"name\": \"com.example.test.RENAME\",\n    \"parameters\": [{\n      \"name\": \"newName\",\n      \"type\": \"SchemaOrg_Text\"\n    }],\n    \"trigger\": {\n      \"queryPatterns\": [\n        \"rename $SchemaOrg_Text:newName\"\n      ]\n    }\n  },\n  \"fulfillment\": {\n    \"conversationName\": \"example\"\n  }\n}\n</code></pre>\n"
        },
        {
            "tags": [
                "azure",
                "azure-active-directory"
            ],
            "owner": {
                "reputation": 1776,
                "user_id": 491436,
                "user_type": "registered",
                "accept_rate": 82,
                "profile_image": "https://www.gravatar.com/avatar/7f135fc2f9806c62f37cb777f3adbbaa?s=128&d=identicon&r=PG",
                "display_name": "cobolstinks",
                "link": "https://stackoverflow.com/users/491436/cobolstinks"
            },
            "is_answered": false,
            "view_count": 45,
            "answer_count": 0,
            "score": 1,
            "last_activity_date": 1526428121,
            "creation_date": 1526428121,
            "question_id": 50360622,
            "body_markdown": "Does Azure AD support code grants from SPA apps?  I&#39;ve built SPA apps that use the implicit flow against AAD and they work mostly fine.  The one caveat is that if we try to get group membership in the jwt tokens, you can only get up to 5 groups listed in the &quot;group&quot; claim otherwise the jwt contains a &quot;hasGroups&quot; claim (which is next to worthless).\r\n\r\nI&#39;ve come across some online article (which I can&#39;t seem to locate at the moment) that states the group claim is capped at 5 to prevent exceeding the url max values because of extra large access or id tokens being placed in the url, and that the code grant will allow for unlimited groups in the jwt because that request goes over a http post.  \r\n\r\ntwo questions:\r\n\r\n 1. It appears that I can&#39;t do a code grant from a SPA because the AAD\r\n    token endpoint doesn&#39;t support CORS.  Is that true?  Why is that? \r\n    That seems like Microsoft admitting JS/SPA apps aren&#39;t first class\r\n    citizens.  I&#39;ve also tried to get the public keys from AAD from a\r\n    client side app and found the lack of CORS on that endpoint as well.\r\n 2. Is it true that a code grant can get more than 5 groups back embedded in the jwt?\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50360622/azure-ad-code-grant-from-spa",
            "title": "Azure AD Code Grant from SPA?",
            "body": "<p>Does Azure AD support code grants from SPA apps?  I've built SPA apps that use the implicit flow against AAD and they work mostly fine.  The one caveat is that if we try to get group membership in the jwt tokens, you can only get up to 5 groups listed in the \"group\" claim otherwise the jwt contains a \"hasGroups\" claim (which is next to worthless).</p>\n\n<p>I've come across some online article (which I can't seem to locate at the moment) that states the group claim is capped at 5 to prevent exceeding the url max values because of extra large access or id tokens being placed in the url, and that the code grant will allow for unlimited groups in the jwt because that request goes over a http post.  </p>\n\n<p>two questions:</p>\n\n<ol>\n<li>It appears that I can't do a code grant from a SPA because the AAD\ntoken endpoint doesn't support CORS.  Is that true?  Why is that? \nThat seems like Microsoft admitting JS/SPA apps aren't first class\ncitizens.  I've also tried to get the public keys from AAD from a\nclient side app and found the lack of CORS on that endpoint as well.</li>\n<li>Is it true that a code grant can get more than 5 groups back embedded in the jwt?</li>\n</ol>\n"
        },
        {
            "tags": [
                "javascript",
                "vue.js"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9128616,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/136d57c76559959e3985c4fde76fe98e?s=128&d=identicon&r=PG&f=1",
                "display_name": "Dante",
                "link": "https://stackoverflow.com/users/9128616/dante"
            },
            "is_answered": true,
            "view_count": 37,
            "answer_count": 3,
            "score": 0,
            "last_activity_date": 1526428114,
            "creation_date": 1526425477,
            "last_edit_date": 1526426255,
            "question_id": 50360304,
            "body_markdown": "I&#39;m working with VueJS, I have an object array like this:\r\n\r\n    `const myarray = [\r\n        {&#39;secction&#39;: &#39;6.2.3&#39;,&#39;title&#39;: &#39;a&#39;},\r\n        {&#39;secction&#39;: &#39;6.2.2&#39;,&#39;title&#39;: &#39;b&#39;},\r\n        {&#39;secction&#39;: &#39;11.3.1&#39;,&#39;title&#39;: &#39;bn&#39;},\r\n        {&#39;secction&#39;: &#39;10.5.1&#39;,&#39;title&#39;: &#39;z&#39;},\r\n        {&#39;secction&#39;: &#39;10.4.1&#39;, title: &#39;da&#39;}\r\n    ]`\r\n\r\nI want to order like: \r\n\r\n    6.2.3\r\n    6.2.2\r\n    10.4.1\r\n    10.5.1\r\n    11.3.1\r\n\r\nbut I aplied this fuction: \r\n\r\n    myarray.sort( (a ,b) =&gt; {\r\n        if (a.Control_num &lt; b.Control_num) return -1\r\n        if (a.Control_num &gt; b.Control_num) return 1\r\n            return 0\r\n    })\r\n\r\nand the result is the following:\r\n\r\n\r\n    10.4.1\r\n    10.5.1\r\n    10.6.2 \r\n    11.2.2\r\n    11.3.1\r\n    6.2.2\r\n    6.2.3\r\n",
            "link": "https://stackoverflow.com/questions/50360304/how-i-can-order-like-a-index-book-in-javascript",
            "title": "How i can order like a index book in Javascript?",
            "body": "<p>I'm working with VueJS, I have an object array like this:</p>\n\n<pre><code>`const myarray = [\n    {'secction': '6.2.3','title': 'a'},\n    {'secction': '6.2.2','title': 'b'},\n    {'secction': '11.3.1','title': 'bn'},\n    {'secction': '10.5.1','title': 'z'},\n    {'secction': '10.4.1', title: 'da'}\n]`\n</code></pre>\n\n<p>I want to order like: </p>\n\n<pre><code>6.2.3\n6.2.2\n10.4.1\n10.5.1\n11.3.1\n</code></pre>\n\n<p>but I aplied this fuction: </p>\n\n<pre><code>myarray.sort( (a ,b) =&gt; {\n    if (a.Control_num &lt; b.Control_num) return -1\n    if (a.Control_num &gt; b.Control_num) return 1\n        return 0\n})\n</code></pre>\n\n<p>and the result is the following:</p>\n\n<pre><code>10.4.1\n10.5.1\n10.6.2 \n11.2.2\n11.3.1\n6.2.2\n6.2.3\n</code></pre>\n"
        },
        {
            "tags": [
                "apache-spark",
                "amazon-s3",
                "amazon-redshift",
                "amazon-athena",
                "aws-glue"
            ],
            "owner": {
                "reputation": 220,
                "user_id": 5235665,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/86093c41776bab1aa6cb1a3b33734672?s=128&d=identicon&r=PG&f=1",
                "display_name": "hotmeatballsoup",
                "link": "https://stackoverflow.com/users/5235665/hotmeatballsoup"
            },
            "is_answered": true,
            "view_count": 41,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428110,
            "creation_date": 1526348817,
            "last_edit_date": 1526428110,
            "question_id": 50341192,
            "body_markdown": "I should preface this with the fact that I&#39;m using Enhanced VPC Routing for my AWS account, which [precludes me](https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html) from using the traditional S3 to Redshift querying:\r\n\r\n&gt; &quot;*Your cluster can&#39;t have Enhanced VPC Routing enabled. (to use Spectrum)*&quot;\r\n\r\n---\r\n\r\nMy *understanding* is that AWS Redshift is a high-octane Postgres-as-a-service that is optimized for extremely fast reads over large data volumes. So if you *lots* of have relational data that you want to query/analyze, then Redshift is a good choice for you.\r\n\r\nMy *understanding* of AWS Athena is that its just using something like Apache Drill (or similar) to provide a SQL-like interface over *any* data stored in S3 buckets (relational and otherwise, as well as any format: unstructured plaintext, JSON, XML, etc.). So if you just have data in S3 that you want to query via SQL-like syntax, Athena is a good choice for you.\r\n\r\n**To begin with, can anyone begin by confirming/clarifying my understanding above?** Assuming I&#39;m more or less correct...\r\n\r\nI have structured/relational (stored in JSON and CSV files) that lives on S3. I&#39;d like to create an ETL process that reads this data out of S3 and dumps it into Redshift so that downstream processes can analyze it.\r\n\r\nSo I&#39;m thinking about creating a Spark-based ETL pipeline whereby:\r\n\r\n1. Spark uses Athena to query S3 data into `DataFrames`; I&#39;m also wondering if AWS Glue can possibly do some heavy lifting here\r\n2. Spark writes the contents of those `DataFrames` to Redshift\r\n\r\nSo my question: is this the most efficient way to port LARGE amounts of partially-structured/relational S3 data (again stored in various file formats) into Redshift, or is there a better/simpler way?",
            "link": "https://stackoverflow.com/questions/50341192/porting-partially-relational-s3-data-into-redshift-via-spark-and-glue",
            "title": "Porting partially-relational S3 data into Redshift via Spark and Glue",
            "body": "<p>I should preface this with the fact that I'm using Enhanced VPC Routing for my AWS account, which <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\" rel=\"nofollow noreferrer\">precludes me</a> from using the traditional S3 to Redshift querying:</p>\n\n<blockquote>\n  <p>\"<em>Your cluster can't have Enhanced VPC Routing enabled. (to use Spectrum)</em>\"</p>\n</blockquote>\n\n<hr>\n\n<p>My <em>understanding</em> is that AWS Redshift is a high-octane Postgres-as-a-service that is optimized for extremely fast reads over large data volumes. So if you <em>lots</em> of have relational data that you want to query/analyze, then Redshift is a good choice for you.</p>\n\n<p>My <em>understanding</em> of AWS Athena is that its just using something like Apache Drill (or similar) to provide a SQL-like interface over <em>any</em> data stored in S3 buckets (relational and otherwise, as well as any format: unstructured plaintext, JSON, XML, etc.). So if you just have data in S3 that you want to query via SQL-like syntax, Athena is a good choice for you.</p>\n\n<p><strong>To begin with, can anyone begin by confirming/clarifying my understanding above?</strong> Assuming I'm more or less correct...</p>\n\n<p>I have structured/relational (stored in JSON and CSV files) that lives on S3. I'd like to create an ETL process that reads this data out of S3 and dumps it into Redshift so that downstream processes can analyze it.</p>\n\n<p>So I'm thinking about creating a Spark-based ETL pipeline whereby:</p>\n\n<ol>\n<li>Spark uses Athena to query S3 data into <code>DataFrames</code>; I'm also wondering if AWS Glue can possibly do some heavy lifting here</li>\n<li>Spark writes the contents of those <code>DataFrames</code> to Redshift</li>\n</ol>\n\n<p>So my question: is this the most efficient way to port LARGE amounts of partially-structured/relational S3 data (again stored in various file formats) into Redshift, or is there a better/simpler way?</p>\n"
        },
        {
            "tags": [
                "wordpress",
                "optimization",
                "coding-style",
                "affiliate"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9796800,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
                "display_name": "Raghav Sehgal",
                "link": "https://stackoverflow.com/users/9796800/raghav-sehgal"
            },
            "is_answered": false,
            "view_count": 26,
            "closed_date": 1526428116,
            "answer_count": 0,
            "score": -6,
            "last_activity_date": 1526428104,
            "creation_date": 1526420018,
            "last_edit_date": 1526428104,
            "question_id": 50359464,
            "body_markdown": "My wordpress website has been completely developed through the wordpress content management system. The wordpress software creates more files in my root directory. Therefore creating more requests and hence increasing the page load time. Any work arounds?",
            "link": "https://stackoverflow.com/questions/50359464/how-can-i-make-convert-my-wordpress-website-into-html",
            "closed_reason": "off-topic",
            "title": "How can I make convert my wordpress website into html?",
            "body": "<p>My wordpress website has been completely developed through the wordpress content management system. The wordpress software creates more files in my root directory. Therefore creating more requests and hence increasing the page load time. Any work arounds?</p>\n"
        },
        {
            "tags": [
                "amazon-s3",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 1095478,
                "user_type": "registered",
                "profile_image": "https://i.stack.imgur.com/sAeDI.jpg?s=128&g=1",
                "display_name": "al.moorthi",
                "link": "https://stackoverflow.com/users/1095478/al-moorthi"
            },
            "is_answered": true,
            "view_count": 17,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428100,
            "creation_date": 1526304848,
            "last_edit_date": 1526428100,
            "question_id": 50331678,
            "body_markdown": "I have a CSV (tab separated) in s3 that needs to be queried on a JSON field.\r\n\r\n    uid\\tname\\taddress\r\n    1\\tmoorthi\\t{&quot;rno&quot;:123,&quot;code&quot;:400111}\r\n    2\\tkiranp\\t{&quot;rno&quot;:124,&quot;street&quot;:&quot;kemp road&quot;}\r\n\r\nHow can I query this data in Amazon Athena?\r\n\r\nI should be able to query like:\r\n\r\n    select uid\r\n    from table1\r\n    where address[&#39;street&#39;]=&quot;kemp road&quot;;\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50331678/how-do-i-load-csv-file-to-amazon-athena-that-contains-json-field",
            "title": "How do I load CSV file to Amazon Athena that contains JSON field",
            "body": "<p>I have a CSV (tab separated) in s3 that needs to be queried on a JSON field.</p>\n\n<pre><code>uid\\tname\\taddress\n1\\tmoorthi\\t{\"rno\":123,\"code\":400111}\n2\\tkiranp\\t{\"rno\":124,\"street\":\"kemp road\"}\n</code></pre>\n\n<p>How can I query this data in Amazon Athena?</p>\n\n<p>I should be able to query like:</p>\n\n<pre><code>select uid\nfrom table1\nwhere address['street']=\"kemp road\";\n</code></pre>\n"
        },
        {
            "tags": [
                "python",
                "powershell",
                "pip"
            ],
            "owner": {
                "reputation": 43,
                "user_id": 3673947,
                "user_type": "registered",
                "accept_rate": 40,
                "profile_image": "https://www.gravatar.com/avatar/9eda25ed45a9c26d58b3f7714d6a41ea?s=128&d=identicon&r=PG&f=1",
                "display_name": "CathyQian",
                "link": "https://stackoverflow.com/users/3673947/cathyqian"
            },
            "is_answered": false,
            "view_count": 21,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526428087,
            "creation_date": 1526428087,
            "question_id": 50360615,
            "body_markdown": "I&#39;m trying to install rake (https://github.com/zelandiya/RAKE-tutorial) as instructed on the README file using \r\n\r\n    python setup.py install. \r\n\r\nHowever, I always get the following error:\r\n\r\n    Command &quot;python setup.py egg_info&quot; failed with error code 1 in C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\pip-install-jz29g5dr\\rake\\\r\n\r\nI tried almost everything I can find online including this comprehensive post: https://stackoverflow.com/questions/35991403/pip-install-returns-python-setup-py-egg-info-failed-with-error-code-1but still can&#39;t find the solution. Any help is needed. Thanks a lot!",
            "link": "https://stackoverflow.com/questions/50360615/error-while-installing-rake",
            "title": "Error while installing rake",
            "body": "<p>I'm trying to install rake (<a href=\"https://github.com/zelandiya/RAKE-tutorial\" rel=\"nofollow noreferrer\">https://github.com/zelandiya/RAKE-tutorial</a>) as instructed on the README file using </p>\n\n<pre><code>python setup.py install. \n</code></pre>\n\n<p>However, I always get the following error:</p>\n\n<pre><code>Command \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\pip-install-jz29g5dr\\rake\\\n</code></pre>\n\n<p>I tried almost everything I can find online including this comprehensive post: <a href=\"https://stackoverflow.com/questions/35991403/pip-install-returns-python-setup-py-egg-info-failed-with-error-code-1but\">pip install returns &quot;python setup.py egg_info&quot; failed with error code 1</a> still can't find the solution. Any help is needed. Thanks a lot!</p>\n"
        },
        {
            "tags": [
                "python",
                "amazon-web-services",
                "amazon-s3",
                "boto3",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 13,
                "user_id": 8604194,
                "user_type": "registered",
                "accept_rate": 33,
                "profile_image": "https://i.stack.imgur.com/x9auS.jpg?s=128&g=1",
                "display_name": "pyhotshot",
                "link": "https://stackoverflow.com/users/8604194/pyhotshot"
            },
            "is_answered": false,
            "view_count": 22,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526428085,
            "creation_date": 1525455644,
            "last_edit_date": 1526428085,
            "question_id": 50180425,
            "body_markdown": "i have a lambda that will query Athena and drop the output of the results in my desired bucket. Athena output contains .csv and .csv.metadata. i don&#39;t want to get .metadata file along with the .csv file when my lambda drops it. here is my code:\r\n\r\n    def wait_for_result(athena, query_id):state = athena.get_query_execution(QueryExecutionId=query_id)[&#39;QueryExecution&#39;][&#39;Status&#39;][&#39;State&#39;]\r\n        while state != &#39;SUCCEEDED&#39;:\r\n        print(&#39;Query state: {}&#39;.format(state))\r\n        time.sleep(5)\r\n        state = athena.get_query_execution(QueryExecutionId=query_id)[&#39;QueryExecution&#39;][&#39;Status&#39;][&#39;State&#39;]\r\n    \r\n\r\n    def lambda_handler(event, context):\r\n        short_date = event[&#39;record&#39;][&#39;short_date&#39;]\r\n\r\n        bucket = &#39;test-rod-us-east-1-orders&#39;\r\n        s3_output = &#39;s3://{0}/arda-orders/f=csv/short_date={1}&#39;.format(bucket, short_date)\r\n\r\n        query = &#39;query_here&#39;.format(short_date)\r\n\r\n        boto_session = assume_role(&#39;arn:aws:iam::account-id:role/test-contr-etl-ec2-role&#39;)\r\n        session = assume_role(&#39;arn:aws:iam::account-id:role/test-xacct-rod-consumer&#39;, boto_session)\r\n\r\n        athena = session.client(&#39;athena&#39;)\r\n        s3 = session.client(&#39;s3&#39;)\r\n        s3_bucket = session.resource(&#39;s3&#39;).Bucket(bucket)\r\n\r\n        response = athena.start_query_execution(QueryString=query,\r\n                                            QueryExecutionContext={\r\n                                                &#39;Database&#39;: &#39;datapond&#39;\r\n                                            },\r\n                                            ResultConfiguration={\r\n                                                &#39;OutputLocation&#39;: s3_output\r\n                                            })\r\n\r\n        query_id = response[&#39;QueryExecutionId&#39;]\r\n        wait_for_result(athena, query_id)\r\n\r\n        # print (&#39;short_date: {}&#39;.format(short_date))\r\n        for key in s3.list_objects(Bucket=bucket)[&#39;Contents&#39;]:\r\n           if short_date in key[&#39;Key&#39;]:\r\n              s3.put_object_acl(ACL=&#39;bucket-owner-full-control&#39;, Bucket=bucket, Key=key[&#39;Key&#39;])\r\n               print(&#39;set \\&#39;bucket-owner-full-control\\&#39; for {}&#39;.format(key[&#39;Key&#39;]))\r\n                 if &#39;.csv.metadata&#39; in key[&#39;Key&#39;]:\r\n                    s3_bucket.delete_objects(\r\n                       Delete={\r\n                         &#39;Objects&#39;: [\r\n                             {&#39;Key&#39;: key[&#39;Key&#39;]},\r\n                        ]\r\n                    }\r\n                )\r\n\r\n                 print(&#39;deleted {}&#39;.format(key[&#39;Key&#39;]))\r\n\r\n       sqs.delete_message(\r\n           QueueUrl=sqs_queue_url,\r\n           ReceiptHandle=event[&#39;receipt_handler&#39;]\r\n       )\r\n\r\n       print (&#39;Complete process for short_date: {}&#39;.format(short_date))\r\n\r\n\r\n\r\n\r\ni just get the &quot;deleted key&quot; message in the logs but i still find the .csv.metadata file in the s3 bucket. please help",
            "link": "https://stackoverflow.com/questions/50180425/ignore-csv-metadata-file-from-athena-output",
            "title": "ignore .csv.metadata file from athena output",
            "body": "<p>i have a lambda that will query Athena and drop the output of the results in my desired bucket. Athena output contains .csv and .csv.metadata. i don't want to get .metadata file along with the .csv file when my lambda drops it. here is my code:</p>\n\n<pre><code>def wait_for_result(athena, query_id):state = athena.get_query_execution(QueryExecutionId=query_id)['QueryExecution']['Status']['State']\n    while state != 'SUCCEEDED':\n    print('Query state: {}'.format(state))\n    time.sleep(5)\n    state = athena.get_query_execution(QueryExecutionId=query_id)['QueryExecution']['Status']['State']\n\n\ndef lambda_handler(event, context):\n    short_date = event['record']['short_date']\n\n    bucket = 'test-rod-us-east-1-orders'\n    s3_output = 's3://{0}/arda-orders/f=csv/short_date={1}'.format(bucket, short_date)\n\n    query = 'query_here'.format(short_date)\n\n    boto_session = assume_role('arn:aws:iam::account-id:role/test-contr-etl-ec2-role')\n    session = assume_role('arn:aws:iam::account-id:role/test-xacct-rod-consumer', boto_session)\n\n    athena = session.client('athena')\n    s3 = session.client('s3')\n    s3_bucket = session.resource('s3').Bucket(bucket)\n\n    response = athena.start_query_execution(QueryString=query,\n                                        QueryExecutionContext={\n                                            'Database': 'datapond'\n                                        },\n                                        ResultConfiguration={\n                                            'OutputLocation': s3_output\n                                        })\n\n    query_id = response['QueryExecutionId']\n    wait_for_result(athena, query_id)\n\n    # print ('short_date: {}'.format(short_date))\n    for key in s3.list_objects(Bucket=bucket)['Contents']:\n       if short_date in key['Key']:\n          s3.put_object_acl(ACL='bucket-owner-full-control', Bucket=bucket, Key=key['Key'])\n           print('set \\'bucket-owner-full-control\\' for {}'.format(key['Key']))\n             if '.csv.metadata' in key['Key']:\n                s3_bucket.delete_objects(\n                   Delete={\n                     'Objects': [\n                         {'Key': key['Key']},\n                    ]\n                }\n            )\n\n             print('deleted {}'.format(key['Key']))\n\n   sqs.delete_message(\n       QueueUrl=sqs_queue_url,\n       ReceiptHandle=event['receipt_handler']\n   )\n\n   print ('Complete process for short_date: {}'.format(short_date))\n</code></pre>\n\n<p>i just get the \"deleted key\" message in the logs but i still find the .csv.metadata file in the s3 bucket. please help</p>\n"
        },
        {
            "tags": [
                "prestodb"
            ],
            "owner": {
                "reputation": 10,
                "user_id": 1555127,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/5e8446751961b17d4b4670cc66fa0d65?s=128&d=identicon&r=PG",
                "display_name": "ni30rocks",
                "link": "https://stackoverflow.com/users/1555127/ni30rocks"
            },
            "is_answered": false,
            "view_count": 17,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526428073,
            "creation_date": 1525413381,
            "last_edit_date": 1526428073,
            "question_id": 50168070,
            "body_markdown": "I am trying to do something similar to https://stackoverflow.com/questions/26274949/mysql-top5-and-sum-remaining-as-others, but in presto or Athena. However, I am not able to find anything similar.\r\n\r\nSome other reference link http://www.silota.com/docs/recipes/sql-top-n-aggregate-rest-other.html\r\n",
            "link": "https://stackoverflow.com/questions/50168070/presto-athena-topk-and-sum-remaining-as-others",
            "title": "Presto / Athena TOPK and sum remaining as others",
            "body": "<p>I am trying to do something similar to <a href=\"https://stackoverflow.com/questions/26274949/mysql-top5-and-sum-remaining-as-others\">Mysql top5 and sum remaining as others</a>, but in presto or Athena. However, I am not able to find anything similar.</p>\n\n<p>Some other reference link <a href=\"http://www.silota.com/docs/recipes/sql-top-n-aggregate-rest-other.html\" rel=\"nofollow noreferrer\">http://www.silota.com/docs/recipes/sql-top-n-aggregate-rest-other.html</a></p>\n"
        },
        {
            "tags": [
                "amazon-s3",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 403,
                "user_id": 4961391,
                "user_type": "registered",
                "accept_rate": 95,
                "profile_image": "https://www.gravatar.com/avatar/df72c3b624d678670e8411806d92ed12?s=128&d=identicon&r=PG&f=1",
                "display_name": "2Big2BeSmall",
                "link": "https://stackoverflow.com/users/4961391/2big2besmall"
            },
            "is_answered": true,
            "view_count": 41,
            "accepted_answer_id": 50087196,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428068,
            "creation_date": 1525005116,
            "last_edit_date": 1526428068,
            "question_id": 50086587,
            "body_markdown": "I have started using Athena Query engine on top of my S3 FILEs\r\nsome of them are timestamp format columns.\r\n\r\nI have created a simple table with 2 columns\r\n\r\n    CREATE EXTERNAL TABLE `test`(\r\n      `date_x` timestamp, \r\n      `clicks` int)\r\n    ROW FORMAT DELIMITED \r\n      FIELDS TERMINATED BY &#39;,&#39; \r\n    STORED AS INPUTFORMAT \r\n      &#39;org.apache.hadoop.mapred.TextInputFormat&#39; \r\n    OUTPUTFORMAT \r\n      &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;\r\n    LOCATION\r\n      &#39;s3://aws-athena-query-results-123-us-east-1/test&#39;\r\n    TBLPROPERTIES (\r\n      &#39;has_encrypted_data&#39;=&#39;false&#39;, \r\n      &#39;transient_lastDdlTime&#39;=&#39;1525003090&#39;)\r\n\r\n\r\nI have tried to load a file and query it with Athena:\r\nwhich look like that:\r\n\r\n\r\n\r\n    &quot;2018-08-09 06:00:00.000&quot;,12\r\n    &quot;2018-08-09 06:00:00.000&quot;,42\r\n    &quot;2018-08-09 06:00:00.000&quot;,22\r\n\r\n\r\n\r\nI have tried a different type format of timestamps such as DD/MM/YYYY AND YYY-MM-DD..., tried setting the time zone for each row - but none of them worked.\r\n\r\nEach value I have tried is showing in Athena as this results:\r\n\r\n        \tdate_x\tclicks\r\n            1\t\t12\r\n            2\t\t42\r\n            3\t\t22\r\n\r\nI have tried using a CSV file with and without headers\r\ntried using with and without quotation marks,\r\nBut all of them showing defected timestamp.\r\nMy column on Athena must be Timestamp - rather it without timezone.\r\nPlease don&#39;t offer to use STRING column or DATE columns, this is not what i need.\r\n\r\nHow should the CSV File look like so Athena will recognize the Timestamp column?",
            "link": "https://stackoverflow.com/questions/50086587/load-csv-with-timestamp-column-to-athena-table",
            "title": "Load csv with timestamp column to athena table",
            "body": "<p>I have started using Athena Query engine on top of my S3 FILEs\nsome of them are timestamp format columns.</p>\n\n<p>I have created a simple table with 2 columns</p>\n\n<pre><code>CREATE EXTERNAL TABLE `test`(\n  `date_x` timestamp, \n  `clicks` int)\nROW FORMAT DELIMITED \n  FIELDS TERMINATED BY ',' \nSTORED AS INPUTFORMAT \n  'org.apache.hadoop.mapred.TextInputFormat' \nOUTPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n  's3://aws-athena-query-results-123-us-east-1/test'\nTBLPROPERTIES (\n  'has_encrypted_data'='false', \n  'transient_lastDdlTime'='1525003090')\n</code></pre>\n\n<p>I have tried to load a file and query it with Athena:\nwhich look like that:</p>\n\n<pre><code>\"2018-08-09 06:00:00.000\",12\n\"2018-08-09 06:00:00.000\",42\n\"2018-08-09 06:00:00.000\",22\n</code></pre>\n\n<p>I have tried a different type format of timestamps such as DD/MM/YYYY AND YYY-MM-DD..., tried setting the time zone for each row - but none of them worked.</p>\n\n<p>Each value I have tried is showing in Athena as this results:</p>\n\n<pre><code>        date_x  clicks\n        1       12\n        2       42\n        3       22\n</code></pre>\n\n<p>I have tried using a CSV file with and without headers\ntried using with and without quotation marks,\nBut all of them showing defected timestamp.\nMy column on Athena must be Timestamp - rather it without timezone.\nPlease don't offer to use STRING column or DATE columns, this is not what i need.</p>\n\n<p>How should the CSV File look like so Athena will recognize the Timestamp column?</p>\n"
        },
        {
            "tags": [
                "python",
                "pip",
                "pycharm",
                "pygobject"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9735679,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/8111c995c3e4660322a865b63763ab3c?s=128&d=identicon&r=PG&f=1",
                "display_name": "Ratchet Hundreda",
                "link": "https://stackoverflow.com/users/9735679/ratchet-hundreda"
            },
            "is_answered": false,
            "view_count": 29,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428054,
            "creation_date": 1525355533,
            "question_id": 50156741,
            "body_markdown": "I&#39;m trying to install PyGObject on a Python2.7 environment in PyCharm which fails with the following details:\r\n\r\n    Running setup.py clean for PyGObject\r\n    Failed to build PyGObject\r\n    Installing collected packages: PyGObject\r\n      Running setup.py install for PyGObject: started\r\n        Running setup.py install for PyGObject: finished with status &#39;error&#39;\r\n        Complete output from command &quot;C:\\Users\\ratch\\PycharmProjects\\Project\\venv\\Scripts\\python.exe&quot; -u -c &quot;import setuptools, tokenize;__file__=&#39;C:\\\\Users\\\\ratch\\\\AppData\\\\Local\\\\Temp\\\\pycharm-packaging\\\\PyGObject\\\\setup.py&#39;;f=getattr(tokenize, &#39;open&#39;, open)(__file__);code=f.read().replace(&#39;\\r\\n&#39;, &#39;\\n&#39;);f.close();exec(compile(code, __file__, &#39;exec&#39;))&quot; install --record c:\\users\\ratch\\appdata\\local\\temp\\pip-record-m6_taf\\install-record.txt --single-version-externally-managed --compile --install-headers &quot;C:\\Users\\ratch\\PycharmProjects\\Project\\venv\\include\\site\\python2.7\\PyGObject&quot;:\r\n        running install\r\n        running build\r\n        running build_py\r\n        creating build\r\n        creating build\\lib.win-amd64-2.7\r\n        creating build\\lib.win-amd64-2.7\\pygtkcompat\r\n        copying pygtkcompat\\generictreemodel.py -&gt; build\\lib.win-amd64-2.7\\pygtkcompat\r\n        copying pygtkcompat\\pygtkcompat.py -&gt; build\\lib.win-amd64-2.7\\pygtkcompat\r\n        copying pygtkcompat\\__init__.py -&gt; build\\lib.win-amd64-2.7\\pygtkcompat\r\n        creating build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\docstring.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\importer.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\module.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\pygtkcompat.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\types.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\_constants.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\_error.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\_option.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\_ossighelper.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\_propertyhelper.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\_signalhelper.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\__init__.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        creating build\\lib.win-amd64-2.7\\gi\\repository\r\n        copying gi\\repository\\__init__.py -&gt; build\\lib.win-amd64-2.7\\gi\\repository\r\n        creating build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\Gdk.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\GIMarshallingTests.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\Gio.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\GLib.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\GObject.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\Gtk.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\keysyms.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\Pango.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\__init__.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        warning: build_py: byte-compiling is disabled, skipping.\r\n        \r\n        running build_ext\r\n        pycairo: new API\r\n        pycairo: trying include directory: &#39;C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\lib\\\\site-packages\\\\cairo&#39;\r\n        pycairo: header file (&#39;C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\lib\\\\site-packages\\\\cairo\\\\pycairo.h&#39;) not found\r\n        pycairo: old API\r\n        pycairo: found pycairo 1.16.3 (c:\\users\\ratch\\pycharmprojects\\Project\\venv\\lib\\site-packages)\r\n        pycairo: trying include directory: &#39;C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\include\\\\pycairo&#39;\r\n        pycairo: found &#39;C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\include\\\\pycairo\\\\pycairo.h&#39;\r\n        building &#39;gi._gi&#39; extension\r\n        creating build\\temp.win-amd64-2.7\r\n        creating build\\temp.win-amd64-2.7\\Release\r\n        creating build\\temp.win-amd64-2.7\\Release\\gi\r\n        C:\\Users\\ratch\\AppData\\Local\\Programs\\Common\\Microsoft\\Visual C++ for Python\\9.0\\VC\\Bin\\amd64\\cl.exe /c /nologo /Ox /MD /W3 /GS- /DNDEBUG -DHAVE_CONFIG_H -DPY_SSIZE_T_CLEAN -IC:\\Users\\ratch\\AppData\\Local\\Temp\\pycharm-packaging\\PyGObject -IC:\\Users\\ratch\\AppData\\Local\\Temp\\pycharm-packaging\\PyGObject\\gi -IC:\\Python27\\include &quot;-IC:\\Users\\ratch\\PycharmProjects\\Project\\venv\\PC&quot; /Tcgi\\gimodule.c /Fobuild\\temp.win-amd64-2.7\\Release\\gi\\gimodule.obj\r\n        gimodule.c\r\n        gi\\gimodule.c(25) : fatal error C1083: Cannot open include file: &#39;glib-object.h&#39;: No such file or directory\r\n        error: command &#39;C:\\\\Users\\\\ratch\\\\AppData\\\\Local\\\\Programs\\\\Common\\\\Microsoft\\\\Visual C++ for Python\\\\9.0\\\\VC\\\\Bin\\\\amd64\\\\cl.exe&#39; failed with exit status 2\r\n\r\nI&#39;ve installed the Microsoft Visual C++ Compiler for Python 2.7 and the Windows 10 SDK properly as far as i know. I would ask for help on getting the environment set up correctly preferably in PyCharm.",
            "link": "https://stackoverflow.com/questions/50156741/installing-pygobject-through-pycharm-yields-error-command-c-cl-exe-fail",
            "title": "Installing PyGObject through PyCharm yields error command &#39;C:\\\\...\\\\cl.exe&#39; failed with exit status 2",
            "body": "<p>I'm trying to install PyGObject on a Python2.7 environment in PyCharm which fails with the following details:</p>\n\n<pre><code>Running setup.py clean for PyGObject\nFailed to build PyGObject\nInstalling collected packages: PyGObject\n  Running setup.py install for PyGObject: started\n    Running setup.py install for PyGObject: finished with status 'error'\n    Complete output from command \"C:\\Users\\ratch\\PycharmProjects\\Project\\venv\\Scripts\\python.exe\" -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\ratch\\\\AppData\\\\Local\\\\Temp\\\\pycharm-packaging\\\\PyGObject\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record c:\\users\\ratch\\appdata\\local\\temp\\pip-record-m6_taf\\install-record.txt --single-version-externally-managed --compile --install-headers \"C:\\Users\\ratch\\PycharmProjects\\Project\\venv\\include\\site\\python2.7\\PyGObject\":\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\\lib.win-amd64-2.7\n    creating build\\lib.win-amd64-2.7\\pygtkcompat\n    copying pygtkcompat\\generictreemodel.py -&gt; build\\lib.win-amd64-2.7\\pygtkcompat\n    copying pygtkcompat\\pygtkcompat.py -&gt; build\\lib.win-amd64-2.7\\pygtkcompat\n    copying pygtkcompat\\__init__.py -&gt; build\\lib.win-amd64-2.7\\pygtkcompat\n    creating build\\lib.win-amd64-2.7\\gi\n    copying gi\\docstring.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\importer.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\module.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\pygtkcompat.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\types.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\_constants.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\_error.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\_option.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\_ossighelper.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\_propertyhelper.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\_signalhelper.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\__init__.py -&gt; build\\lib.win-amd64-2.7\\gi\n    creating build\\lib.win-amd64-2.7\\gi\\repository\n    copying gi\\repository\\__init__.py -&gt; build\\lib.win-amd64-2.7\\gi\\repository\n    creating build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\Gdk.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\GIMarshallingTests.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\Gio.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\GLib.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\GObject.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\Gtk.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\keysyms.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\Pango.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\__init__.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    warning: build_py: byte-compiling is disabled, skipping.\n\n    running build_ext\n    pycairo: new API\n    pycairo: trying include directory: 'C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\lib\\\\site-packages\\\\cairo'\n    pycairo: header file ('C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\lib\\\\site-packages\\\\cairo\\\\pycairo.h') not found\n    pycairo: old API\n    pycairo: found pycairo 1.16.3 (c:\\users\\ratch\\pycharmprojects\\Project\\venv\\lib\\site-packages)\n    pycairo: trying include directory: 'C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\include\\\\pycairo'\n    pycairo: found 'C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\include\\\\pycairo\\\\pycairo.h'\n    building 'gi._gi' extension\n    creating build\\temp.win-amd64-2.7\n    creating build\\temp.win-amd64-2.7\\Release\n    creating build\\temp.win-amd64-2.7\\Release\\gi\n    C:\\Users\\ratch\\AppData\\Local\\Programs\\Common\\Microsoft\\Visual C++ for Python\\9.0\\VC\\Bin\\amd64\\cl.exe /c /nologo /Ox /MD /W3 /GS- /DNDEBUG -DHAVE_CONFIG_H -DPY_SSIZE_T_CLEAN -IC:\\Users\\ratch\\AppData\\Local\\Temp\\pycharm-packaging\\PyGObject -IC:\\Users\\ratch\\AppData\\Local\\Temp\\pycharm-packaging\\PyGObject\\gi -IC:\\Python27\\include \"-IC:\\Users\\ratch\\PycharmProjects\\Project\\venv\\PC\" /Tcgi\\gimodule.c /Fobuild\\temp.win-amd64-2.7\\Release\\gi\\gimodule.obj\n    gimodule.c\n    gi\\gimodule.c(25) : fatal error C1083: Cannot open include file: 'glib-object.h': No such file or directory\n    error: command 'C:\\\\Users\\\\ratch\\\\AppData\\\\Local\\\\Programs\\\\Common\\\\Microsoft\\\\Visual C++ for Python\\\\9.0\\\\VC\\\\Bin\\\\amd64\\\\cl.exe' failed with exit status 2\n</code></pre>\n\n<p>I've installed the Microsoft Visual C++ Compiler for Python 2.7 and the Windows 10 SDK properly as far as i know. I would ask for help on getting the environment set up correctly preferably in PyCharm.</p>\n"
        },
        {
            "tags": [
                "r",
                "sqlite",
                "dplyr"
            ],
            "owner": {
                "reputation": 3194,
                "user_id": 152860,
                "user_type": "registered",
                "accept_rate": 81,
                "profile_image": "https://www.gravatar.com/avatar/e87b25ac6bec6427d102828128f8ac80?s=128&d=identicon&r=PG",
                "display_name": "Hedgehog",
                "link": "https://stackoverflow.com/users/152860/hedgehog"
            },
            "is_answered": false,
            "view_count": 33,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1526428029,
            "creation_date": 1526357314,
            "last_edit_date": 1526428029,
            "question_id": 50342182,
            "body_markdown": "Your data is stored in a large number of SQLite data base files.\r\nYou would like to gather the data from one table across all these database files.\r\n\r\nIs this possible using `dplyr`, or `tidyverse`?\r\n\r\n**Example Data:**\r\n\r\n    # Required Libraries\r\n    require(&#39;tidyverse&#39;)\r\n    require(&#39;RSQLite&#39;)\r\n    require(&#39;pool&#39;)\r\n    require(&#39;here&#39;)\r\n    \r\n    # Create the dummy data\r\n    test &lt;- data.frame(t(replicate(2,sample(0:10,4,rep=TRUE))))\r\n    \r\n    fn &lt;- here::here(&#39;1testing.sqlite3&#39;)\r\n    con &lt;- dbPool(drv = RSQLite::SQLite(), \r\n                  dbname = fn)\r\n    write_result = dbWriteTable(con, &quot;TEST&quot;, test)\r\n    poolClose(con)\r\n    rm(con)\r\n    \r\n    # Create multiple SQLite databases\r\n    fn = here::here(&#39;1testing.sqlite3&#39;)\r\n    file.copy(from=fn, to=here::here(&#39;2testing.sqlite3&#39;))\r\n    file.copy(from=fn, to=here::here(&#39;3testing.sqlite3&#39;))\r\n    \r\n\r\n**NOTE:** The accepted answer suggests creating a user-defined-function (UDF).  within this you could merge and process data from several tables, returning the end result. ",
            "link": "https://stackoverflow.com/questions/50342182/dplyr-gather-data-from-many-sqlite-data-bases",
            "title": "dplyr: Gather data from *many* SQLite data bases",
            "body": "<p>Your data is stored in a large number of SQLite data base files.\nYou would like to gather the data from one table across all these database files.</p>\n\n<p>Is this possible using <code>dplyr</code>, or <code>tidyverse</code>?</p>\n\n<p><strong>Example Data:</strong></p>\n\n<pre><code># Required Libraries\nrequire('tidyverse')\nrequire('RSQLite')\nrequire('pool')\nrequire('here')\n\n# Create the dummy data\ntest &lt;- data.frame(t(replicate(2,sample(0:10,4,rep=TRUE))))\n\nfn &lt;- here::here('1testing.sqlite3')\ncon &lt;- dbPool(drv = RSQLite::SQLite(), \n              dbname = fn)\nwrite_result = dbWriteTable(con, \"TEST\", test)\npoolClose(con)\nrm(con)\n\n# Create multiple SQLite databases\nfn = here::here('1testing.sqlite3')\nfile.copy(from=fn, to=here::here('2testing.sqlite3'))\nfile.copy(from=fn, to=here::here('3testing.sqlite3'))\n</code></pre>\n\n<p><strong>NOTE:</strong> The accepted answer suggests creating a user-defined-function (UDF).  within this you could merge and process data from several tables, returning the end result. </p>\n"
        },
        {
            "tags": [
                "pyspark",
                "amazon-emr",
                "pyspark-sql",
                "aws-glue"
            ],
            "owner": {
                "reputation": 15321,
                "user_id": 44757,
                "user_type": "registered",
                "accept_rate": 84,
                "profile_image": "https://www.gravatar.com/avatar/df2295610c6940e95160ce80b7b4e283?s=128&d=identicon&r=PG",
                "display_name": "Jared",
                "link": "https://stackoverflow.com/users/44757/jared"
            },
            "is_answered": false,
            "view_count": 35,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1526428018,
            "creation_date": 1525042058,
            "last_edit_date": 1526428018,
            "question_id": 50092022,
            "body_markdown": "Assume I have a CSV file like this:\r\n\r\n    &quot;Col1Name&quot;, &quot;Col2Name&quot;\r\n    &quot;a&quot;, &quot;b&quot;\r\n    &quot;c&quot;, &quot;d&quot;\r\n\r\nAssume I issue the following CREATE EXTERNAL TABLE command in Athena:\r\n\r\n    CREATE EXTERNAL TABLE test.sometable (\r\n       col1name string,\r\n       col2name string\r\n    ) \r\n    row format serde &#39;org.apache.hadoop.hive.serde2.OpenCSVSerde&#39;\r\n    with serdeproperties (\r\n       &#39;separatorChar&#39; = &#39;,&#39;,\r\n       &#39;quoteChar&#39; = &#39;\\&quot;&#39;,\r\n       &#39;escapeChar&#39; = &#39;\\\\&#39;\r\n    ) \r\n    stored as textfile\r\n    location &#39;s3://somebucket/some/path/&#39;\r\n    tblproperties(&quot;skip.header.line.count&quot;=&quot;1&quot;)\r\n\r\nThen I issue the following SELECT:\r\n\r\n    SELECT * FROM test.sometable\r\n\r\nI expect to get the following:\r\n\r\n    +----------+----------+\r\n    |  col1name|  col2name|\r\n    +----------+----------+\r\n    |         a|         b|\r\n    |         c|         d|\r\n    +----------+----------+\r\n\r\n...and sure enough, that&#39;s exactly what I get.\r\n\r\nOn an EMR cluster using the AWS Glue metadata catalog in Spark, I issue the following in the pyspark REPL:\r\n\r\n    a = spark.sql(&quot;select * from test.sometable&quot;)\r\n    a.show()\r\n\r\nI expect to receive the same output, but, instead, I get this:\r\n\r\n    +----------+----------+\r\n    |  col1name|  col2name|\r\n    +----------+----------+\r\n    |  col1name|  col2name|\r\n    |         a|         b|\r\n    |         c|         d|\r\n    +----------+----------+\r\n\r\nObviously, Athena is honoring the &quot;skip.header.line.count&quot; tblproperty, but PySpark appears to be ignoring it.\r\n\r\nHow can I get PySpark to ignore this header line, as Athena does?",
            "link": "https://stackoverflow.com/questions/50092022/how-to-ignore-headers-in-pyspark-when-using-athena-and-aws-glue-data-catalog",
            "title": "How to ignore headers in PySpark when using Athena and AWS Glue Data Catalog",
            "body": "<p>Assume I have a CSV file like this:</p>\n\n<pre><code>\"Col1Name\", \"Col2Name\"\n\"a\", \"b\"\n\"c\", \"d\"\n</code></pre>\n\n<p>Assume I issue the following CREATE EXTERNAL TABLE command in Athena:</p>\n\n<pre><code>CREATE EXTERNAL TABLE test.sometable (\n   col1name string,\n   col2name string\n) \nrow format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nwith serdeproperties (\n   'separatorChar' = ',',\n   'quoteChar' = '\\\"',\n   'escapeChar' = '\\\\'\n) \nstored as textfile\nlocation 's3://somebucket/some/path/'\ntblproperties(\"skip.header.line.count\"=\"1\")\n</code></pre>\n\n<p>Then I issue the following SELECT:</p>\n\n<pre><code>SELECT * FROM test.sometable\n</code></pre>\n\n<p>I expect to get the following:</p>\n\n<pre><code>+----------+----------+\n|  col1name|  col2name|\n+----------+----------+\n|         a|         b|\n|         c|         d|\n+----------+----------+\n</code></pre>\n\n<p>...and sure enough, that's exactly what I get.</p>\n\n<p>On an EMR cluster using the AWS Glue metadata catalog in Spark, I issue the following in the pyspark REPL:</p>\n\n<pre><code>a = spark.sql(\"select * from test.sometable\")\na.show()\n</code></pre>\n\n<p>I expect to receive the same output, but, instead, I get this:</p>\n\n<pre><code>+----------+----------+\n|  col1name|  col2name|\n+----------+----------+\n|  col1name|  col2name|\n|         a|         b|\n|         c|         d|\n+----------+----------+\n</code></pre>\n\n<p>Obviously, Athena is honoring the \"skip.header.line.count\" tblproperty, but PySpark appears to be ignoring it.</p>\n\n<p>How can I get PySpark to ignore this header line, as Athena does?</p>\n"
        },
        {
            "tags": [
                "hive",
                "prestodb",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 3697,
                "user_id": 196032,
                "user_type": "registered",
                "accept_rate": 49,
                "profile_image": "https://www.gravatar.com/avatar/b75136316e93a66545dd346297fad09e?s=128&d=identicon&r=PG",
                "display_name": "Alex R",
                "link": "https://stackoverflow.com/users/196032/alex-r"
            },
            "is_answered": true,
            "view_count": 30,
            "accepted_answer_id": 50106429,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1526428006,
            "creation_date": 1525112251,
            "last_edit_date": 1526428006,
            "question_id": 50106046,
            "body_markdown": "I tried creating this simple table in Athena:\r\n\r\n    CREATE EXTERNAL TABLE ctc.rets (\r\n      `SystemID` string,\r\n      `blah` string\r\n    ) \r\n    ROW FORMAT SERDE &#39;org.openx.data.jsonserde.JsonSerDe&#39;\r\n    WITH SERDEPROPERTIES (\r\n      &#39;mapping.SystemID&#39; = &#39;L_ListingID&#39;,\r\n      &#39;mapping.blah&#39; = &#39;Ext_Char10_11&#39; \r\n    ) \r\n    LOCATION &#39;s3://xyz.bucket/mydata/&#39;\r\n    TBLPROPERTIES (&#39;has_encrypted_data&#39;=&#39;false&#39;);\r\n\r\nThe field named `blah` maps fine, but the field named `SystemID` comes up blank on every row.\r\n\r\nAnd then it gets really interesting:\r\n\r\n - I change the `SystemID` field name to `WTF`, or `foobar`, or `strawberry`, and **it works fine** (the data shows up).\r\n - I change the `SystemID` field name to `_SystemID`, `f_SystemID`, `ystemID`, `System_I_D`, and **none of them work**\r\n\r\nThere is never an error message.\r\n\r\nWhat are the actual rules that need to be followed for the field names?",
            "link": "https://stackoverflow.com/questions/50106046/aws-athena-presto-with-jsonserde-fails-quietly-on-some-column-names-which-one",
            "title": "AWS Athena (Presto with JsonSerde) fails quietly on some Column Names, which ones are acceptable?",
            "body": "<p>I tried creating this simple table in Athena:</p>\n\n<pre><code>CREATE EXTERNAL TABLE ctc.rets (\n  `SystemID` string,\n  `blah` string\n) \nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\nWITH SERDEPROPERTIES (\n  'mapping.SystemID' = 'L_ListingID',\n  'mapping.blah' = 'Ext_Char10_11' \n) \nLOCATION 's3://xyz.bucket/mydata/'\nTBLPROPERTIES ('has_encrypted_data'='false');\n</code></pre>\n\n<p>The field named <code>blah</code> maps fine, but the field named <code>SystemID</code> comes up blank on every row.</p>\n\n<p>And then it gets really interesting:</p>\n\n<ul>\n<li>I change the <code>SystemID</code> field name to <code>WTF</code>, or <code>foobar</code>, or <code>strawberry</code>, and <strong>it works fine</strong> (the data shows up).</li>\n<li>I change the <code>SystemID</code> field name to <code>_SystemID</code>, <code>f_SystemID</code>, <code>ystemID</code>, <code>System_I_D</code>, and <strong>none of them work</strong></li>\n</ul>\n\n<p>There is never an error message.</p>\n\n<p>What are the actual rules that need to be followed for the field names?</p>\n"
        },
        {
            "tags": [
                "prestodb",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 649,
                "user_id": 5231528,
                "user_type": "registered",
                "accept_rate": 85,
                "profile_image": "https://i.stack.imgur.com/xhIP7.jpg?s=128&g=1",
                "display_name": "Miguel Coder",
                "link": "https://stackoverflow.com/users/5231528/miguel-coder"
            },
            "is_answered": true,
            "view_count": 19,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526427993,
            "creation_date": 1525202505,
            "last_edit_date": 1526427993,
            "question_id": 50122530,
            "body_markdown": "I have a few things I want to accomplish with PrestoDB. I currently getting some data in the following formats\r\n\r\n1. `date 16-Jan-2018`\r\n2. `num 1000`\r\n\r\nI want to write a query that can convert these values to\r\n\r\n1. `2018-01-16`\r\n2. `1,000`",
            "link": "https://stackoverflow.com/questions/50122530/converting-values-in-athena-prestodb",
            "title": "Converting values in Athena PrestoDB",
            "body": "<p>I have a few things I want to accomplish with PrestoDB. I currently getting some data in the following formats</p>\n\n<ol>\n<li><code>date 16-Jan-2018</code></li>\n<li><code>num 1000</code></li>\n</ol>\n\n<p>I want to write a query that can convert these values to</p>\n\n<ol>\n<li><code>2018-01-16</code></li>\n<li><code>1,000</code></li>\n</ol>\n"
        },
        {
            "tags": [
                "woocommerce-rest-api"
            ],
            "owner": {
                "reputation": 836,
                "user_id": 1532104,
                "user_type": "registered",
                "accept_rate": 29,
                "profile_image": "https://www.gravatar.com/avatar/9163f18494c738d1a484e9af63b40ed3?s=128&d=identicon&r=PG",
                "display_name": "user125264",
                "link": "https://stackoverflow.com/users/1532104/user125264"
            },
            "is_answered": false,
            "view_count": 8,
            "answer_count": 0,
            "score": 1,
            "last_activity_date": 1526427977,
            "creation_date": 1526427977,
            "question_id": 50360607,
            "body_markdown": "I&#39;m trying to figure out how to add a role to a customer via the WooCommerce REST API.\r\n\r\nWhat I thought would be correct of simply adding &quot;role&quot;:&quot;test_role&quot; doesnt appear to work as the docs are indicating its read only.\r\n\r\nIm quite new to WooCommerce, and would have thought simply adding a role to user wouldnt be a difficult task.\r\n\r\nIs there a way to extend the API to make this writable? or is it even possible to add a role to a customer?",
            "link": "https://stackoverflow.com/questions/50360607/woocommerce-api-add-role-to-customer",
            "title": "WooCommerce API Add Role to Customer",
            "body": "<p>I'm trying to figure out how to add a role to a customer via the WooCommerce REST API.</p>\n\n<p>What I thought would be correct of simply adding \"role\":\"test_role\" doesnt appear to work as the docs are indicating its read only.</p>\n\n<p>Im quite new to WooCommerce, and would have thought simply adding a role to user wouldnt be a difficult task.</p>\n\n<p>Is there a way to extend the API to make this writable? or is it even possible to add a role to a customer?</p>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 160,
                "user_id": 467498,
                "user_type": "registered",
                "accept_rate": 50,
                "profile_image": "https://www.gravatar.com/avatar/9ebdae3cfed543713f4e89f51ac0736e?s=128&d=identicon&r=PG",
                "display_name": "Austin",
                "link": "https://stackoverflow.com/users/467498/austin"
            },
            "is_answered": false,
            "view_count": 17,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526427970,
            "creation_date": 1525375818,
            "last_edit_date": 1526427970,
            "question_id": 50162586,
            "body_markdown": "I&#39;m in a bit of a dilemna here:\r\n\r\nI&#39;m using AWS Athena to query against some JSON objects. Most of the JSON records are structured, but one field in particular (&quot;changes&quot;) has dynamic objects whose fields don&#39;t really have a set structure. For example, here&#39;s a record: \r\n\r\n    {\r\n        id: 1,\r\n        user_id: 2,\r\n        changes: {\r\n        &quot;customer_id&quot; 1,\r\n        &quot;business_name: [&#39;old name&#39;, &#39;new name&#39;]\r\n        }\r\n    }\r\n\r\nEvery record has different keys and the value types vary. How can I represent this data? I thought maybe a string, but when I try to store it that way I get JSON parsing errors when decoding. Any help would be appreciated! Thanks!\r\n",
            "link": "https://stackoverflow.com/questions/50162586/athena-creating-a-dynamic-json-column",
            "title": "Athena - Creating a Dynamic JSON Column",
            "body": "<p>I'm in a bit of a dilemna here:</p>\n\n<p>I'm using AWS Athena to query against some JSON objects. Most of the JSON records are structured, but one field in particular (\"changes\") has dynamic objects whose fields don't really have a set structure. For example, here's a record: </p>\n\n<pre><code>{\n    id: 1,\n    user_id: 2,\n    changes: {\n    \"customer_id\" 1,\n    \"business_name: ['old name', 'new name']\n    }\n}\n</code></pre>\n\n<p>Every record has different keys and the value types vary. How can I represent this data? I thought maybe a string, but when I try to store it that way I get JSON parsing errors when decoding. Any help would be appreciated! Thanks!</p>\n"
        }
    ],
    "has_more": true,
    "quota_max": 300,
    "quota_remaining": 299
}