{
    "items": [
        {
            "tags": [
                "spring",
                "transactions"
            ],
            "owner": {
                "reputation": 6,
                "user_id": 9101889,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/0eeb4ebbc6758476e95e429e57168c01?s=128&d=identicon&r=PG&f=1",
                "display_name": "Gaurav Sinha",
                "link": "https://stackoverflow.com/users/9101889/gaurav-sinha"
            },
            "is_answered": false,
            "view_count": 9,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526428138,
            "creation_date": 1526428138,
            "question_id": 50360626,
            "body_markdown": "I have a spring  trsaction metod bar() which calls one more  transactional method foo which inserts to database.Now I want to roll back   entry in the table which foo has commited if there are any exceptions(checked/unchecked)   in writetoAfile() method.I tried using below way but it did not work .Can you please let me know how to handle this.\r\n\r\n\r\n\r\n    @Transaction(Proporation.REQUIRES_NEW,rollbackFor = Exception.class)\r\n        bar(){\r\n         foo()\r\n         writetoAfile()\r\n        \r\n        }\r\n        \r\n        @Transaction(Proporation.REQUIRED)\r\n         foo{\r\n          // insert into foo table\r\n        \r\n        }\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50360626/spring-nested-transaction-roll-back",
            "title": "Spring nested transaction roll back",
            "body": "<p>I have a spring  trsaction metod bar() which calls one more  transactional method foo which inserts to database.Now I want to roll back   entry in the table which foo has commited if there are any exceptions(checked/unchecked)   in writetoAfile() method.I tried using below way but it did not work .Can you please let me know how to handle this.</p>\n\n<pre><code>@Transaction(Proporation.REQUIRES_NEW,rollbackFor = Exception.class)\n    bar(){\n     foo()\n     writetoAfile()\n\n    }\n\n    @Transaction(Proporation.REQUIRED)\n     foo{\n      // insert into foo table\n\n    }\n</code></pre>\n"
        },
        {
            "tags": [
                "excel",
                "excel-vba"
            ],
            "owner": {
                "reputation": 14,
                "user_id": 4477300,
                "user_type": "registered",
                "accept_rate": 50,
                "profile_image": "https://graph.facebook.com/1540523858/picture?type=large",
                "display_name": "Patrick Handcock",
                "link": "https://stackoverflow.com/users/4477300/patrick-handcock"
            },
            "is_answered": true,
            "view_count": 5560,
            "accepted_answer_id": 28065567,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428134,
            "creation_date": 1421837256,
            "question_id": 28065252,
            "body_markdown": "I&#39;m not an excel whiz but really stuck on this one and hoping a guru out there will be able to help me out as it&#39;s really important for some research I am doing - please!! I think it will be quite a simple fix (see attached example excel sheet for context below). If you are even able to modify the formula in question and re-attach that would be awesome too!!\r\n\r\nThere are 2 datasets in sheet 1 side-by-side. The data on left (rows A to K) displays data in 10 sec time epochs, the data on right (rows N to X) in 1 min time epochs. I would like to be able to drag the formula in P2 all the way down the column P based on data in column C (as per colour coding in red and blue).\r\n\r\nYou&#39;ll note that the P2 formula is taking a SUM of C2-C7 and P3 is taking sum of C8-C13. I would like to be able to continue this pattern down the column, perhaps with a better drag-down formula (or more efficiently – as there is loads of data!) if possible. Essentially I want each single data row on the right to move onto the next block of 6 rows from data on the left. \r\n\r\nI hope I explained that well enough. Really hoping someone can help! Really important to me!\r\n\r\nPatrick\r\n\r\nSee attached excel example - thanks so much!! I will be ever grateful!\r\n\r\nhttps://www.dropbox.com/s/72r7ty9v15vzyyv/drag%20formula%20quick%20way%20-%20help.xlsx?dl=0 \r\n",
            "link": "https://stackoverflow.com/questions/28065252/repeat-a-specific-excel-formula-pattern-down-column",
            "title": "repeat a specific excel formula pattern down column",
            "body": "<p>I'm not an excel whiz but really stuck on this one and hoping a guru out there will be able to help me out as it's really important for some research I am doing - please!! I think it will be quite a simple fix (see attached example excel sheet for context below). If you are even able to modify the formula in question and re-attach that would be awesome too!!</p>\n\n<p>There are 2 datasets in sheet 1 side-by-side. The data on left (rows A to K) displays data in 10 sec time epochs, the data on right (rows N to X) in 1 min time epochs. I would like to be able to drag the formula in P2 all the way down the column P based on data in column C (as per colour coding in red and blue).</p>\n\n<p>You'll note that the P2 formula is taking a SUM of C2-C7 and P3 is taking sum of C8-C13. I would like to be able to continue this pattern down the column, perhaps with a better drag-down formula (or more efficiently – as there is loads of data!) if possible. Essentially I want each single data row on the right to move onto the next block of 6 rows from data on the left. </p>\n\n<p>I hope I explained that well enough. Really hoping someone can help! Really important to me!</p>\n\n<p>Patrick</p>\n\n<p>See attached excel example - thanks so much!! I will be ever grateful!</p>\n\n<p><a href=\"https://www.dropbox.com/s/72r7ty9v15vzyyv/drag%20formula%20quick%20way%20-%20help.xlsx?dl=0\" rel=\"nofollow\">https://www.dropbox.com/s/72r7ty9v15vzyyv/drag%20formula%20quick%20way%20-%20help.xlsx?dl=0</a> </p>\n"
        },
        {
            "tags": [
                "javascript",
                "node.js",
                "actions-on-google"
            ],
            "owner": {
                "reputation": 96,
                "user_id": 8599469,
                "user_type": "registered",
                "accept_rate": 100,
                "profile_image": "https://www.gravatar.com/avatar/483a7ca4de59ee50a2e920fb25fcda69?s=128&d=identicon&r=PG&f=1",
                "display_name": "chan3600",
                "link": "https://stackoverflow.com/users/8599469/chan3600"
            },
            "is_answered": true,
            "view_count": 52,
            "accepted_answer_id": 50360624,
            "answer_count": 2,
            "score": 0,
            "last_activity_date": 1526428129,
            "creation_date": 1526347512,
            "last_edit_date": 1526407531,
            "question_id": 50341054,
            "body_markdown": "I am trying to pass parameter value from google actions to my fulfillment.\r\nHowever, I am not able to get only the parameter value. How should I able to get only &quot;Asda&quot; under `newName` field from the argument?  Do I have to extract it from `conv` (like `conv.inputs.arguments[1].rawText`)? if like this, then what is the purpose of having a name for the parameter?\r\n\r\nRequest JSON from Google Actions:\r\n\r\n    {\r\n      &quot;user&quot;: {\r\n        &quot;userId&quot;: &quot;ABwppHEAPgcgb2yFUFURYFEJGg4VdAVcL9UKO9cS7a7rVfAMr9ht67LzgrmMseTvb5mmJjbjj7UV&quot;,\r\n        &quot;locale&quot;: &quot;en-US&quot;,\r\n        &quot;lastSeen&quot;: &quot;2018-05-15T01:08:55Z&quot;,\r\n        &quot;userStorage&quot;: &quot;{\\&quot;data\\&quot;:{}}&quot;\r\n      },\r\n      &quot;conversation&quot;: {\r\n        &quot;conversationId&quot;: &quot;1526346570079&quot;,\r\n        &quot;type&quot;: &quot;NEW&quot;\r\n      },\r\n      &quot;inputs&quot;: [\r\n        {\r\n          &quot;intent&quot;: &quot;com.example.test.RENAME&quot;,\r\n          &quot;rawInputs&quot;: [\r\n            {\r\n              &quot;inputType&quot;: &quot;KEYBOARD&quot;,\r\n              &quot;query&quot;: &quot;Talk to GoTestApp to rename Asda&quot;\r\n            }\r\n          ],\r\n          &quot;arguments&quot;: [\r\n            {\r\n              &quot;name&quot;: &quot;trigger_query&quot;,\r\n              &quot;rawText&quot;: &quot;rename Asda&quot;,\r\n              &quot;textValue&quot;: &quot;rename Asda&quot;\r\n            },\r\n            {\r\n              &quot;name&quot;: &quot;newName&quot;,\r\n              &quot;rawText&quot;: &quot;Asda&quot;,\r\n              &quot;textValue&quot;: &quot;Asda&quot;\r\n            }\r\n          ]\r\n        }\r\n      ],\r\n      &quot;surface&quot;: {\r\n        &quot;capabilities&quot;: [\r\n          {\r\n            &quot;name&quot;: &quot;actions.capability.MEDIA_RESPONSE_AUDIO&quot;\r\n          },\r\n          {\r\n            &quot;name&quot;: &quot;actions.capability.SCREEN_OUTPUT&quot;\r\n          },\r\n          {\r\n            &quot;name&quot;: &quot;actions.capability.AUDIO_OUTPUT&quot;\r\n          },\r\n          {\r\n            &quot;name&quot;: &quot;actions.capability.WEB_BROWSER&quot;\r\n          }\r\n        ]\r\n      },\r\n      &quot;isInSandbox&quot;: true,\r\n      &quot;availableSurfaces&quot;: [\r\n        {\r\n          &quot;capabilities&quot;: [\r\n            {\r\n              &quot;name&quot;: &quot;actions.capability.SCREEN_OUTPUT&quot;\r\n            },\r\n            {\r\n              &quot;name&quot;: &quot;actions.capability.AUDIO_OUTPUT&quot;\r\n            }\r\n          ]\r\n        }\r\n      ]\r\n    }\r\n\r\nMy code of Fulfillment side:\r\n\r\n    app.intent(&#39;com.example.test.RENAME&#39;, (conv, input, arg) =&gt; {\r\n      console.log(input); //print Talk to GoTestApp to rename Asda\r\n      console.log(arg); //only print &quot;rename Asda&quot;\r\n      console.log(arg[1]) //only print &quot;e&quot;\r\n    }\r\n\r\nAction Package action:\r\n\r\n      &quot;name&quot;: &quot;RENAME&quot;,\r\n      &quot;intent&quot;: {\r\n        &quot;name&quot;: &quot;com.example.test.RENAME&quot;,\r\n        &quot;parameters&quot;: [{\r\n          &quot;name&quot;: &quot;newName&quot;,\r\n          &quot;type&quot;: &quot;SchemaOrg_Text&quot;\r\n        }],\r\n        &quot;trigger&quot;: {\r\n          &quot;queryPatterns&quot;: [\r\n            &quot;rename $SchemaOrg_Text:newName&quot;\r\n          ]\r\n        }\r\n      },\r\n      &quot;fulfillment&quot;: {\r\n        &quot;conversationName&quot;: &quot;example&quot;\r\n      }\r\n    }",
            "link": "https://stackoverflow.com/questions/50341054/how-do-i-get-parameter-value-from-arguments",
            "title": "How do I get parameter value from arguments?",
            "body": "<p>I am trying to pass parameter value from google actions to my fulfillment.\nHowever, I am not able to get only the parameter value. How should I able to get only \"Asda\" under <code>newName</code> field from the argument?  Do I have to extract it from <code>conv</code> (like <code>conv.inputs.arguments[1].rawText</code>)? if like this, then what is the purpose of having a name for the parameter?</p>\n\n<p>Request JSON from Google Actions:</p>\n\n<pre><code>{\n  \"user\": {\n    \"userId\": \"ABwppHEAPgcgb2yFUFURYFEJGg4VdAVcL9UKO9cS7a7rVfAMr9ht67LzgrmMseTvb5mmJjbjj7UV\",\n    \"locale\": \"en-US\",\n    \"lastSeen\": \"2018-05-15T01:08:55Z\",\n    \"userStorage\": \"{\\\"data\\\":{}}\"\n  },\n  \"conversation\": {\n    \"conversationId\": \"1526346570079\",\n    \"type\": \"NEW\"\n  },\n  \"inputs\": [\n    {\n      \"intent\": \"com.example.test.RENAME\",\n      \"rawInputs\": [\n        {\n          \"inputType\": \"KEYBOARD\",\n          \"query\": \"Talk to GoTestApp to rename Asda\"\n        }\n      ],\n      \"arguments\": [\n        {\n          \"name\": \"trigger_query\",\n          \"rawText\": \"rename Asda\",\n          \"textValue\": \"rename Asda\"\n        },\n        {\n          \"name\": \"newName\",\n          \"rawText\": \"Asda\",\n          \"textValue\": \"Asda\"\n        }\n      ]\n    }\n  ],\n  \"surface\": {\n    \"capabilities\": [\n      {\n        \"name\": \"actions.capability.MEDIA_RESPONSE_AUDIO\"\n      },\n      {\n        \"name\": \"actions.capability.SCREEN_OUTPUT\"\n      },\n      {\n        \"name\": \"actions.capability.AUDIO_OUTPUT\"\n      },\n      {\n        \"name\": \"actions.capability.WEB_BROWSER\"\n      }\n    ]\n  },\n  \"isInSandbox\": true,\n  \"availableSurfaces\": [\n    {\n      \"capabilities\": [\n        {\n          \"name\": \"actions.capability.SCREEN_OUTPUT\"\n        },\n        {\n          \"name\": \"actions.capability.AUDIO_OUTPUT\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre>\n\n<p>My code of Fulfillment side:</p>\n\n<pre><code>app.intent('com.example.test.RENAME', (conv, input, arg) =&gt; {\n  console.log(input); //print Talk to GoTestApp to rename Asda\n  console.log(arg); //only print \"rename Asda\"\n  console.log(arg[1]) //only print \"e\"\n}\n</code></pre>\n\n<p>Action Package action:</p>\n\n<pre><code>  \"name\": \"RENAME\",\n  \"intent\": {\n    \"name\": \"com.example.test.RENAME\",\n    \"parameters\": [{\n      \"name\": \"newName\",\n      \"type\": \"SchemaOrg_Text\"\n    }],\n    \"trigger\": {\n      \"queryPatterns\": [\n        \"rename $SchemaOrg_Text:newName\"\n      ]\n    }\n  },\n  \"fulfillment\": {\n    \"conversationName\": \"example\"\n  }\n}\n</code></pre>\n"
        },
        {
            "tags": [
                "azure",
                "azure-active-directory"
            ],
            "owner": {
                "reputation": 1776,
                "user_id": 491436,
                "user_type": "registered",
                "accept_rate": 82,
                "profile_image": "https://www.gravatar.com/avatar/7f135fc2f9806c62f37cb777f3adbbaa?s=128&d=identicon&r=PG",
                "display_name": "cobolstinks",
                "link": "https://stackoverflow.com/users/491436/cobolstinks"
            },
            "is_answered": false,
            "view_count": 45,
            "answer_count": 0,
            "score": 1,
            "last_activity_date": 1526428121,
            "creation_date": 1526428121,
            "question_id": 50360622,
            "body_markdown": "Does Azure AD support code grants from SPA apps?  I&#39;ve built SPA apps that use the implicit flow against AAD and they work mostly fine.  The one caveat is that if we try to get group membership in the jwt tokens, you can only get up to 5 groups listed in the &quot;group&quot; claim otherwise the jwt contains a &quot;hasGroups&quot; claim (which is next to worthless).\r\n\r\nI&#39;ve come across some online article (which I can&#39;t seem to locate at the moment) that states the group claim is capped at 5 to prevent exceeding the url max values because of extra large access or id tokens being placed in the url, and that the code grant will allow for unlimited groups in the jwt because that request goes over a http post.  \r\n\r\ntwo questions:\r\n\r\n 1. It appears that I can&#39;t do a code grant from a SPA because the AAD\r\n    token endpoint doesn&#39;t support CORS.  Is that true?  Why is that? \r\n    That seems like Microsoft admitting JS/SPA apps aren&#39;t first class\r\n    citizens.  I&#39;ve also tried to get the public keys from AAD from a\r\n    client side app and found the lack of CORS on that endpoint as well.\r\n 2. Is it true that a code grant can get more than 5 groups back embedded in the jwt?\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50360622/azure-ad-code-grant-from-spa",
            "title": "Azure AD Code Grant from SPA?",
            "body": "<p>Does Azure AD support code grants from SPA apps?  I've built SPA apps that use the implicit flow against AAD and they work mostly fine.  The one caveat is that if we try to get group membership in the jwt tokens, you can only get up to 5 groups listed in the \"group\" claim otherwise the jwt contains a \"hasGroups\" claim (which is next to worthless).</p>\n\n<p>I've come across some online article (which I can't seem to locate at the moment) that states the group claim is capped at 5 to prevent exceeding the url max values because of extra large access or id tokens being placed in the url, and that the code grant will allow for unlimited groups in the jwt because that request goes over a http post.  </p>\n\n<p>two questions:</p>\n\n<ol>\n<li>It appears that I can't do a code grant from a SPA because the AAD\ntoken endpoint doesn't support CORS.  Is that true?  Why is that? \nThat seems like Microsoft admitting JS/SPA apps aren't first class\ncitizens.  I've also tried to get the public keys from AAD from a\nclient side app and found the lack of CORS on that endpoint as well.</li>\n<li>Is it true that a code grant can get more than 5 groups back embedded in the jwt?</li>\n</ol>\n"
        },
        {
            "tags": [
                "javascript",
                "vue.js"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9128616,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/136d57c76559959e3985c4fde76fe98e?s=128&d=identicon&r=PG&f=1",
                "display_name": "Dante",
                "link": "https://stackoverflow.com/users/9128616/dante"
            },
            "is_answered": true,
            "view_count": 37,
            "answer_count": 3,
            "score": 0,
            "last_activity_date": 1526428114,
            "creation_date": 1526425477,
            "last_edit_date": 1526426255,
            "question_id": 50360304,
            "body_markdown": "I&#39;m working with VueJS, I have an object array like this:\r\n\r\n    `const myarray = [\r\n        {&#39;secction&#39;: &#39;6.2.3&#39;,&#39;title&#39;: &#39;a&#39;},\r\n        {&#39;secction&#39;: &#39;6.2.2&#39;,&#39;title&#39;: &#39;b&#39;},\r\n        {&#39;secction&#39;: &#39;11.3.1&#39;,&#39;title&#39;: &#39;bn&#39;},\r\n        {&#39;secction&#39;: &#39;10.5.1&#39;,&#39;title&#39;: &#39;z&#39;},\r\n        {&#39;secction&#39;: &#39;10.4.1&#39;, title: &#39;da&#39;}\r\n    ]`\r\n\r\nI want to order like: \r\n\r\n    6.2.3\r\n    6.2.2\r\n    10.4.1\r\n    10.5.1\r\n    11.3.1\r\n\r\nbut I aplied this fuction: \r\n\r\n    myarray.sort( (a ,b) =&gt; {\r\n        if (a.Control_num &lt; b.Control_num) return -1\r\n        if (a.Control_num &gt; b.Control_num) return 1\r\n            return 0\r\n    })\r\n\r\nand the result is the following:\r\n\r\n\r\n    10.4.1\r\n    10.5.1\r\n    10.6.2 \r\n    11.2.2\r\n    11.3.1\r\n    6.2.2\r\n    6.2.3\r\n",
            "link": "https://stackoverflow.com/questions/50360304/how-i-can-order-like-a-index-book-in-javascript",
            "title": "How i can order like a index book in Javascript?",
            "body": "<p>I'm working with VueJS, I have an object array like this:</p>\n\n<pre><code>`const myarray = [\n    {'secction': '6.2.3','title': 'a'},\n    {'secction': '6.2.2','title': 'b'},\n    {'secction': '11.3.1','title': 'bn'},\n    {'secction': '10.5.1','title': 'z'},\n    {'secction': '10.4.1', title: 'da'}\n]`\n</code></pre>\n\n<p>I want to order like: </p>\n\n<pre><code>6.2.3\n6.2.2\n10.4.1\n10.5.1\n11.3.1\n</code></pre>\n\n<p>but I aplied this fuction: </p>\n\n<pre><code>myarray.sort( (a ,b) =&gt; {\n    if (a.Control_num &lt; b.Control_num) return -1\n    if (a.Control_num &gt; b.Control_num) return 1\n        return 0\n})\n</code></pre>\n\n<p>and the result is the following:</p>\n\n<pre><code>10.4.1\n10.5.1\n10.6.2 \n11.2.2\n11.3.1\n6.2.2\n6.2.3\n</code></pre>\n"
        },
        {
            "tags": [
                "apache-spark",
                "amazon-s3",
                "amazon-redshift",
                "amazon-athena",
                "aws-glue"
            ],
            "owner": {
                "reputation": 220,
                "user_id": 5235665,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/86093c41776bab1aa6cb1a3b33734672?s=128&d=identicon&r=PG&f=1",
                "display_name": "hotmeatballsoup",
                "link": "https://stackoverflow.com/users/5235665/hotmeatballsoup"
            },
            "is_answered": true,
            "view_count": 41,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428110,
            "creation_date": 1526348817,
            "last_edit_date": 1526428110,
            "question_id": 50341192,
            "body_markdown": "I should preface this with the fact that I&#39;m using Enhanced VPC Routing for my AWS account, which [precludes me](https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html) from using the traditional S3 to Redshift querying:\r\n\r\n&gt; &quot;*Your cluster can&#39;t have Enhanced VPC Routing enabled. (to use Spectrum)*&quot;\r\n\r\n---\r\n\r\nMy *understanding* is that AWS Redshift is a high-octane Postgres-as-a-service that is optimized for extremely fast reads over large data volumes. So if you *lots* of have relational data that you want to query/analyze, then Redshift is a good choice for you.\r\n\r\nMy *understanding* of AWS Athena is that its just using something like Apache Drill (or similar) to provide a SQL-like interface over *any* data stored in S3 buckets (relational and otherwise, as well as any format: unstructured plaintext, JSON, XML, etc.). So if you just have data in S3 that you want to query via SQL-like syntax, Athena is a good choice for you.\r\n\r\n**To begin with, can anyone begin by confirming/clarifying my understanding above?** Assuming I&#39;m more or less correct...\r\n\r\nI have structured/relational (stored in JSON and CSV files) that lives on S3. I&#39;d like to create an ETL process that reads this data out of S3 and dumps it into Redshift so that downstream processes can analyze it.\r\n\r\nSo I&#39;m thinking about creating a Spark-based ETL pipeline whereby:\r\n\r\n1. Spark uses Athena to query S3 data into `DataFrames`; I&#39;m also wondering if AWS Glue can possibly do some heavy lifting here\r\n2. Spark writes the contents of those `DataFrames` to Redshift\r\n\r\nSo my question: is this the most efficient way to port LARGE amounts of partially-structured/relational S3 data (again stored in various file formats) into Redshift, or is there a better/simpler way?",
            "link": "https://stackoverflow.com/questions/50341192/porting-partially-relational-s3-data-into-redshift-via-spark-and-glue",
            "title": "Porting partially-relational S3 data into Redshift via Spark and Glue",
            "body": "<p>I should preface this with the fact that I'm using Enhanced VPC Routing for my AWS account, which <a href=\"https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html\" rel=\"nofollow noreferrer\">precludes me</a> from using the traditional S3 to Redshift querying:</p>\n\n<blockquote>\n  <p>\"<em>Your cluster can't have Enhanced VPC Routing enabled. (to use Spectrum)</em>\"</p>\n</blockquote>\n\n<hr>\n\n<p>My <em>understanding</em> is that AWS Redshift is a high-octane Postgres-as-a-service that is optimized for extremely fast reads over large data volumes. So if you <em>lots</em> of have relational data that you want to query/analyze, then Redshift is a good choice for you.</p>\n\n<p>My <em>understanding</em> of AWS Athena is that its just using something like Apache Drill (or similar) to provide a SQL-like interface over <em>any</em> data stored in S3 buckets (relational and otherwise, as well as any format: unstructured plaintext, JSON, XML, etc.). So if you just have data in S3 that you want to query via SQL-like syntax, Athena is a good choice for you.</p>\n\n<p><strong>To begin with, can anyone begin by confirming/clarifying my understanding above?</strong> Assuming I'm more or less correct...</p>\n\n<p>I have structured/relational (stored in JSON and CSV files) that lives on S3. I'd like to create an ETL process that reads this data out of S3 and dumps it into Redshift so that downstream processes can analyze it.</p>\n\n<p>So I'm thinking about creating a Spark-based ETL pipeline whereby:</p>\n\n<ol>\n<li>Spark uses Athena to query S3 data into <code>DataFrames</code>; I'm also wondering if AWS Glue can possibly do some heavy lifting here</li>\n<li>Spark writes the contents of those <code>DataFrames</code> to Redshift</li>\n</ol>\n\n<p>So my question: is this the most efficient way to port LARGE amounts of partially-structured/relational S3 data (again stored in various file formats) into Redshift, or is there a better/simpler way?</p>\n"
        },
        {
            "tags": [
                "wordpress",
                "optimization",
                "coding-style",
                "affiliate"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9796800,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
                "display_name": "Raghav Sehgal",
                "link": "https://stackoverflow.com/users/9796800/raghav-sehgal"
            },
            "is_answered": false,
            "view_count": 26,
            "closed_date": 1526428116,
            "answer_count": 0,
            "score": -6,
            "last_activity_date": 1526428104,
            "creation_date": 1526420018,
            "last_edit_date": 1526428104,
            "question_id": 50359464,
            "body_markdown": "My wordpress website has been completely developed through the wordpress content management system. The wordpress software creates more files in my root directory. Therefore creating more requests and hence increasing the page load time. Any work arounds?",
            "link": "https://stackoverflow.com/questions/50359464/how-can-i-make-convert-my-wordpress-website-into-html",
            "closed_reason": "off-topic",
            "title": "How can I make convert my wordpress website into html?",
            "body": "<p>My wordpress website has been completely developed through the wordpress content management system. The wordpress software creates more files in my root directory. Therefore creating more requests and hence increasing the page load time. Any work arounds?</p>\n"
        },
        {
            "tags": [
                "amazon-s3",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 1095478,
                "user_type": "registered",
                "profile_image": "https://i.stack.imgur.com/sAeDI.jpg?s=128&g=1",
                "display_name": "al.moorthi",
                "link": "https://stackoverflow.com/users/1095478/al-moorthi"
            },
            "is_answered": true,
            "view_count": 17,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428100,
            "creation_date": 1526304848,
            "last_edit_date": 1526428100,
            "question_id": 50331678,
            "body_markdown": "I have a CSV (tab separated) in s3 that needs to be queried on a JSON field.\r\n\r\n    uid\\tname\\taddress\r\n    1\\tmoorthi\\t{&quot;rno&quot;:123,&quot;code&quot;:400111}\r\n    2\\tkiranp\\t{&quot;rno&quot;:124,&quot;street&quot;:&quot;kemp road&quot;}\r\n\r\nHow can I query this data in Amazon Athena?\r\n\r\nI should be able to query like:\r\n\r\n    select uid\r\n    from table1\r\n    where address[&#39;street&#39;]=&quot;kemp road&quot;;\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50331678/how-do-i-load-csv-file-to-amazon-athena-that-contains-json-field",
            "title": "How do I load CSV file to Amazon Athena that contains JSON field",
            "body": "<p>I have a CSV (tab separated) in s3 that needs to be queried on a JSON field.</p>\n\n<pre><code>uid\\tname\\taddress\n1\\tmoorthi\\t{\"rno\":123,\"code\":400111}\n2\\tkiranp\\t{\"rno\":124,\"street\":\"kemp road\"}\n</code></pre>\n\n<p>How can I query this data in Amazon Athena?</p>\n\n<p>I should be able to query like:</p>\n\n<pre><code>select uid\nfrom table1\nwhere address['street']=\"kemp road\";\n</code></pre>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "partitioning",
                "amazon-athena",
                "aws-glue"
            ],
            "owner": {
                "reputation": 1738,
                "user_id": 3002273,
                "user_type": "registered",
                "accept_rate": 86,
                "profile_image": "https://www.gravatar.com/avatar/14663c07f99bd2fdc191b05db9d3e900?s=128&d=identicon&r=PG&f=1",
                "display_name": "aidan.plenert.macdonald",
                "link": "https://stackoverflow.com/users/3002273/aidan-plenert-macdonald"
            },
            "is_answered": false,
            "view_count": 25,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526428094,
            "creation_date": 1525976464,
            "last_edit_date": 1526428094,
            "question_id": 50279209,
            "body_markdown": "Using [AWS::Glue::Table](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-glue-table.html), you can set up an Athena table like [here](https://stackoverflow.com/a/47760146/3002273). Athena [supports partitioning data](https://docs.aws.amazon.com/athena/latest/ug/partitions.html) based on folder structure in S3. I would like to partition my Athena table from my Glue template.\r\n\r\nFrom [AWS Glue Table TableInput](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-glue-table-tableinput.html), it appears that I can use `PartitionKeys` to partition my data, but when I try to use the below template, Athena fails and can&#39;t get any data.\r\n\r\n    Resources:\r\n      ...\r\n    \r\n      MyGlueTable:\r\n        Type: AWS::Glue::Table\r\n        Properties:\r\n          DatabaseName: !Ref MyGlueDatabase\r\n          CatalogId: !Ref AWS::AccountId\r\n          TableInput:\r\n            Name: my-glue-table\r\n            Parameters: { &quot;classification&quot; : &quot;json&quot; }\r\n            PartitionKeys:\r\n              - {Name: dt, Type: string}\r\n            StorageDescriptor:\r\n              Location: &quot;s3://elasticmapreduce/samples/hive-ads/tables/impressions/&quot;\r\n              InputFormat: &quot;org.apache.hadoop.mapred.TextInputFormat&quot;\r\n              OutputFormat: &quot;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&quot;\r\n              SerdeInfo:\r\n                Parameters: { &quot;separatorChar&quot; : &quot;,&quot; }\r\n                SerializationLibrary: &quot;org.apache.hive.hcatalog.data.JsonSerDe&quot;\r\n              StoredAsSubDirectories: false\r\n              Columns:\r\n                - {Name: requestBeginTime, Type: string}\r\n                - {Name: adId, Type: string}\r\n                - {Name: impressionId, Type: string}\r\n                - {Name: referrer, Type: string}\r\n                - {Name: userAgent, Type: string}\r\n                - {Name: userCookie, Type: string}\r\n                - {Name: ip, Type: string}\r\n                - {Name: number, Type: string}\r\n                - {Name: processId, Type: string}\r\n                - {Name: browserCookie, Type: string}\r\n                - {Name: requestEndTime, Type: string}\r\n                - {Name: timers, Type: &quot;struct&lt;modellookup:string,requesttime:string&gt;&quot;}\r\n                - {Name: threadId, Type: string}\r\n                - {Name: hostname, Type: string}\r\n                - {Name: sessionId, Type: string}\r\n\r\nHow do I partition my data in AWS Glue?",
            "link": "https://stackoverflow.com/questions/50279209/partitioning-athena-tables-from-glue-cloudformation-template",
            "title": "Partitioning Athena Tables from Glue Cloudformation template",
            "body": "<p>Using <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-glue-table.html\" rel=\"nofollow noreferrer\">AWS::Glue::Table</a>, you can set up an Athena table like <a href=\"https://stackoverflow.com/a/47760146/3002273\">here</a>. Athena <a href=\"https://docs.aws.amazon.com/athena/latest/ug/partitions.html\" rel=\"nofollow noreferrer\">supports partitioning data</a> based on folder structure in S3. I would like to partition my Athena table from my Glue template.</p>\n\n<p>From <a href=\"https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-glue-table-tableinput.html\" rel=\"nofollow noreferrer\">AWS Glue Table TableInput</a>, it appears that I can use <code>PartitionKeys</code> to partition my data, but when I try to use the below template, Athena fails and can't get any data.</p>\n\n<pre><code>Resources:\n  ...\n\n  MyGlueTable:\n    Type: AWS::Glue::Table\n    Properties:\n      DatabaseName: !Ref MyGlueDatabase\n      CatalogId: !Ref AWS::AccountId\n      TableInput:\n        Name: my-glue-table\n        Parameters: { \"classification\" : \"json\" }\n        PartitionKeys:\n          - {Name: dt, Type: string}\n        StorageDescriptor:\n          Location: \"s3://elasticmapreduce/samples/hive-ads/tables/impressions/\"\n          InputFormat: \"org.apache.hadoop.mapred.TextInputFormat\"\n          OutputFormat: \"org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\"\n          SerdeInfo:\n            Parameters: { \"separatorChar\" : \",\" }\n            SerializationLibrary: \"org.apache.hive.hcatalog.data.JsonSerDe\"\n          StoredAsSubDirectories: false\n          Columns:\n            - {Name: requestBeginTime, Type: string}\n            - {Name: adId, Type: string}\n            - {Name: impressionId, Type: string}\n            - {Name: referrer, Type: string}\n            - {Name: userAgent, Type: string}\n            - {Name: userCookie, Type: string}\n            - {Name: ip, Type: string}\n            - {Name: number, Type: string}\n            - {Name: processId, Type: string}\n            - {Name: browserCookie, Type: string}\n            - {Name: requestEndTime, Type: string}\n            - {Name: timers, Type: \"struct&lt;modellookup:string,requesttime:string&gt;\"}\n            - {Name: threadId, Type: string}\n            - {Name: hostname, Type: string}\n            - {Name: sessionId, Type: string}\n</code></pre>\n\n<p>How do I partition my data in AWS Glue?</p>\n"
        },
        {
            "tags": [
                "python",
                "powershell",
                "pip"
            ],
            "owner": {
                "reputation": 43,
                "user_id": 3673947,
                "user_type": "registered",
                "accept_rate": 40,
                "profile_image": "https://www.gravatar.com/avatar/9eda25ed45a9c26d58b3f7714d6a41ea?s=128&d=identicon&r=PG&f=1",
                "display_name": "CathyQian",
                "link": "https://stackoverflow.com/users/3673947/cathyqian"
            },
            "is_answered": false,
            "view_count": 21,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526428087,
            "creation_date": 1526428087,
            "question_id": 50360615,
            "body_markdown": "I&#39;m trying to install rake (https://github.com/zelandiya/RAKE-tutorial) as instructed on the README file using \r\n\r\n    python setup.py install. \r\n\r\nHowever, I always get the following error:\r\n\r\n    Command &quot;python setup.py egg_info&quot; failed with error code 1 in C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\pip-install-jz29g5dr\\rake\\\r\n\r\nI tried almost everything I can find online including this comprehensive post: https://stackoverflow.com/questions/35991403/pip-install-returns-python-setup-py-egg-info-failed-with-error-code-1but still can&#39;t find the solution. Any help is needed. Thanks a lot!",
            "link": "https://stackoverflow.com/questions/50360615/error-while-installing-rake",
            "title": "Error while installing rake",
            "body": "<p>I'm trying to install rake (<a href=\"https://github.com/zelandiya/RAKE-tutorial\" rel=\"nofollow noreferrer\">https://github.com/zelandiya/RAKE-tutorial</a>) as instructed on the README file using </p>\n\n<pre><code>python setup.py install. \n</code></pre>\n\n<p>However, I always get the following error:</p>\n\n<pre><code>Command \"python setup.py egg_info\" failed with error code 1 in C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\pip-install-jz29g5dr\\rake\\\n</code></pre>\n\n<p>I tried almost everything I can find online including this comprehensive post: <a href=\"https://stackoverflow.com/questions/35991403/pip-install-returns-python-setup-py-egg-info-failed-with-error-code-1but\">pip install returns &quot;python setup.py egg_info&quot; failed with error code 1</a> still can't find the solution. Any help is needed. Thanks a lot!</p>\n"
        },
        {
            "tags": [
                "python",
                "amazon-web-services",
                "amazon-s3",
                "boto3",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 15,
                "user_id": 8604194,
                "user_type": "registered",
                "accept_rate": 33,
                "profile_image": "https://i.stack.imgur.com/x9auS.jpg?s=128&g=1",
                "display_name": "pyhotshot",
                "link": "https://stackoverflow.com/users/8604194/pyhotshot"
            },
            "is_answered": false,
            "view_count": 22,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526428085,
            "creation_date": 1525455644,
            "last_edit_date": 1526428085,
            "question_id": 50180425,
            "body_markdown": "i have a lambda that will query Athena and drop the output of the results in my desired bucket. Athena output contains .csv and .csv.metadata. i don&#39;t want to get .metadata file along with the .csv file when my lambda drops it. here is my code:\r\n\r\n    def wait_for_result(athena, query_id):state = athena.get_query_execution(QueryExecutionId=query_id)[&#39;QueryExecution&#39;][&#39;Status&#39;][&#39;State&#39;]\r\n        while state != &#39;SUCCEEDED&#39;:\r\n        print(&#39;Query state: {}&#39;.format(state))\r\n        time.sleep(5)\r\n        state = athena.get_query_execution(QueryExecutionId=query_id)[&#39;QueryExecution&#39;][&#39;Status&#39;][&#39;State&#39;]\r\n    \r\n\r\n    def lambda_handler(event, context):\r\n        short_date = event[&#39;record&#39;][&#39;short_date&#39;]\r\n\r\n        bucket = &#39;test-rod-us-east-1-orders&#39;\r\n        s3_output = &#39;s3://{0}/arda-orders/f=csv/short_date={1}&#39;.format(bucket, short_date)\r\n\r\n        query = &#39;query_here&#39;.format(short_date)\r\n\r\n        boto_session = assume_role(&#39;arn:aws:iam::account-id:role/test-contr-etl-ec2-role&#39;)\r\n        session = assume_role(&#39;arn:aws:iam::account-id:role/test-xacct-rod-consumer&#39;, boto_session)\r\n\r\n        athena = session.client(&#39;athena&#39;)\r\n        s3 = session.client(&#39;s3&#39;)\r\n        s3_bucket = session.resource(&#39;s3&#39;).Bucket(bucket)\r\n\r\n        response = athena.start_query_execution(QueryString=query,\r\n                                            QueryExecutionContext={\r\n                                                &#39;Database&#39;: &#39;datapond&#39;\r\n                                            },\r\n                                            ResultConfiguration={\r\n                                                &#39;OutputLocation&#39;: s3_output\r\n                                            })\r\n\r\n        query_id = response[&#39;QueryExecutionId&#39;]\r\n        wait_for_result(athena, query_id)\r\n\r\n        # print (&#39;short_date: {}&#39;.format(short_date))\r\n        for key in s3.list_objects(Bucket=bucket)[&#39;Contents&#39;]:\r\n           if short_date in key[&#39;Key&#39;]:\r\n              s3.put_object_acl(ACL=&#39;bucket-owner-full-control&#39;, Bucket=bucket, Key=key[&#39;Key&#39;])\r\n               print(&#39;set \\&#39;bucket-owner-full-control\\&#39; for {}&#39;.format(key[&#39;Key&#39;]))\r\n                 if &#39;.csv.metadata&#39; in key[&#39;Key&#39;]:\r\n                    s3_bucket.delete_objects(\r\n                       Delete={\r\n                         &#39;Objects&#39;: [\r\n                             {&#39;Key&#39;: key[&#39;Key&#39;]},\r\n                        ]\r\n                    }\r\n                )\r\n\r\n                 print(&#39;deleted {}&#39;.format(key[&#39;Key&#39;]))\r\n\r\n       sqs.delete_message(\r\n           QueueUrl=sqs_queue_url,\r\n           ReceiptHandle=event[&#39;receipt_handler&#39;]\r\n       )\r\n\r\n       print (&#39;Complete process for short_date: {}&#39;.format(short_date))\r\n\r\n\r\n\r\n\r\ni just get the &quot;deleted key&quot; message in the logs but i still find the .csv.metadata file in the s3 bucket. please help",
            "link": "https://stackoverflow.com/questions/50180425/ignore-csv-metadata-file-from-athena-output",
            "title": "ignore .csv.metadata file from athena output",
            "body": "<p>i have a lambda that will query Athena and drop the output of the results in my desired bucket. Athena output contains .csv and .csv.metadata. i don't want to get .metadata file along with the .csv file when my lambda drops it. here is my code:</p>\n\n<pre><code>def wait_for_result(athena, query_id):state = athena.get_query_execution(QueryExecutionId=query_id)['QueryExecution']['Status']['State']\n    while state != 'SUCCEEDED':\n    print('Query state: {}'.format(state))\n    time.sleep(5)\n    state = athena.get_query_execution(QueryExecutionId=query_id)['QueryExecution']['Status']['State']\n\n\ndef lambda_handler(event, context):\n    short_date = event['record']['short_date']\n\n    bucket = 'test-rod-us-east-1-orders'\n    s3_output = 's3://{0}/arda-orders/f=csv/short_date={1}'.format(bucket, short_date)\n\n    query = 'query_here'.format(short_date)\n\n    boto_session = assume_role('arn:aws:iam::account-id:role/test-contr-etl-ec2-role')\n    session = assume_role('arn:aws:iam::account-id:role/test-xacct-rod-consumer', boto_session)\n\n    athena = session.client('athena')\n    s3 = session.client('s3')\n    s3_bucket = session.resource('s3').Bucket(bucket)\n\n    response = athena.start_query_execution(QueryString=query,\n                                        QueryExecutionContext={\n                                            'Database': 'datapond'\n                                        },\n                                        ResultConfiguration={\n                                            'OutputLocation': s3_output\n                                        })\n\n    query_id = response['QueryExecutionId']\n    wait_for_result(athena, query_id)\n\n    # print ('short_date: {}'.format(short_date))\n    for key in s3.list_objects(Bucket=bucket)['Contents']:\n       if short_date in key['Key']:\n          s3.put_object_acl(ACL='bucket-owner-full-control', Bucket=bucket, Key=key['Key'])\n           print('set \\'bucket-owner-full-control\\' for {}'.format(key['Key']))\n             if '.csv.metadata' in key['Key']:\n                s3_bucket.delete_objects(\n                   Delete={\n                     'Objects': [\n                         {'Key': key['Key']},\n                    ]\n                }\n            )\n\n             print('deleted {}'.format(key['Key']))\n\n   sqs.delete_message(\n       QueueUrl=sqs_queue_url,\n       ReceiptHandle=event['receipt_handler']\n   )\n\n   print ('Complete process for short_date: {}'.format(short_date))\n</code></pre>\n\n<p>i just get the \"deleted key\" message in the logs but i still find the .csv.metadata file in the s3 bucket. please help</p>\n"
        },
        {
            "tags": [
                "prestodb"
            ],
            "owner": {
                "reputation": 10,
                "user_id": 1555127,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/5e8446751961b17d4b4670cc66fa0d65?s=128&d=identicon&r=PG",
                "display_name": "ni30rocks",
                "link": "https://stackoverflow.com/users/1555127/ni30rocks"
            },
            "is_answered": false,
            "view_count": 17,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526428073,
            "creation_date": 1525413381,
            "last_edit_date": 1526428073,
            "question_id": 50168070,
            "body_markdown": "I am trying to do something similar to https://stackoverflow.com/questions/26274949/mysql-top5-and-sum-remaining-as-others, but in presto or Athena. However, I am not able to find anything similar.\r\n\r\nSome other reference link http://www.silota.com/docs/recipes/sql-top-n-aggregate-rest-other.html\r\n",
            "link": "https://stackoverflow.com/questions/50168070/presto-athena-topk-and-sum-remaining-as-others",
            "title": "Presto / Athena TOPK and sum remaining as others",
            "body": "<p>I am trying to do something similar to <a href=\"https://stackoverflow.com/questions/26274949/mysql-top5-and-sum-remaining-as-others\">Mysql top5 and sum remaining as others</a>, but in presto or Athena. However, I am not able to find anything similar.</p>\n\n<p>Some other reference link <a href=\"http://www.silota.com/docs/recipes/sql-top-n-aggregate-rest-other.html\" rel=\"nofollow noreferrer\">http://www.silota.com/docs/recipes/sql-top-n-aggregate-rest-other.html</a></p>\n"
        },
        {
            "tags": [
                "amazon-s3",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 403,
                "user_id": 4961391,
                "user_type": "registered",
                "accept_rate": 95,
                "profile_image": "https://www.gravatar.com/avatar/df72c3b624d678670e8411806d92ed12?s=128&d=identicon&r=PG&f=1",
                "display_name": "2Big2BeSmall",
                "link": "https://stackoverflow.com/users/4961391/2big2besmall"
            },
            "is_answered": true,
            "view_count": 39,
            "accepted_answer_id": 50087196,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428068,
            "creation_date": 1525005116,
            "last_edit_date": 1526428068,
            "question_id": 50086587,
            "body_markdown": "I have started using Athena Query engine on top of my S3 FILEs\r\nsome of them are timestamp format columns.\r\n\r\nI have created a simple table with 2 columns\r\n\r\n    CREATE EXTERNAL TABLE `test`(\r\n      `date_x` timestamp, \r\n      `clicks` int)\r\n    ROW FORMAT DELIMITED \r\n      FIELDS TERMINATED BY &#39;,&#39; \r\n    STORED AS INPUTFORMAT \r\n      &#39;org.apache.hadoop.mapred.TextInputFormat&#39; \r\n    OUTPUTFORMAT \r\n      &#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#39;\r\n    LOCATION\r\n      &#39;s3://aws-athena-query-results-123-us-east-1/test&#39;\r\n    TBLPROPERTIES (\r\n      &#39;has_encrypted_data&#39;=&#39;false&#39;, \r\n      &#39;transient_lastDdlTime&#39;=&#39;1525003090&#39;)\r\n\r\n\r\nI have tried to load a file and query it with Athena:\r\nwhich look like that:\r\n\r\n\r\n\r\n    &quot;2018-08-09 06:00:00.000&quot;,12\r\n    &quot;2018-08-09 06:00:00.000&quot;,42\r\n    &quot;2018-08-09 06:00:00.000&quot;,22\r\n\r\n\r\n\r\nI have tried a different type format of timestamps such as DD/MM/YYYY AND YYY-MM-DD..., tried setting the time zone for each row - but none of them worked.\r\n\r\nEach value I have tried is showing in Athena as this results:\r\n\r\n        \tdate_x\tclicks\r\n            1\t\t12\r\n            2\t\t42\r\n            3\t\t22\r\n\r\nI have tried using a CSV file with and without headers\r\ntried using with and without quotation marks,\r\nBut all of them showing defected timestamp.\r\nMy column on Athena must be Timestamp - rather it without timezone.\r\nPlease don&#39;t offer to use STRING column or DATE columns, this is not what i need.\r\n\r\nHow should the CSV File look like so Athena will recognize the Timestamp column?",
            "link": "https://stackoverflow.com/questions/50086587/load-csv-with-timestamp-column-to-athena-table",
            "title": "Load csv with timestamp column to athena table",
            "body": "<p>I have started using Athena Query engine on top of my S3 FILEs\nsome of them are timestamp format columns.</p>\n\n<p>I have created a simple table with 2 columns</p>\n\n<pre><code>CREATE EXTERNAL TABLE `test`(\n  `date_x` timestamp, \n  `clicks` int)\nROW FORMAT DELIMITED \n  FIELDS TERMINATED BY ',' \nSTORED AS INPUTFORMAT \n  'org.apache.hadoop.mapred.TextInputFormat' \nOUTPUTFORMAT \n  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\nLOCATION\n  's3://aws-athena-query-results-123-us-east-1/test'\nTBLPROPERTIES (\n  'has_encrypted_data'='false', \n  'transient_lastDdlTime'='1525003090')\n</code></pre>\n\n<p>I have tried to load a file and query it with Athena:\nwhich look like that:</p>\n\n<pre><code>\"2018-08-09 06:00:00.000\",12\n\"2018-08-09 06:00:00.000\",42\n\"2018-08-09 06:00:00.000\",22\n</code></pre>\n\n<p>I have tried a different type format of timestamps such as DD/MM/YYYY AND YYY-MM-DD..., tried setting the time zone for each row - but none of them worked.</p>\n\n<p>Each value I have tried is showing in Athena as this results:</p>\n\n<pre><code>        date_x  clicks\n        1       12\n        2       42\n        3       22\n</code></pre>\n\n<p>I have tried using a CSV file with and without headers\ntried using with and without quotation marks,\nBut all of them showing defected timestamp.\nMy column on Athena must be Timestamp - rather it without timezone.\nPlease don't offer to use STRING column or DATE columns, this is not what i need.</p>\n\n<p>How should the CSV File look like so Athena will recognize the Timestamp column?</p>\n"
        },
        {
            "tags": [
                "python",
                "pip",
                "pycharm",
                "pygobject"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9735679,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/8111c995c3e4660322a865b63763ab3c?s=128&d=identicon&r=PG&f=1",
                "display_name": "Ratchet Hundreda",
                "link": "https://stackoverflow.com/users/9735679/ratchet-hundreda"
            },
            "is_answered": false,
            "view_count": 28,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526428054,
            "creation_date": 1525355533,
            "question_id": 50156741,
            "body_markdown": "I&#39;m trying to install PyGObject on a Python2.7 environment in PyCharm which fails with the following details:\r\n\r\n    Running setup.py clean for PyGObject\r\n    Failed to build PyGObject\r\n    Installing collected packages: PyGObject\r\n      Running setup.py install for PyGObject: started\r\n        Running setup.py install for PyGObject: finished with status &#39;error&#39;\r\n        Complete output from command &quot;C:\\Users\\ratch\\PycharmProjects\\Project\\venv\\Scripts\\python.exe&quot; -u -c &quot;import setuptools, tokenize;__file__=&#39;C:\\\\Users\\\\ratch\\\\AppData\\\\Local\\\\Temp\\\\pycharm-packaging\\\\PyGObject\\\\setup.py&#39;;f=getattr(tokenize, &#39;open&#39;, open)(__file__);code=f.read().replace(&#39;\\r\\n&#39;, &#39;\\n&#39;);f.close();exec(compile(code, __file__, &#39;exec&#39;))&quot; install --record c:\\users\\ratch\\appdata\\local\\temp\\pip-record-m6_taf\\install-record.txt --single-version-externally-managed --compile --install-headers &quot;C:\\Users\\ratch\\PycharmProjects\\Project\\venv\\include\\site\\python2.7\\PyGObject&quot;:\r\n        running install\r\n        running build\r\n        running build_py\r\n        creating build\r\n        creating build\\lib.win-amd64-2.7\r\n        creating build\\lib.win-amd64-2.7\\pygtkcompat\r\n        copying pygtkcompat\\generictreemodel.py -&gt; build\\lib.win-amd64-2.7\\pygtkcompat\r\n        copying pygtkcompat\\pygtkcompat.py -&gt; build\\lib.win-amd64-2.7\\pygtkcompat\r\n        copying pygtkcompat\\__init__.py -&gt; build\\lib.win-amd64-2.7\\pygtkcompat\r\n        creating build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\docstring.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\importer.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\module.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\pygtkcompat.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\types.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\_constants.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\_error.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\_option.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\_ossighelper.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\_propertyhelper.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\_signalhelper.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        copying gi\\__init__.py -&gt; build\\lib.win-amd64-2.7\\gi\r\n        creating build\\lib.win-amd64-2.7\\gi\\repository\r\n        copying gi\\repository\\__init__.py -&gt; build\\lib.win-amd64-2.7\\gi\\repository\r\n        creating build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\Gdk.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\GIMarshallingTests.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\Gio.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\GLib.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\GObject.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\Gtk.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\keysyms.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\Pango.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        copying gi\\overrides\\__init__.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\r\n        warning: build_py: byte-compiling is disabled, skipping.\r\n        \r\n        running build_ext\r\n        pycairo: new API\r\n        pycairo: trying include directory: &#39;C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\lib\\\\site-packages\\\\cairo&#39;\r\n        pycairo: header file (&#39;C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\lib\\\\site-packages\\\\cairo\\\\pycairo.h&#39;) not found\r\n        pycairo: old API\r\n        pycairo: found pycairo 1.16.3 (c:\\users\\ratch\\pycharmprojects\\Project\\venv\\lib\\site-packages)\r\n        pycairo: trying include directory: &#39;C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\include\\\\pycairo&#39;\r\n        pycairo: found &#39;C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\include\\\\pycairo\\\\pycairo.h&#39;\r\n        building &#39;gi._gi&#39; extension\r\n        creating build\\temp.win-amd64-2.7\r\n        creating build\\temp.win-amd64-2.7\\Release\r\n        creating build\\temp.win-amd64-2.7\\Release\\gi\r\n        C:\\Users\\ratch\\AppData\\Local\\Programs\\Common\\Microsoft\\Visual C++ for Python\\9.0\\VC\\Bin\\amd64\\cl.exe /c /nologo /Ox /MD /W3 /GS- /DNDEBUG -DHAVE_CONFIG_H -DPY_SSIZE_T_CLEAN -IC:\\Users\\ratch\\AppData\\Local\\Temp\\pycharm-packaging\\PyGObject -IC:\\Users\\ratch\\AppData\\Local\\Temp\\pycharm-packaging\\PyGObject\\gi -IC:\\Python27\\include &quot;-IC:\\Users\\ratch\\PycharmProjects\\Project\\venv\\PC&quot; /Tcgi\\gimodule.c /Fobuild\\temp.win-amd64-2.7\\Release\\gi\\gimodule.obj\r\n        gimodule.c\r\n        gi\\gimodule.c(25) : fatal error C1083: Cannot open include file: &#39;glib-object.h&#39;: No such file or directory\r\n        error: command &#39;C:\\\\Users\\\\ratch\\\\AppData\\\\Local\\\\Programs\\\\Common\\\\Microsoft\\\\Visual C++ for Python\\\\9.0\\\\VC\\\\Bin\\\\amd64\\\\cl.exe&#39; failed with exit status 2\r\n\r\nI&#39;ve installed the Microsoft Visual C++ Compiler for Python 2.7 and the Windows 10 SDK properly as far as i know. I would ask for help on getting the environment set up correctly preferably in PyCharm.",
            "link": "https://stackoverflow.com/questions/50156741/installing-pygobject-through-pycharm-yields-error-command-c-cl-exe-fail",
            "title": "Installing PyGObject through PyCharm yields error command &#39;C:\\\\...\\\\cl.exe&#39; failed with exit status 2",
            "body": "<p>I'm trying to install PyGObject on a Python2.7 environment in PyCharm which fails with the following details:</p>\n\n<pre><code>Running setup.py clean for PyGObject\nFailed to build PyGObject\nInstalling collected packages: PyGObject\n  Running setup.py install for PyGObject: started\n    Running setup.py install for PyGObject: finished with status 'error'\n    Complete output from command \"C:\\Users\\ratch\\PycharmProjects\\Project\\venv\\Scripts\\python.exe\" -u -c \"import setuptools, tokenize;__file__='C:\\\\Users\\\\ratch\\\\AppData\\\\Local\\\\Temp\\\\pycharm-packaging\\\\PyGObject\\\\setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record c:\\users\\ratch\\appdata\\local\\temp\\pip-record-m6_taf\\install-record.txt --single-version-externally-managed --compile --install-headers \"C:\\Users\\ratch\\PycharmProjects\\Project\\venv\\include\\site\\python2.7\\PyGObject\":\n    running install\n    running build\n    running build_py\n    creating build\n    creating build\\lib.win-amd64-2.7\n    creating build\\lib.win-amd64-2.7\\pygtkcompat\n    copying pygtkcompat\\generictreemodel.py -&gt; build\\lib.win-amd64-2.7\\pygtkcompat\n    copying pygtkcompat\\pygtkcompat.py -&gt; build\\lib.win-amd64-2.7\\pygtkcompat\n    copying pygtkcompat\\__init__.py -&gt; build\\lib.win-amd64-2.7\\pygtkcompat\n    creating build\\lib.win-amd64-2.7\\gi\n    copying gi\\docstring.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\importer.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\module.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\pygtkcompat.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\types.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\_constants.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\_error.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\_option.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\_ossighelper.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\_propertyhelper.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\_signalhelper.py -&gt; build\\lib.win-amd64-2.7\\gi\n    copying gi\\__init__.py -&gt; build\\lib.win-amd64-2.7\\gi\n    creating build\\lib.win-amd64-2.7\\gi\\repository\n    copying gi\\repository\\__init__.py -&gt; build\\lib.win-amd64-2.7\\gi\\repository\n    creating build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\Gdk.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\GIMarshallingTests.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\Gio.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\GLib.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\GObject.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\Gtk.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\keysyms.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\Pango.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    copying gi\\overrides\\__init__.py -&gt; build\\lib.win-amd64-2.7\\gi\\overrides\n    warning: build_py: byte-compiling is disabled, skipping.\n\n    running build_ext\n    pycairo: new API\n    pycairo: trying include directory: 'C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\lib\\\\site-packages\\\\cairo'\n    pycairo: header file ('C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\lib\\\\site-packages\\\\cairo\\\\pycairo.h') not found\n    pycairo: old API\n    pycairo: found pycairo 1.16.3 (c:\\users\\ratch\\pycharmprojects\\Project\\venv\\lib\\site-packages)\n    pycairo: trying include directory: 'C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\include\\\\pycairo'\n    pycairo: found 'C:\\\\Users\\\\ratch\\\\PycharmProjects\\\\Project\\\\venv\\\\include\\\\pycairo\\\\pycairo.h'\n    building 'gi._gi' extension\n    creating build\\temp.win-amd64-2.7\n    creating build\\temp.win-amd64-2.7\\Release\n    creating build\\temp.win-amd64-2.7\\Release\\gi\n    C:\\Users\\ratch\\AppData\\Local\\Programs\\Common\\Microsoft\\Visual C++ for Python\\9.0\\VC\\Bin\\amd64\\cl.exe /c /nologo /Ox /MD /W3 /GS- /DNDEBUG -DHAVE_CONFIG_H -DPY_SSIZE_T_CLEAN -IC:\\Users\\ratch\\AppData\\Local\\Temp\\pycharm-packaging\\PyGObject -IC:\\Users\\ratch\\AppData\\Local\\Temp\\pycharm-packaging\\PyGObject\\gi -IC:\\Python27\\include \"-IC:\\Users\\ratch\\PycharmProjects\\Project\\venv\\PC\" /Tcgi\\gimodule.c /Fobuild\\temp.win-amd64-2.7\\Release\\gi\\gimodule.obj\n    gimodule.c\n    gi\\gimodule.c(25) : fatal error C1083: Cannot open include file: 'glib-object.h': No such file or directory\n    error: command 'C:\\\\Users\\\\ratch\\\\AppData\\\\Local\\\\Programs\\\\Common\\\\Microsoft\\\\Visual C++ for Python\\\\9.0\\\\VC\\\\Bin\\\\amd64\\\\cl.exe' failed with exit status 2\n</code></pre>\n\n<p>I've installed the Microsoft Visual C++ Compiler for Python 2.7 and the Windows 10 SDK properly as far as i know. I would ask for help on getting the environment set up correctly preferably in PyCharm.</p>\n"
        },
        {
            "tags": [
                "r",
                "sqlite",
                "dplyr"
            ],
            "owner": {
                "reputation": 3184,
                "user_id": 152860,
                "user_type": "registered",
                "accept_rate": 81,
                "profile_image": "https://www.gravatar.com/avatar/e87b25ac6bec6427d102828128f8ac80?s=128&d=identicon&r=PG",
                "display_name": "Hedgehog",
                "link": "https://stackoverflow.com/users/152860/hedgehog"
            },
            "is_answered": false,
            "view_count": 33,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1526428029,
            "creation_date": 1526357314,
            "last_edit_date": 1526428029,
            "question_id": 50342182,
            "body_markdown": "Your data is stored in a large number of SQLite data base files.\r\nYou would like to gather the data from one table across all these database files.\r\n\r\nIs this possible using `dplyr`, or `tidyverse`?\r\n\r\n**Example Data:**\r\n\r\n    # Required Libraries\r\n    require(&#39;tidyverse&#39;)\r\n    require(&#39;RSQLite&#39;)\r\n    require(&#39;pool&#39;)\r\n    require(&#39;here&#39;)\r\n    \r\n    # Create the dummy data\r\n    test &lt;- data.frame(t(replicate(2,sample(0:10,4,rep=TRUE))))\r\n    \r\n    fn &lt;- here::here(&#39;1testing.sqlite3&#39;)\r\n    con &lt;- dbPool(drv = RSQLite::SQLite(), \r\n                  dbname = fn)\r\n    write_result = dbWriteTable(con, &quot;TEST&quot;, test)\r\n    poolClose(con)\r\n    rm(con)\r\n    \r\n    # Create multiple SQLite databases\r\n    fn = here::here(&#39;1testing.sqlite3&#39;)\r\n    file.copy(from=fn, to=here::here(&#39;2testing.sqlite3&#39;))\r\n    file.copy(from=fn, to=here::here(&#39;3testing.sqlite3&#39;))\r\n    \r\n\r\n**NOTE:** The accepted answer suggests creating a user-defined-function (UDF).  within this you could merge and process data from several tables, returning the end result. ",
            "link": "https://stackoverflow.com/questions/50342182/dplyr-gather-data-from-many-sqlite-data-bases",
            "title": "dplyr: Gather data from *many* SQLite data bases",
            "body": "<p>Your data is stored in a large number of SQLite data base files.\nYou would like to gather the data from one table across all these database files.</p>\n\n<p>Is this possible using <code>dplyr</code>, or <code>tidyverse</code>?</p>\n\n<p><strong>Example Data:</strong></p>\n\n<pre><code># Required Libraries\nrequire('tidyverse')\nrequire('RSQLite')\nrequire('pool')\nrequire('here')\n\n# Create the dummy data\ntest &lt;- data.frame(t(replicate(2,sample(0:10,4,rep=TRUE))))\n\nfn &lt;- here::here('1testing.sqlite3')\ncon &lt;- dbPool(drv = RSQLite::SQLite(), \n              dbname = fn)\nwrite_result = dbWriteTable(con, \"TEST\", test)\npoolClose(con)\nrm(con)\n\n# Create multiple SQLite databases\nfn = here::here('1testing.sqlite3')\nfile.copy(from=fn, to=here::here('2testing.sqlite3'))\nfile.copy(from=fn, to=here::here('3testing.sqlite3'))\n</code></pre>\n\n<p><strong>NOTE:</strong> The accepted answer suggests creating a user-defined-function (UDF).  within this you could merge and process data from several tables, returning the end result. </p>\n"
        },
        {
            "tags": [
                "pyspark",
                "amazon-emr",
                "pyspark-sql",
                "aws-glue"
            ],
            "owner": {
                "reputation": 15321,
                "user_id": 44757,
                "user_type": "registered",
                "accept_rate": 84,
                "profile_image": "https://www.gravatar.com/avatar/df2295610c6940e95160ce80b7b4e283?s=128&d=identicon&r=PG",
                "display_name": "Jared",
                "link": "https://stackoverflow.com/users/44757/jared"
            },
            "is_answered": false,
            "view_count": 35,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1526428018,
            "creation_date": 1525042058,
            "last_edit_date": 1526428018,
            "question_id": 50092022,
            "body_markdown": "Assume I have a CSV file like this:\r\n\r\n    &quot;Col1Name&quot;, &quot;Col2Name&quot;\r\n    &quot;a&quot;, &quot;b&quot;\r\n    &quot;c&quot;, &quot;d&quot;\r\n\r\nAssume I issue the following CREATE EXTERNAL TABLE command in Athena:\r\n\r\n    CREATE EXTERNAL TABLE test.sometable (\r\n       col1name string,\r\n       col2name string\r\n    ) \r\n    row format serde &#39;org.apache.hadoop.hive.serde2.OpenCSVSerde&#39;\r\n    with serdeproperties (\r\n       &#39;separatorChar&#39; = &#39;,&#39;,\r\n       &#39;quoteChar&#39; = &#39;\\&quot;&#39;,\r\n       &#39;escapeChar&#39; = &#39;\\\\&#39;\r\n    ) \r\n    stored as textfile\r\n    location &#39;s3://somebucket/some/path/&#39;\r\n    tblproperties(&quot;skip.header.line.count&quot;=&quot;1&quot;)\r\n\r\nThen I issue the following SELECT:\r\n\r\n    SELECT * FROM test.sometable\r\n\r\nI expect to get the following:\r\n\r\n    +----------+----------+\r\n    |  col1name|  col2name|\r\n    +----------+----------+\r\n    |         a|         b|\r\n    |         c|         d|\r\n    +----------+----------+\r\n\r\n...and sure enough, that&#39;s exactly what I get.\r\n\r\nOn an EMR cluster using the AWS Glue metadata catalog in Spark, I issue the following in the pyspark REPL:\r\n\r\n    a = spark.sql(&quot;select * from test.sometable&quot;)\r\n    a.show()\r\n\r\nI expect to receive the same output, but, instead, I get this:\r\n\r\n    +----------+----------+\r\n    |  col1name|  col2name|\r\n    +----------+----------+\r\n    |  col1name|  col2name|\r\n    |         a|         b|\r\n    |         c|         d|\r\n    +----------+----------+\r\n\r\nObviously, Athena is honoring the &quot;skip.header.line.count&quot; tblproperty, but PySpark appears to be ignoring it.\r\n\r\nHow can I get PySpark to ignore this header line, as Athena does?",
            "link": "https://stackoverflow.com/questions/50092022/how-to-ignore-headers-in-pyspark-when-using-athena-and-aws-glue-data-catalog",
            "title": "How to ignore headers in PySpark when using Athena and AWS Glue Data Catalog",
            "body": "<p>Assume I have a CSV file like this:</p>\n\n<pre><code>\"Col1Name\", \"Col2Name\"\n\"a\", \"b\"\n\"c\", \"d\"\n</code></pre>\n\n<p>Assume I issue the following CREATE EXTERNAL TABLE command in Athena:</p>\n\n<pre><code>CREATE EXTERNAL TABLE test.sometable (\n   col1name string,\n   col2name string\n) \nrow format serde 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nwith serdeproperties (\n   'separatorChar' = ',',\n   'quoteChar' = '\\\"',\n   'escapeChar' = '\\\\'\n) \nstored as textfile\nlocation 's3://somebucket/some/path/'\ntblproperties(\"skip.header.line.count\"=\"1\")\n</code></pre>\n\n<p>Then I issue the following SELECT:</p>\n\n<pre><code>SELECT * FROM test.sometable\n</code></pre>\n\n<p>I expect to get the following:</p>\n\n<pre><code>+----------+----------+\n|  col1name|  col2name|\n+----------+----------+\n|         a|         b|\n|         c|         d|\n+----------+----------+\n</code></pre>\n\n<p>...and sure enough, that's exactly what I get.</p>\n\n<p>On an EMR cluster using the AWS Glue metadata catalog in Spark, I issue the following in the pyspark REPL:</p>\n\n<pre><code>a = spark.sql(\"select * from test.sometable\")\na.show()\n</code></pre>\n\n<p>I expect to receive the same output, but, instead, I get this:</p>\n\n<pre><code>+----------+----------+\n|  col1name|  col2name|\n+----------+----------+\n|  col1name|  col2name|\n|         a|         b|\n|         c|         d|\n+----------+----------+\n</code></pre>\n\n<p>Obviously, Athena is honoring the \"skip.header.line.count\" tblproperty, but PySpark appears to be ignoring it.</p>\n\n<p>How can I get PySpark to ignore this header line, as Athena does?</p>\n"
        },
        {
            "tags": [
                "hive",
                "prestodb",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 3697,
                "user_id": 196032,
                "user_type": "registered",
                "accept_rate": 49,
                "profile_image": "https://www.gravatar.com/avatar/b75136316e93a66545dd346297fad09e?s=128&d=identicon&r=PG",
                "display_name": "Alex R",
                "link": "https://stackoverflow.com/users/196032/alex-r"
            },
            "is_answered": true,
            "view_count": 30,
            "accepted_answer_id": 50106429,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1526428006,
            "creation_date": 1525112251,
            "last_edit_date": 1526428006,
            "question_id": 50106046,
            "body_markdown": "I tried creating this simple table in Athena:\r\n\r\n    CREATE EXTERNAL TABLE ctc.rets (\r\n      `SystemID` string,\r\n      `blah` string\r\n    ) \r\n    ROW FORMAT SERDE &#39;org.openx.data.jsonserde.JsonSerDe&#39;\r\n    WITH SERDEPROPERTIES (\r\n      &#39;mapping.SystemID&#39; = &#39;L_ListingID&#39;,\r\n      &#39;mapping.blah&#39; = &#39;Ext_Char10_11&#39; \r\n    ) \r\n    LOCATION &#39;s3://xyz.bucket/mydata/&#39;\r\n    TBLPROPERTIES (&#39;has_encrypted_data&#39;=&#39;false&#39;);\r\n\r\nThe field named `blah` maps fine, but the field named `SystemID` comes up blank on every row.\r\n\r\nAnd then it gets really interesting:\r\n\r\n - I change the `SystemID` field name to `WTF`, or `foobar`, or `strawberry`, and **it works fine** (the data shows up).\r\n - I change the `SystemID` field name to `_SystemID`, `f_SystemID`, `ystemID`, `System_I_D`, and **none of them work**\r\n\r\nThere is never an error message.\r\n\r\nWhat are the actual rules that need to be followed for the field names?",
            "link": "https://stackoverflow.com/questions/50106046/aws-athena-presto-with-jsonserde-fails-quietly-on-some-column-names-which-one",
            "title": "AWS Athena (Presto with JsonSerde) fails quietly on some Column Names, which ones are acceptable?",
            "body": "<p>I tried creating this simple table in Athena:</p>\n\n<pre><code>CREATE EXTERNAL TABLE ctc.rets (\n  `SystemID` string,\n  `blah` string\n) \nROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\nWITH SERDEPROPERTIES (\n  'mapping.SystemID' = 'L_ListingID',\n  'mapping.blah' = 'Ext_Char10_11' \n) \nLOCATION 's3://xyz.bucket/mydata/'\nTBLPROPERTIES ('has_encrypted_data'='false');\n</code></pre>\n\n<p>The field named <code>blah</code> maps fine, but the field named <code>SystemID</code> comes up blank on every row.</p>\n\n<p>And then it gets really interesting:</p>\n\n<ul>\n<li>I change the <code>SystemID</code> field name to <code>WTF</code>, or <code>foobar</code>, or <code>strawberry</code>, and <strong>it works fine</strong> (the data shows up).</li>\n<li>I change the <code>SystemID</code> field name to <code>_SystemID</code>, <code>f_SystemID</code>, <code>ystemID</code>, <code>System_I_D</code>, and <strong>none of them work</strong></li>\n</ul>\n\n<p>There is never an error message.</p>\n\n<p>What are the actual rules that need to be followed for the field names?</p>\n"
        },
        {
            "tags": [
                "prestodb",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 649,
                "user_id": 5231528,
                "user_type": "registered",
                "accept_rate": 85,
                "profile_image": "https://i.stack.imgur.com/xhIP7.jpg?s=128&g=1",
                "display_name": "Miguel Coder",
                "link": "https://stackoverflow.com/users/5231528/miguel-coder"
            },
            "is_answered": true,
            "view_count": 19,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526427993,
            "creation_date": 1525202505,
            "last_edit_date": 1526427993,
            "question_id": 50122530,
            "body_markdown": "I have a few things I want to accomplish with PrestoDB. I currently getting some data in the following formats\r\n\r\n1. `date 16-Jan-2018`\r\n2. `num 1000`\r\n\r\nI want to write a query that can convert these values to\r\n\r\n1. `2018-01-16`\r\n2. `1,000`",
            "link": "https://stackoverflow.com/questions/50122530/converting-values-in-athena-prestodb",
            "title": "Converting values in Athena PrestoDB",
            "body": "<p>I have a few things I want to accomplish with PrestoDB. I currently getting some data in the following formats</p>\n\n<ol>\n<li><code>date 16-Jan-2018</code></li>\n<li><code>num 1000</code></li>\n</ol>\n\n<p>I want to write a query that can convert these values to</p>\n\n<ol>\n<li><code>2018-01-16</code></li>\n<li><code>1,000</code></li>\n</ol>\n"
        },
        {
            "tags": [
                "woocommerce-rest-api"
            ],
            "owner": {
                "reputation": 836,
                "user_id": 1532104,
                "user_type": "registered",
                "accept_rate": 29,
                "profile_image": "https://www.gravatar.com/avatar/9163f18494c738d1a484e9af63b40ed3?s=128&d=identicon&r=PG",
                "display_name": "user125264",
                "link": "https://stackoverflow.com/users/1532104/user125264"
            },
            "is_answered": false,
            "view_count": 7,
            "answer_count": 0,
            "score": 1,
            "last_activity_date": 1526427977,
            "creation_date": 1526427977,
            "question_id": 50360607,
            "body_markdown": "I&#39;m trying to figure out how to add a role to a customer via the WooCommerce REST API.\r\n\r\nWhat I thought would be correct of simply adding &quot;role&quot;:&quot;test_role&quot; doesnt appear to work as the docs are indicating its read only.\r\n\r\nIm quite new to WooCommerce, and would have thought simply adding a role to user wouldnt be a difficult task.\r\n\r\nIs there a way to extend the API to make this writable? or is it even possible to add a role to a customer?",
            "link": "https://stackoverflow.com/questions/50360607/woocommerce-api-add-role-to-customer",
            "title": "WooCommerce API Add Role to Customer",
            "body": "<p>I'm trying to figure out how to add a role to a customer via the WooCommerce REST API.</p>\n\n<p>What I thought would be correct of simply adding \"role\":\"test_role\" doesnt appear to work as the docs are indicating its read only.</p>\n\n<p>Im quite new to WooCommerce, and would have thought simply adding a role to user wouldnt be a difficult task.</p>\n\n<p>Is there a way to extend the API to make this writable? or is it even possible to add a role to a customer?</p>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "amazon-athena"
            ],
            "owner": {
                "reputation": 160,
                "user_id": 467498,
                "user_type": "registered",
                "accept_rate": 50,
                "profile_image": "https://www.gravatar.com/avatar/9ebdae3cfed543713f4e89f51ac0736e?s=128&d=identicon&r=PG",
                "display_name": "Austin",
                "link": "https://stackoverflow.com/users/467498/austin"
            },
            "is_answered": false,
            "view_count": 17,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526427970,
            "creation_date": 1525375818,
            "last_edit_date": 1526427970,
            "question_id": 50162586,
            "body_markdown": "I&#39;m in a bit of a dilemna here:\r\n\r\nI&#39;m using AWS Athena to query against some JSON objects. Most of the JSON records are structured, but one field in particular (&quot;changes&quot;) has dynamic objects whose fields don&#39;t really have a set structure. For example, here&#39;s a record: \r\n\r\n    {\r\n        id: 1,\r\n        user_id: 2,\r\n        changes: {\r\n        &quot;customer_id&quot; 1,\r\n        &quot;business_name: [&#39;old name&#39;, &#39;new name&#39;]\r\n        }\r\n    }\r\n\r\nEvery record has different keys and the value types vary. How can I represent this data? I thought maybe a string, but when I try to store it that way I get JSON parsing errors when decoding. Any help would be appreciated! Thanks!\r\n",
            "link": "https://stackoverflow.com/questions/50162586/athena-creating-a-dynamic-json-column",
            "title": "Athena - Creating a Dynamic JSON Column",
            "body": "<p>I'm in a bit of a dilemna here:</p>\n\n<p>I'm using AWS Athena to query against some JSON objects. Most of the JSON records are structured, but one field in particular (\"changes\") has dynamic objects whose fields don't really have a set structure. For example, here's a record: </p>\n\n<pre><code>{\n    id: 1,\n    user_id: 2,\n    changes: {\n    \"customer_id\" 1,\n    \"business_name: ['old name', 'new name']\n    }\n}\n</code></pre>\n\n<p>Every record has different keys and the value types vary. How can I represent this data? I thought maybe a string, but when I try to store it that way I get JSON parsing errors when decoding. Any help would be appreciated! Thanks!</p>\n"
        },
        {
            "tags": [
                "ractivejs"
            ],
            "owner": {
                "reputation": 1469,
                "user_id": 3144603,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/c473ee7208c0326be23fd1b474f89014?s=128&d=identicon&r=PG",
                "display_name": "JohnnyFun",
                "link": "https://stackoverflow.com/users/3144603/johnnyfun"
            },
            "is_answered": false,
            "view_count": 8,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526427965,
            "creation_date": 1526422011,
            "last_edit_date": 1526427965,
            "question_id": 50359825,
            "body_markdown": "I wanted to insert my modals at the end of the root element, so they don&#39;t inherit styling and/or clicks awkwardly, but as my short example below shows, it appears it doesn&#39;t quite work if I `ractive.insert()` a component in the context of an `#each`, which is in the context of a `{{yield}}`:\r\n\r\n&lt;!-- begin snippet: js hide: false console: false babel: false --&gt;\r\n\r\n&lt;!-- language: lang-js --&gt;\r\n\r\n    Ractive.components.scenario = Ractive.extend({ \r\n        template: &#39;#scenario&#39;, \r\n        data: {\r\n            things: [{name: &#39;Inside Each&#39;}]\r\n        }  \r\n    })\r\n    Ractive.components.modal = Ractive.extend({\r\n        template: &#39;#modal&#39;,\r\n        data() {\r\n            return {\r\n                open: true\r\n            }\r\n        },\r\n        on: {\r\n            render() {\r\n                this.observe(&#39;open&#39;, open =&gt; {\r\n                    if (open === true) {\r\n                        this.insert(this.root.el)\r\n                    } else {\r\n                        this.detach()\r\n                    }\r\n                })\r\n            }\r\n        }\r\n    })\r\n    Ractive.components.wrapper = Ractive.extend({ template: &#39;#wrapper&#39; })\r\n    var ractive = new Ractive({\r\n        target: &#39;body&#39;,\r\n        template: &#39;#app&#39;\r\n    })\r\n\r\n&lt;!-- language: lang-css --&gt;\r\n\r\n    .modal {\r\n        padding:10px;\r\n        border:1px solid black;\r\n    }\r\n\r\n&lt;!-- language: lang-html --&gt;\r\n\r\n    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/ractive@0.9.13/ractive.js&quot;&gt;&lt;/script&gt;\r\n    &lt;script id=&quot;app&quot; type=&quot;text/ractive&quot;&gt;\r\n        &lt;wrapper&gt;\r\n            &lt;p&gt;Inside the wrapper yield, the &#39;close&#39; buttons don&#39;t work if I use ractive.insert()&lt;/p&gt;\r\n            &lt;scenario /&gt;\r\n        &lt;/wrapper&gt;\r\n        &lt;p&gt;But outside the wrapper yield, all works as expected:&lt;/p&gt;\r\n        &lt;scenario /&gt;\r\n    &lt;/script&gt;\r\n\r\n    &lt;script id=&quot;scenario&quot; type=&quot;text/ractive&quot;&gt;\r\n        &lt;modal&gt;\r\n            &lt;strong&gt;Outside Each&lt;/strong&gt;\r\n        &lt;/modal&gt;\r\n        {{#each things}}\r\n            &lt;modal&gt;{{.name}}&lt;/modal&gt;\r\n        {{/each}}\r\n    &lt;/script&gt;\r\n\r\n    &lt;script id=&quot;wrapper&quot; type=&quot;text/ractive&quot;&gt;\r\n        &lt;div class=&quot;wrapper&quot;&gt;\r\n            {{yield}}\r\n        &lt;/div&gt;\r\n    &lt;/script&gt;\r\n\r\n    &lt;script id=&quot;modal&quot; type=&quot;text/ractive&quot;&gt;\r\n        &lt;div&gt;\r\n            {{#if open}}\r\n            &lt;div class=&quot;modal&quot;&gt;\r\n                {{yield}}\r\n                &lt;button on-click=&quot;@this.set(&#39;open&#39;, false)&quot;&gt;\r\n                    Close\r\n                &lt;/button&gt;\r\n            &lt;/div&gt;\r\n            {{/if}}\r\n        &lt;/div&gt;\r\n    &lt;/script&gt;\r\n\r\n&lt;!-- end snippet --&gt;\r\n\r\nfwiw, I&#39;m using ractive 0.9.13. I tried with edge, but no luck.\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50359825/ractive-insert-doesnt-seem-to-work-inside-an-each-loop-which-is-inside-a-yield",
            "title": "ractive.insert doesn&#39;t seem to work inside an each loop, which is inside a yield",
            "body": "<p>I wanted to insert my modals at the end of the root element, so they don't inherit styling and/or clicks awkwardly, but as my short example below shows, it appears it doesn't quite work if I <code>ractive.insert()</code> a component in the context of an <code>#each</code>, which is in the context of a <code>{{yield}}</code>:</p>\n\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"false\" data-babel=\"false\">\r\n<div class=\"snippet-code\">\r\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>Ractive.components.scenario = Ractive.extend({ \r\n    template: '#scenario', \r\n    data: {\r\n        things: [{name: 'Inside Each'}]\r\n    }  \r\n})\r\nRactive.components.modal = Ractive.extend({\r\n    template: '#modal',\r\n    data() {\r\n        return {\r\n            open: true\r\n        }\r\n    },\r\n    on: {\r\n        render() {\r\n            this.observe('open', open =&gt; {\r\n                if (open === true) {\r\n                    this.insert(this.root.el)\r\n                } else {\r\n                    this.detach()\r\n                }\r\n            })\r\n        }\r\n    }\r\n})\r\nRactive.components.wrapper = Ractive.extend({ template: '#wrapper' })\r\nvar ractive = new Ractive({\r\n    target: 'body',\r\n    template: '#app'\r\n})</code></pre>\r\n<pre class=\"snippet-code-css lang-css prettyprint-override\"><code>.modal {\r\n    padding:10px;\r\n    border:1px solid black;\r\n}</code></pre>\r\n<pre class=\"snippet-code-html lang-html prettyprint-override\"><code>&lt;script src=\"https://cdn.jsdelivr.net/npm/ractive@0.9.13/ractive.js\"&gt;&lt;/script&gt;\r\n&lt;script id=\"app\" type=\"text/ractive\"&gt;\r\n    &lt;wrapper&gt;\r\n        &lt;p&gt;Inside the wrapper yield, the 'close' buttons don't work if I use ractive.insert()&lt;/p&gt;\r\n        &lt;scenario /&gt;\r\n    &lt;/wrapper&gt;\r\n    &lt;p&gt;But outside the wrapper yield, all works as expected:&lt;/p&gt;\r\n    &lt;scenario /&gt;\r\n&lt;/script&gt;\r\n\r\n&lt;script id=\"scenario\" type=\"text/ractive\"&gt;\r\n    &lt;modal&gt;\r\n        &lt;strong&gt;Outside Each&lt;/strong&gt;\r\n    &lt;/modal&gt;\r\n    {{#each things}}\r\n        &lt;modal&gt;{{.name}}&lt;/modal&gt;\r\n    {{/each}}\r\n&lt;/script&gt;\r\n\r\n&lt;script id=\"wrapper\" type=\"text/ractive\"&gt;\r\n    &lt;div class=\"wrapper\"&gt;\r\n        {{yield}}\r\n    &lt;/div&gt;\r\n&lt;/script&gt;\r\n\r\n&lt;script id=\"modal\" type=\"text/ractive\"&gt;\r\n    &lt;div&gt;\r\n        {{#if open}}\r\n        &lt;div class=\"modal\"&gt;\r\n            {{yield}}\r\n            &lt;button on-click=\"@this.set('open', false)\"&gt;\r\n                Close\r\n            &lt;/button&gt;\r\n        &lt;/div&gt;\r\n        {{/if}}\r\n    &lt;/div&gt;\r\n&lt;/script&gt;</code></pre>\r\n</div>\r\n</div>\r\n</p>\n\n<p>fwiw, I'm using ractive 0.9.13. I tried with edge, but no luck.</p>\n"
        },
        {
            "tags": [
                "angular5"
            ],
            "owner": {
                "reputation": 12,
                "user_id": 2600658,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/9d17387830348b990143180962b6e4d1?s=128&d=identicon&r=PG",
                "display_name": "Shankar",
                "link": "https://stackoverflow.com/users/2600658/shankar"
            },
            "is_answered": true,
            "view_count": 12,
            "accepted_answer_id": 50360603,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1526427964,
            "creation_date": 1526423906,
            "question_id": 50360088,
            "body_markdown": "I have 4 Angular components. I would like to read the content of entire component for 3 of them into the 4 component as HTML string.\r\nI would like to use this HTML string to create a WORD document.\r\n\r\nPlease let me know how to do this.\r\n",
            "link": "https://stackoverflow.com/questions/50360088/angular-5-read-html-from-other-components",
            "title": "Angular 5: Read HTML from other components",
            "body": "<p>I have 4 Angular components. I would like to read the content of entire component for 3 of them into the 4 component as HTML string.\nI would like to use this HTML string to create a WORD document.</p>\n\n<p>Please let me know how to do this.</p>\n"
        },
        {
            "tags": [
                "python",
                "parallel-processing",
                "joblib"
            ],
            "owner": {
                "reputation": 169,
                "user_id": 5037235,
                "user_type": "registered",
                "accept_rate": 79,
                "profile_image": "https://lh5.googleusercontent.com/-bEbbj3FOfNc/AAAAAAAAAAI/AAAAAAAAArk/8vcRH1Ane5A/photo.jpg?sz=128",
                "display_name": "Shaowu",
                "link": "https://stackoverflow.com/users/5037235/shaowu"
            },
            "is_answered": false,
            "view_count": 19,
            "answer_count": 0,
            "score": -1,
            "last_activity_date": 1526427940,
            "creation_date": 1526423595,
            "last_edit_date": 1526427940,
            "question_id": 50360043,
            "body_markdown": "## I was trying to check if joblib is really working....\r\nSo I generate a few data points first, save them in the disk. \r\nthen I read them, computing the square sum. \r\n\r\nIt turns out that on my laptop with **4 cores intel i5**, time for n_jobs = 1, is 13.7s, n_jobs = 4 is 15.7 s.\r\n\r\n    from math import sqrt\r\n    from joblib import Parallel, delayed\r\n    import numpy as np\r\n\r\n    def my_func(x,y):\r\n        return x*x+y*y\r\n\r\n    def load1d(path):\r\n        data = np.load(path)\r\n        return data\r\n\r\n    def gen_data(size):\r\n        np.savez(&#39;./data.npz&#39;, data=np.random.rand(size,size).flatten())\r\n        return\r\n\r\n    def run(n_jobs):\r\n        data_1d = load1d(&#39;./data.npz&#39;)\r\n        val = Parallel(n_jobs=n_jobs, verbose=50)(delayed(my_func)(data_1d[i], data_1d[i]) for i in xrange(data_1d.size))\r\n        # print &#39;jobs = &#39;,n_jobs\r\n        # print val\r\n\r\n    if __name__==&#39;__main__&#39;:\r\n        # gen_data(500)\r\n        run(n_jobs=4)\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50360043/python-joblib-isnt-faster-four-cores-intel-i5",
            "title": "python-joblib isn&#39;t faster, four cores intel i5",
            "body": "<h2>I was trying to check if joblib is really working....</h2>\n\n<p>So I generate a few data points first, save them in the disk. \nthen I read them, computing the square sum. </p>\n\n<p>It turns out that on my laptop with <strong>4 cores intel i5</strong>, time for n_jobs = 1, is 13.7s, n_jobs = 4 is 15.7 s.</p>\n\n<pre><code>from math import sqrt\nfrom joblib import Parallel, delayed\nimport numpy as np\n\ndef my_func(x,y):\n    return x*x+y*y\n\ndef load1d(path):\n    data = np.load(path)\n    return data\n\ndef gen_data(size):\n    np.savez('./data.npz', data=np.random.rand(size,size).flatten())\n    return\n\ndef run(n_jobs):\n    data_1d = load1d('./data.npz')\n    val = Parallel(n_jobs=n_jobs, verbose=50)(delayed(my_func)(data_1d[i], data_1d[i]) for i in xrange(data_1d.size))\n    # print 'jobs = ',n_jobs\n    # print val\n\nif __name__=='__main__':\n    # gen_data(500)\n    run(n_jobs=4)\n</code></pre>\n"
        },
        {
            "tags": [
                "c#",
                "performance",
                "entity-framework",
                "linq",
                "if-statement"
            ],
            "owner": {
                "reputation": 15936,
                "user_id": 2218635,
                "user_type": "registered",
                "accept_rate": 57,
                "profile_image": "https://i.stack.imgur.com/Uh7jt.jpg?s=128&g=1",
                "display_name": "Ramesh Rajendran",
                "link": "https://stackoverflow.com/users/2218635/ramesh-rajendran"
            },
            "is_answered": true,
            "view_count": 6086,
            "accepted_answer_id": 30070286,
            "answer_count": 3,
            "score": 6,
            "last_activity_date": 1526427939,
            "creation_date": 1430896699,
            "last_edit_date": 1473679334,
            "question_id": 30070011,
            "body_markdown": "Difference between and condition and two where condition in entity framework query\r\n------------------------------------------------------------------------\r\n\r\n**Code 1** \r\n\r\nI have using two where condition in my query\r\n\r\n     dbContext.Projects.Where(p=&gt;p.ProjectId!=ProjectId).Where(p=&gt;p.Name==Name)\r\n     .SingleOrDefault();\r\n\r\n\r\n----------\r\n\r\n\r\n**code 2**\r\n\r\n \r\n\r\n   I have using &amp;&amp; condition without using two where condition \r\n\r\n      dbContext.Projects.Where(p=&gt;p.ProjectId!=ProjectId &amp;&amp;  \r\n      p.Name==Name).SingleOrDefault();\r\n\r\n\r\n----------\r\n- What is the difference between **code1** and **code2**????\r\n\r\n&gt;The both queries are return same value. but i don&#39;t know the differences. Please explain to me, which one is better. and why?",
            "link": "https://stackoverflow.com/questions/30070011/difference-between-and-where-condition-in-entity-framework-query",
            "title": "Difference between &amp;&amp; and where condition in entity framework query",
            "body": "<h2>Difference between and condition and two where condition in entity framework query</h2>\n\n<p><strong>Code 1</strong> </p>\n\n<p>I have using two where condition in my query</p>\n\n<pre><code> dbContext.Projects.Where(p=&gt;p.ProjectId!=ProjectId).Where(p=&gt;p.Name==Name)\n .SingleOrDefault();\n</code></pre>\n\n<hr>\n\n<p><strong>code 2</strong></p>\n\n<p>I have using &amp;&amp; condition without using two where condition </p>\n\n<pre><code>  dbContext.Projects.Where(p=&gt;p.ProjectId!=ProjectId &amp;&amp;  \n  p.Name==Name).SingleOrDefault();\n</code></pre>\n\n<hr>\n\n<ul>\n<li>What is the difference between <strong>code1</strong> and <strong>code2</strong>????</li>\n</ul>\n\n<blockquote>\n  <p>The both queries are return same value. but i don't know the differences. Please explain to me, which one is better. and why?</p>\n</blockquote>\n"
        },
        {
            "tags": [
                "python-2.7",
                "amazon-web-services",
                "amazon-kinesis",
                "amazon-kinesis-firehose",
                "amazon-kinesis-agent"
            ],
            "owner": {
                "reputation": 139,
                "user_id": 4470126,
                "user_type": "registered",
                "accept_rate": 82,
                "profile_image": "https://www.gravatar.com/avatar/30c5924d5c914cc8d98254c8c9cea597?s=128&d=identicon&r=PG&f=1",
                "display_name": "Yuva Kumar",
                "link": "https://stackoverflow.com/users/4470126/yuva-kumar"
            },
            "is_answered": false,
            "view_count": 30,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526427912,
            "creation_date": 1523008365,
            "last_edit_date": 1526427912,
            "question_id": 49690054,
            "body_markdown": "Am trying to create a simple stream data from a python code, being appended to a log file, and load the streaming data to a Redshift cluster using a Kinesis Agent-&gt;Kinesis Firehose.\r\n\r\nI confirm that the python code is running fine, creates streaming data which is being appended to a log file.\r\nMy kinesis agent is started successfully and I verified, and have also done appropriate configurations to pull data from the log file and push it to firehose stream.\r\n\r\nThe python file is a simple code that logs lat/long positions randomly.  This is only to do a sample firehose streaming, and hence used sample data:\r\n\r\n    latitude = 19.99\r\n    longitude = 73.78\r\n    file_n = &#39;/tmp/random_lat_lon.log&#39;\r\n    \r\n    def generate_random_data(lat, lon, num_rows, file_name):\r\n        with open(file_name, &#39;w+&#39;, 1) as output:\r\n    #        for _ in xrange(num_rows):\r\n             while True:\r\n                hex1 = &#39;%012x&#39; % random.randrange(16**12)\r\n                flt = float(random.randint(0,100))\r\n                dec_lat = random.random()/100\r\n                dec_lon = random.random()/100\r\n                output.write(&#39;%s,%.1f,%.6f,%.6f \\n&#39; % (hex1.lower(), flt, lon+dec_lon, lat+dec_lat))\r\n                time.sleep(5)\r\n    \r\n    generate_random_data(latitude, longitude, 5, file_n)\r\n\r\nOutput in the random_lat_lon.log file:\r\n\r\n    &gt; 83d6c9f7a0be,25.0,73.782042,19.997504\r\n    &gt; 18b69c5c5248,25.0,73.788921,19.995153\r\n    &gt; 6a0d182996f0,91.0,73.783399,19.998097\r\n    &gt; 431ba9e4f38e,0.0,73.781139,19.995481\r\n\r\nWhen I check the kinesis-Agent, I see that its not working, and i am getting the following error trace:\r\n\r\n    (FileTailer[kinesis:python-stream:/tmp/random_lat_lon.log*]) com.amazon.kinesis.streaming.agent.tailing.FileTailer [ERROR] FileTailer[kinesis:python-stream:/tmp/random_lat_lon.log*]: Error when processing current input file or when tracking its status.\r\n    java.lang.IllegalStateException\r\n            at com.google.common.base.Preconditions.checkState(Preconditions.java:158)\r\n            at com.amazon.kinesis.streaming.agent.tailing.TrackedFileRotationAnalyzer.findCurrentOpenFileAfterTruncate(Unknown Source)\r\n            at com.amazon.kinesis.streaming.agent.tailing.SourceFileTracker.updateCurrentFile(Unknown Source)\r\n            at com.amazon.kinesis.streaming.agent.tailing.SourceFileTracker.refresh(Unknown Source)\r\n            at com.amazon.kinesis.streaming.agent.tailing.FileTailer.updateRecordParser(Unknown Source)\r\n            at com.amazon.kinesis.streaming.agent.tailing.FileTailer.processRecords(Unknown Source)\r\n            at com.amazon.kinesis.streaming.agent.tailing.FileTailer.runOnce(Unknown Source)\r\n            at com.amazon.kinesis.streaming.agent.tailing.FileTailer.run(Unknown Source)\r\n            at com.google.common.util.concurrent.AbstractExecutionThreadService$1$2.run(AbstractExecutionThreadService.java:60)\r\n            at com.google.common.util.concurrent.Callables$3.run(Callables.java:95)\r\n            at java.lang.Thread.run(Thread.java:748)\r\n\r\nMy kinesis-Agent.json configuration is as follows:\r\n\r\n    {\r\n      &quot;cloudwatch.emitMetrics&quot;: true,\r\n      &quot;kinesis.endpoint&quot;: &quot;https://kinesis.us-east-1.amazonaws.com&quot;,\r\n      &quot;firehose.endpoint&quot;: &quot;&quot;,\r\n    \r\n      &quot;flows&quot;: [\r\n        {\r\n          &quot;filePattern&quot;: &quot;/tmp/random_lat_lon.log*&quot;,\r\n          &quot;kinesisStream&quot;: &quot;python-stream&quot;\r\n        }\r\n      ]\r\n    }\r\n\r\nThis is my first sample lab experience with Kinesis Firehose (using Python).  Am missing something which I couldn&#39;t figure out.\r\n\r\nCan someone please help with suggestions.  Let me know if any details required.\r\n\r\nRegards",
            "link": "https://stackoverflow.com/questions/49690054/kinesis-agent-reports-error-from-python-stream-data",
            "title": "Kinesis Agent reports error from python stream data",
            "body": "<p>Am trying to create a simple stream data from a python code, being appended to a log file, and load the streaming data to a Redshift cluster using a Kinesis Agent->Kinesis Firehose.</p>\n\n<p>I confirm that the python code is running fine, creates streaming data which is being appended to a log file.\nMy kinesis agent is started successfully and I verified, and have also done appropriate configurations to pull data from the log file and push it to firehose stream.</p>\n\n<p>The python file is a simple code that logs lat/long positions randomly.  This is only to do a sample firehose streaming, and hence used sample data:</p>\n\n<pre><code>latitude = 19.99\nlongitude = 73.78\nfile_n = '/tmp/random_lat_lon.log'\n\ndef generate_random_data(lat, lon, num_rows, file_name):\n    with open(file_name, 'w+', 1) as output:\n#        for _ in xrange(num_rows):\n         while True:\n            hex1 = '%012x' % random.randrange(16**12)\n            flt = float(random.randint(0,100))\n            dec_lat = random.random()/100\n            dec_lon = random.random()/100\n            output.write('%s,%.1f,%.6f,%.6f \\n' % (hex1.lower(), flt, lon+dec_lon, lat+dec_lat))\n            time.sleep(5)\n\ngenerate_random_data(latitude, longitude, 5, file_n)\n</code></pre>\n\n<p>Output in the random_lat_lon.log file:</p>\n\n<pre><code>&gt; 83d6c9f7a0be,25.0,73.782042,19.997504\n&gt; 18b69c5c5248,25.0,73.788921,19.995153\n&gt; 6a0d182996f0,91.0,73.783399,19.998097\n&gt; 431ba9e4f38e,0.0,73.781139,19.995481\n</code></pre>\n\n<p>When I check the kinesis-Agent, I see that its not working, and i am getting the following error trace:</p>\n\n<pre><code>(FileTailer[kinesis:python-stream:/tmp/random_lat_lon.log*]) com.amazon.kinesis.streaming.agent.tailing.FileTailer [ERROR] FileTailer[kinesis:python-stream:/tmp/random_lat_lon.log*]: Error when processing current input file or when tracking its status.\njava.lang.IllegalStateException\n        at com.google.common.base.Preconditions.checkState(Preconditions.java:158)\n        at com.amazon.kinesis.streaming.agent.tailing.TrackedFileRotationAnalyzer.findCurrentOpenFileAfterTruncate(Unknown Source)\n        at com.amazon.kinesis.streaming.agent.tailing.SourceFileTracker.updateCurrentFile(Unknown Source)\n        at com.amazon.kinesis.streaming.agent.tailing.SourceFileTracker.refresh(Unknown Source)\n        at com.amazon.kinesis.streaming.agent.tailing.FileTailer.updateRecordParser(Unknown Source)\n        at com.amazon.kinesis.streaming.agent.tailing.FileTailer.processRecords(Unknown Source)\n        at com.amazon.kinesis.streaming.agent.tailing.FileTailer.runOnce(Unknown Source)\n        at com.amazon.kinesis.streaming.agent.tailing.FileTailer.run(Unknown Source)\n        at com.google.common.util.concurrent.AbstractExecutionThreadService$1$2.run(AbstractExecutionThreadService.java:60)\n        at com.google.common.util.concurrent.Callables$3.run(Callables.java:95)\n        at java.lang.Thread.run(Thread.java:748)\n</code></pre>\n\n<p>My kinesis-Agent.json configuration is as follows:</p>\n\n<pre><code>{\n  \"cloudwatch.emitMetrics\": true,\n  \"kinesis.endpoint\": \"https://kinesis.us-east-1.amazonaws.com\",\n  \"firehose.endpoint\": \"\",\n\n  \"flows\": [\n    {\n      \"filePattern\": \"/tmp/random_lat_lon.log*\",\n      \"kinesisStream\": \"python-stream\"\n    }\n  ]\n}\n</code></pre>\n\n<p>This is my first sample lab experience with Kinesis Firehose (using Python).  Am missing something which I couldn't figure out.</p>\n\n<p>Can someone please help with suggestions.  Let me know if any details required.</p>\n\n<p>Regards</p>\n"
        },
        {
            "tags": [
                "angularjs",
                "ngmodel"
            ],
            "owner": {
                "reputation": 236,
                "user_id": 6847172,
                "user_type": "registered",
                "accept_rate": 71,
                "profile_image": "https://www.gravatar.com/avatar/6163251617b8f12d4fc93a66b1a5da1e?s=128&d=identicon&r=PG&f=1",
                "display_name": "victor",
                "link": "https://stackoverflow.com/users/6847172/victor"
            },
            "is_answered": false,
            "view_count": 20,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526427906,
            "creation_date": 1526427906,
            "question_id": 50360598,
            "body_markdown": "I&#39;m trying to work this idea out... ng-model inside ng-model directive... \r\nexample:\r\n\r\n    &lt;div ng-model=&quot;objectTest.testing&quot; ngModelDirective &gt;\r\n      Blah blah blah...\r\n      &lt;span ng-model=&quot;objectTest2.testing2&quot; ngModelDirective&gt;This is not working&lt;/span&gt;\r\n    &lt;/div&gt;\r\n\r\nOnly objectTest.testing is updated while objectTest2.testing2 is not working with no errors at all... I hope find some ideas how to make this work ... thanks much !!",
            "link": "https://stackoverflow.com/questions/50360598/is-it-possible-in-angularjs-v1-to-ng-model-inside-ng-model",
            "title": "Is it possible in AngularJS v1.* to ng-model inside ng-model?",
            "body": "<p>I'm trying to work this idea out... ng-model inside ng-model directive... \nexample:</p>\n\n<pre><code>&lt;div ng-model=\"objectTest.testing\" ngModelDirective &gt;\n  Blah blah blah...\n  &lt;span ng-model=\"objectTest2.testing2\" ngModelDirective&gt;This is not working&lt;/span&gt;\n&lt;/div&gt;\n</code></pre>\n\n<p>Only objectTest.testing is updated while objectTest2.testing2 is not working with no errors at all... I hope find some ideas how to make this work ... thanks much !!</p>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "aws-lambda",
                "protocol-buffers",
                "parquet",
                "amazon-kinesis-firehose"
            ],
            "owner": {
                "reputation": 361,
                "user_id": 1017056,
                "user_type": "registered",
                "accept_rate": 89,
                "profile_image": "https://i.stack.imgur.com/lOdk3.jpg?s=128&g=1",
                "display_name": "KunalC",
                "link": "https://stackoverflow.com/users/1017056/kunalc"
            },
            "is_answered": false,
            "view_count": 55,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526427898,
            "creation_date": 1522959541,
            "last_edit_date": 1526427898,
            "question_id": 49680869,
            "body_markdown": "I have a long-running process which generates control signals for different entities (approx. 10k) each minute. I would like to output these control signals along with various inputs (reference points, observed values) to S3 in parquet so that we can perform some analysis on the generated control signals in an ad-hoc way. The long-running process outputs the control signals and the various inputs each minute as a proto (or JSON, proto to JSON is pretty straightforward). I am thinking of streaming the output of the long-running process into a Kinesis Firehose and using a Lambda to convert to proto/JSON to parquet. Can someone provide some pointers for lambda to convert proto/JSON to parquet files?\r\n",
            "link": "https://stackoverflow.com/questions/49680869/converting-protos-json-in-aws-kinesis-firehose-to-parquet",
            "title": "Converting protos/json in AWS Kinesis Firehose to Parquet",
            "body": "<p>I have a long-running process which generates control signals for different entities (approx. 10k) each minute. I would like to output these control signals along with various inputs (reference points, observed values) to S3 in parquet so that we can perform some analysis on the generated control signals in an ad-hoc way. The long-running process outputs the control signals and the various inputs each minute as a proto (or JSON, proto to JSON is pretty straightforward). I am thinking of streaming the output of the long-running process into a Kinesis Firehose and using a Lambda to convert to proto/JSON to parquet. Can someone provide some pointers for lambda to convert proto/JSON to parquet files?</p>\n"
        },
        {
            "tags": [
                "amazon-web-services",
                "tableau-server",
                "amazon-kinesis-firehose"
            ],
            "owner": {
                "reputation": 56,
                "user_id": 2682394,
                "user_type": "registered",
                "accept_rate": 88,
                "profile_image": "https://graph.facebook.com/1832077718/picture?type=large",
                "display_name": "Krishnakanth Jc",
                "link": "https://stackoverflow.com/users/2682394/krishnakanth-jc"
            },
            "is_answered": false,
            "view_count": 24,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1526427893,
            "creation_date": 1522788459,
            "last_edit_date": 1526427893,
            "question_id": 49638928,
            "body_markdown": "I am creating a data pipeline and using Kinesis Firehose Delivery stream. \r\nSince I have this realtime data in a pipeline, May I know what is the best way to visualize this data in realtime. Are there any Tableau connectors for Kinesis. Any Suggested articles/ approaches would be helpful. ",
            "link": "https://stackoverflow.com/questions/49638928/amazon-kinesis-analytics-realtime-visualization",
            "title": "Amazon Kinesis Analytics RealTime Visualization",
            "body": "<p>I am creating a data pipeline and using Kinesis Firehose Delivery stream. \nSince I have this realtime data in a pipeline, May I know what is the best way to visualize this data in realtime. Are there any Tableau connectors for Kinesis. Any Suggested articles/ approaches would be helpful. </p>\n"
        },
        {
            "tags": [
                "scala",
                "function-literal"
            ],
            "owner": {
                "reputation": 2036,
                "user_id": 3837778,
                "user_type": "registered",
                "accept_rate": 71,
                "profile_image": "https://www.gravatar.com/avatar/12c217d346b797e1a9b4d34b9aa91dce?s=128&d=identicon&r=PG&f=1",
                "display_name": "BAE",
                "link": "https://stackoverflow.com/users/3837778/bae"
            },
            "is_answered": true,
            "view_count": 43,
            "accepted_answer_id": 50359493,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1526427883,
            "creation_date": 1526419003,
            "last_edit_date": 1526427883,
            "question_id": 50359289,
            "body_markdown": "Code piece 1\r\n\r\n    maps foreach { case (k, v) =&gt;\r\n      // do something\r\n    }\r\n\r\n\r\ncode piece 2:\r\n\r\n    maps foreach { \r\n      case (k, v) =&gt; {\r\n        // do something\r\n      }\r\n    }\r\n\r\nI am new to scala. Just wonder whether the above two pieces of codes are the same or not? which one is better?\r\n\r\nThanks\r\n",
            "link": "https://stackoverflow.com/questions/50359289/scala-whether-the-following-two-are-the-same",
            "title": "scala: whether the following two are the same",
            "body": "<p>Code piece 1</p>\n\n<pre><code>maps foreach { case (k, v) =&gt;\n  // do something\n}\n</code></pre>\n\n<p>code piece 2:</p>\n\n<pre><code>maps foreach { \n  case (k, v) =&gt; {\n    // do something\n  }\n}\n</code></pre>\n\n<p>I am new to scala. Just wonder whether the above two pieces of codes are the same or not? which one is better?</p>\n\n<p>Thanks</p>\n"
        },
        {
            "tags": [
                "r",
                "largenumber"
            ],
            "owner": {
                "reputation": 18,
                "user_id": 7949478,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/c7488a1358059e151e3f92ec9878152d?s=128&d=identicon&r=PG&f=1",
                "display_name": "Maryam",
                "link": "https://stackoverflow.com/users/7949478/maryam"
            },
            "is_answered": true,
            "view_count": 64,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1526427877,
            "creation_date": 1526416121,
            "last_edit_date": 1526416919,
            "question_id": 50358702,
            "body_markdown": "I want to write a vector into a file and then read it using Rstudio. The vector includes some large integers (numbers are of order 10^40) and it seems that It can not be written properly since when I want to read it I keep getting these errors:\r\n\r\n&quot;ReadList::readn: Invalid real number found when reading from &quot;/Users/Research/RF_improvment/testNTT.txt.&quot; \r\n\r\nand \r\n\r\n&quot;Part::partw: Part 1025 of {$Failed} does not exist.\r\nSet::partw: Part 1025 of {Mod[$Failed + {$Failed}[[1025]], 115792089237316195423570985008687907853269984665640564039457584007913129461761]} does not exist.&quot;\r\n\r\nDoes anyone know how to write large numbers into a file using write function in R? I do not have a problem with calculations and the errors are for reading and writing into a file.\r\n",
            "link": "https://stackoverflow.com/questions/50358702/write-large-numbers-into-a-file-in-r",
            "title": "Write large numbers into a file in R",
            "body": "<p>I want to write a vector into a file and then read it using Rstudio. The vector includes some large integers (numbers are of order 10^40) and it seems that It can not be written properly since when I want to read it I keep getting these errors:</p>\n\n<p>\"ReadList::readn: Invalid real number found when reading from \"/Users/Research/RF_improvment/testNTT.txt.\" </p>\n\n<p>and </p>\n\n<p>\"Part::partw: Part 1025 of {$Failed} does not exist.\nSet::partw: Part 1025 of {Mod[$Failed + {$Failed}[[1025]], 115792089237316195423570985008687907853269984665640564039457584007913129461761]} does not exist.\"</p>\n\n<p>Does anyone know how to write large numbers into a file using write function in R? I do not have a problem with calculations and the errors are for reading and writing into a file.</p>\n"
        }
    ],
    "has_more": true,
    "quota_max": 300,
    "quota_remaining": 295
}