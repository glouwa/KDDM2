{
    "items": [
        {
            "tags": [
                "python",
                "scikit-learn",
                "pip"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9693187,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/6a87ce86781600e57a9d99523b581755?s=128&d=identicon&r=PG&f=1",
                "display_name": "Adunlop",
                "link": "https://stackoverflow.com/users/9693187/adunlop"
            },
            "is_answered": false,
            "view_count": 30,
            "closed_date": 1524690318,
            "answer_count": 1,
            "score": -4,
            "last_activity_date": 1524599432,
            "creation_date": 1524599087,
            "question_id": 50009682,
            "body_markdown": "I am trying to install scikit-learn through pip on Mac OSX. I have updated all of numpy, scipy and pip itself but when I type:\r\n\r\n&gt; &quot;pip install scikit-learn -U&quot; \r\n\r\nI receive an error saying:\r\n\r\n&gt; &quot;Could not find a version that satisfies the requirement scikit-learn (from versions: )\r\nNo matching distribution found for scikit-learn&quot;",
            "link": "https://stackoverflow.com/questions/50009682/pip-install-of-scikit-learn-no-matching-distribiution-found",
            "closed_reason": "duplicate",
            "title": "Pip install of scikit-learn: No matching distribiution found.",
            "body": "<p>I am trying to install scikit-learn through pip on Mac OSX. I have updated all of numpy, scipy and pip itself but when I type:</p>\n\n<blockquote>\n  <p>\"pip install scikit-learn -U\" </p>\n</blockquote>\n\n<p>I receive an error saying:</p>\n\n<blockquote>\n  <p>\"Could not find a version that satisfies the requirement scikit-learn (from versions: )\n  No matching distribution found for scikit-learn\"</p>\n</blockquote>\n"
        },
        {
            "tags": [
                "c++",
                "boost",
                "boost-spirit",
                "boost-spirit-qi"
            ],
            "owner": {
                "reputation": 26723,
                "user_id": 60628,
                "user_type": "registered",
                "accept_rate": 74,
                "profile_image": "https://www.gravatar.com/avatar/837d78446554c800d404141dd0172ccc?s=128&d=identicon&r=PG",
                "display_name": "Frank",
                "link": "https://stackoverflow.com/users/60628/frank"
            },
            "is_answered": true,
            "view_count": 765,
            "accepted_answer_id": 10707422,
            "answer_count": 2,
            "score": 6,
            "last_activity_date": 1524599429,
            "creation_date": 1337706172,
            "question_id": 10706657,
            "body_markdown": "In Boost::Spirit, how can I trigger an `expectation_failure` from a function bound with `Boost::Bind`?\r\n\r\nBackground: I parse a large file that contains complex entries. When an entry is inconsistent with a previous entry I want to fail and throw an `expectation_failure` (containing proper parse position information). When I parse an entry I bind a function that decides if the entry is inconsistent with something seen before.\r\n\r\nI made up a little toy example that shows the point. Here I simply want to throw an `expectation_failure` when the `int` is not divisible by 10: \r\n\r\n    #include &lt;iostream&gt;\r\n    #include &lt;iomanip&gt;\r\n    #include &lt;boost/spirit/include/qi.hpp&gt;\r\n    #include &lt;boost/bind.hpp&gt;\r\n    #include &lt;boost/spirit/include/classic_position_iterator.hpp&gt;\r\n    namespace qi = boost::spirit::qi;\r\n    namespace classic = boost::spirit::classic;\r\n    \r\n    void checkNum(int const&amp; i) {\r\n      if (i % 10 != 0) // &gt;&gt; How to throw proper expectation_failure? &lt;&lt;\r\n        std::cerr &lt;&lt; &quot;ERROR: Number check failed&quot; &lt;&lt; std::endl;\r\n    }\r\n    \r\n    template &lt;typename Iterator, typename Skipper&gt;\r\n    struct MyGrammar : qi::grammar&lt;Iterator, int(), Skipper&gt; {\r\n      MyGrammar() : MyGrammar::base_type(start) {\r\n        start %= qi::eps &gt; qi::int_[boost::bind(&amp;checkNum, _1)];\r\n      }\r\n      qi::rule&lt;Iterator, int(), Skipper&gt; start;\r\n    };\r\n    \r\n    template&lt;class PosIter&gt;\r\n    std::string errorMsg(PosIter const&amp; iter) {\r\n      const classic::file_position_base&lt;std::string&gt;&amp; pos = iter.get_position();\r\n      std::stringstream msg;\r\n      msg &lt;&lt; &quot;parse error at file &quot; &lt;&lt; pos.file\r\n          &lt;&lt; &quot; line &quot; &lt;&lt; pos.line &lt;&lt; &quot; column &quot; &lt;&lt; pos.column &lt;&lt; std::endl\r\n          &lt;&lt; &quot;&#39;&quot; &lt;&lt; iter.get_currentline() &lt;&lt; &quot;&#39;&quot; &lt;&lt; std::endl\r\n          &lt;&lt; std::setw(pos.column) &lt;&lt; &quot; &quot; &lt;&lt; &quot;^- here&quot;;\r\n      return msg.str();\r\n    }\r\n    \r\n    int main() {\r\n      std::string in = &quot;11&quot;;\r\n      typedef std::string::const_iterator Iter;\r\n      typedef classic::position_iterator2&lt;Iter&gt; PosIter;\r\n      MyGrammar&lt;PosIter, qi::space_type&gt; grm;\r\n      int i;\r\n      PosIter it(in.begin(), in.end(), &quot;&lt;string&gt;&quot;);\r\n      PosIter end;\r\n      try {\r\n        qi::phrase_parse(it, end, grm, qi::space, i);\r\n        if (it != end)\r\n          throw std::runtime_error(errorMsg(it));\r\n      } catch(const qi::expectation_failure&lt;PosIter&gt;&amp; e) {\r\n        throw std::runtime_error(errorMsg(e.first));\r\n      }\r\n      return 0;\r\n    }\r\n\r\nThrowing an `expectation_failure` would mean that I get an error message like this on an int that is not divisible by 10:\r\n\r\n    parse error at file &lt;string&gt; line 1 column 2\r\n    &#39;11&#39;\r\n      ^- here\r\n\r\n",
            "link": "https://stackoverflow.com/questions/10706657/how-to-throw-an-expectation-failure-from-a-function-in-boost-spirit",
            "title": "How to throw an expectation_failure from a function in Boost Spirit?",
            "body": "<p>In Boost::Spirit, how can I trigger an <code>expectation_failure</code> from a function bound with <code>Boost::Bind</code>?</p>\n\n<p>Background: I parse a large file that contains complex entries. When an entry is inconsistent with a previous entry I want to fail and throw an <code>expectation_failure</code> (containing proper parse position information). When I parse an entry I bind a function that decides if the entry is inconsistent with something seen before.</p>\n\n<p>I made up a little toy example that shows the point. Here I simply want to throw an <code>expectation_failure</code> when the <code>int</code> is not divisible by 10: </p>\n\n<pre><code>#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;boost/spirit/include/qi.hpp&gt;\n#include &lt;boost/bind.hpp&gt;\n#include &lt;boost/spirit/include/classic_position_iterator.hpp&gt;\nnamespace qi = boost::spirit::qi;\nnamespace classic = boost::spirit::classic;\n\nvoid checkNum(int const&amp; i) {\n  if (i % 10 != 0) // &gt;&gt; How to throw proper expectation_failure? &lt;&lt;\n    std::cerr &lt;&lt; \"ERROR: Number check failed\" &lt;&lt; std::endl;\n}\n\ntemplate &lt;typename Iterator, typename Skipper&gt;\nstruct MyGrammar : qi::grammar&lt;Iterator, int(), Skipper&gt; {\n  MyGrammar() : MyGrammar::base_type(start) {\n    start %= qi::eps &gt; qi::int_[boost::bind(&amp;checkNum, _1)];\n  }\n  qi::rule&lt;Iterator, int(), Skipper&gt; start;\n};\n\ntemplate&lt;class PosIter&gt;\nstd::string errorMsg(PosIter const&amp; iter) {\n  const classic::file_position_base&lt;std::string&gt;&amp; pos = iter.get_position();\n  std::stringstream msg;\n  msg &lt;&lt; \"parse error at file \" &lt;&lt; pos.file\n      &lt;&lt; \" line \" &lt;&lt; pos.line &lt;&lt; \" column \" &lt;&lt; pos.column &lt;&lt; std::endl\n      &lt;&lt; \"'\" &lt;&lt; iter.get_currentline() &lt;&lt; \"'\" &lt;&lt; std::endl\n      &lt;&lt; std::setw(pos.column) &lt;&lt; \" \" &lt;&lt; \"^- here\";\n  return msg.str();\n}\n\nint main() {\n  std::string in = \"11\";\n  typedef std::string::const_iterator Iter;\n  typedef classic::position_iterator2&lt;Iter&gt; PosIter;\n  MyGrammar&lt;PosIter, qi::space_type&gt; grm;\n  int i;\n  PosIter it(in.begin(), in.end(), \"&lt;string&gt;\");\n  PosIter end;\n  try {\n    qi::phrase_parse(it, end, grm, qi::space, i);\n    if (it != end)\n      throw std::runtime_error(errorMsg(it));\n  } catch(const qi::expectation_failure&lt;PosIter&gt;&amp; e) {\n    throw std::runtime_error(errorMsg(e.first));\n  }\n  return 0;\n}\n</code></pre>\n\n<p>Throwing an <code>expectation_failure</code> would mean that I get an error message like this on an int that is not divisible by 10:</p>\n\n<pre><code>parse error at file &lt;string&gt; line 1 column 2\n'11'\n  ^- here\n</code></pre>\n"
        },
        {
            "tags": [
                "python",
                "list",
                "optimization",
                "flatten"
            ],
            "owner": {
                "reputation": 2988,
                "user_id": 215679,
                "user_type": "registered",
                "accept_rate": 97,
                "profile_image": "https://www.gravatar.com/avatar/d43671b8e7d205239683eab7ac23f6d8?s=128&d=identicon&r=PG",
                "display_name": "telliott99",
                "link": "https://stackoverflow.com/users/215679/telliott99"
            },
            "is_answered": true,
            "view_count": 86964,
            "protected_date": 1377946657,
            "accepted_answer_id": 2158532,
            "answer_count": 38,
            "score": 337,
            "last_activity_date": 1524599418,
            "creation_date": 1264716942,
            "last_edit_date": 1524599418,
            "question_id": 2158395,
            "body_markdown": "Yes, I know this subject has been covered before ([here][1], [here][2], [here][3], [here][4]), but as far as I know, all solutions, except for one, fail on a list like this:\r\n\r\n    L = [[[1, 2, 3], [4, 5]], 6]\r\n\r\nWhere the desired output is\r\n\r\n    [1, 2, 3, 4, 5, 6]\r\n\r\nOr perhaps even better, an iterator. The only solution I saw that works for an arbitrary nesting is found [in this question][6]:\r\n\r\n    def flatten(x):\r\n        result = []\r\n        for el in x:\r\n            if hasattr(el, &quot;__iter__&quot;) and not isinstance(el, basestring):\r\n                result.extend(flatten(el))\r\n            else:\r\n                result.append(el)\r\n        return result\r\n    \r\n    flatten(L)\r\n\r\nIs this the best model? Did I overlook something? Any problems?\r\n\r\n  [1]: https://stackoverflow.com/questions/120886\r\n  [2]: https://stackoverflow.com/questions/406121\r\n  [3]: https://stackoverflow.com/questions/457215\r\n  [4]: https://stackoverflow.com/questions/952914\r\n  [5]: https://stackoverflow.com/users/29903/alabaster-codify\r\n  [6]: https://stackoverflow.com/questions/406121",
            "link": "https://stackoverflow.com/questions/2158395/flatten-an-irregular-list-of-lists",
            "title": "Flatten an irregular list of lists",
            "body": "<p>Yes, I know this subject has been covered before (<a href=\"https://stackoverflow.com/questions/120886\">here</a>, <a href=\"https://stackoverflow.com/questions/406121\">here</a>, <a href=\"https://stackoverflow.com/questions/457215\">here</a>, <a href=\"https://stackoverflow.com/questions/952914\">here</a>), but as far as I know, all solutions, except for one, fail on a list like this:</p>\n\n<pre><code>L = [[[1, 2, 3], [4, 5]], 6]\n</code></pre>\n\n<p>Where the desired output is</p>\n\n<pre><code>[1, 2, 3, 4, 5, 6]\n</code></pre>\n\n<p>Or perhaps even better, an iterator. The only solution I saw that works for an arbitrary nesting is found <a href=\"https://stackoverflow.com/questions/406121\">in this question</a>:</p>\n\n<pre><code>def flatten(x):\n    result = []\n    for el in x:\n        if hasattr(el, \"__iter__\") and not isinstance(el, basestring):\n            result.extend(flatten(el))\n        else:\n            result.append(el)\n    return result\n\nflatten(L)\n</code></pre>\n\n<p>Is this the best model? Did I overlook something? Any problems?</p>\n"
        },
        {
            "tags": [
                "javascript",
                "jquery",
                "datatable",
                "datatables",
                "datatables-1.10"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9631803,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
                "display_name": "Amritleen Singh",
                "link": "https://stackoverflow.com/users/9631803/amritleen-singh"
            },
            "is_answered": false,
            "view_count": 12,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524599411,
            "creation_date": 1524594774,
            "last_edit_date": 1524599411,
            "question_id": 50008632,
            "body_markdown": "I am new to datatables. I am using this to display results from ajax call. \r\n\r\n\r\n\r\nOn a single page, I have user form and the result section. User fills the details and submit. On this, datatable is displayed. This works perfectly fine. But when user updates anything on the form and submits it, I am trying to destroy datatable and recreate it. Doing this, i am getting the error - clientWidth of undefined. Below is the code - \r\n\r\n\r\n\r\n\t\r\n\tfunction CBResults(response){\r\n\tvar dataArray = response.data;\r\n\tif ($.fn.DataTable.isDataTable(&quot;#thetable&quot;)) {\r\n\t\t$(&#39;#thetable&#39;).dataTable();\r\n\t\t$(&#39;#thetable&#39;).DataTable().clear().destroy();\r\n\t\t\t\t\r\n\t}\r\n\tvar html = &#39;&lt;tbody&gt;&#39;;\r\n\tfor(var i = 0; i &lt; dataArray.length; i++)\r\n\t\thtml += &#39;&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;&#39;;  // code to populate columns of the table\r\n\t\r\n\thtml += &#39;&lt;/tbody&gt;&#39;;\r\n\t\r\n\t\r\n\t\t$(&#39;#thetable thead&#39;).first().after(html);\r\n\r\n\t\t$(&#39;#thetable&#39;).DataTable( {\r\n\t\t\tretrieve: true,\t\t\t \r\n\t\t\tdom: &#39;Blfrtip&#39;,\r\n\t\t\tscrollY: &quot;300px&quot;,\r\n\t\t\tresponsive: true,\r\n\t\t\tscrollX:        true,\r\n\t\t\tscrollCollapse: true,\r\n\t\t\tcolumnDefs: [ {\r\n\t\t\t\ttargets: [4,5,6,7,8,9,10,11],\r\n\t\t\t\trender: $.fn.dataTable.render.ellipsis(10)\r\n\t\t\t} ], \r\n\t\t\tbuttons: [\r\n\t\t\t\t&#39;colvis&#39;,&#39;copy&#39;, &#39;csv&#39;, &#39;excel&#39;, &#39;pdf&#39;\t\r\n\t\t\t],\r\n\t\t\tfixedColumns:   {\r\n\t\t\t\tleftColumns: 2\r\n\t\t\t},\r\n\t\t\t\r\n\t\t\t &quot;lengthMenu&quot;: [[10, 25, 50, -1], [10, 25, 50, &quot;All&quot;]]\r\n\t\t} );\r\n\t}\r\n\r\nLet me know what I am doing wrong and how can I correct it? Appreciate quick reply on this.\r\n",
            "link": "https://stackoverflow.com/questions/50008632/uncaught-typeerror-cannot-read-property-clientwidth-of-null-jquery-datatables",
            "title": "Uncaught TypeError: Cannot read property &#39;clientWidth&#39; of null jquery.dataTables.min.js:63",
            "body": "<p>I am new to datatables. I am using this to display results from ajax call. </p>\n\n<p>On a single page, I have user form and the result section. User fills the details and submit. On this, datatable is displayed. This works perfectly fine. But when user updates anything on the form and submits it, I am trying to destroy datatable and recreate it. Doing this, i am getting the error - clientWidth of undefined. Below is the code - </p>\n\n<pre><code>function CBResults(response){\nvar dataArray = response.data;\nif ($.fn.DataTable.isDataTable(\"#thetable\")) {\n    $('#thetable').dataTable();\n    $('#thetable').DataTable().clear().destroy();\n\n}\nvar html = '&lt;tbody&gt;';\nfor(var i = 0; i &lt; dataArray.length; i++)\n    html += '&lt;tr&gt;&lt;td&gt;&lt;/td&gt;&lt;/tr&gt;';  // code to populate columns of the table\n\nhtml += '&lt;/tbody&gt;';\n\n\n    $('#thetable thead').first().after(html);\n\n    $('#thetable').DataTable( {\n        retrieve: true,          \n        dom: 'Blfrtip',\n        scrollY: \"300px\",\n        responsive: true,\n        scrollX:        true,\n        scrollCollapse: true,\n        columnDefs: [ {\n            targets: [4,5,6,7,8,9,10,11],\n            render: $.fn.dataTable.render.ellipsis(10)\n        } ], \n        buttons: [\n            'colvis','copy', 'csv', 'excel', 'pdf'  \n        ],\n        fixedColumns:   {\n            leftColumns: 2\n        },\n\n         \"lengthMenu\": [[10, 25, 50, -1], [10, 25, 50, \"All\"]]\n    } );\n}\n</code></pre>\n\n<p>Let me know what I am doing wrong and how can I correct it? Appreciate quick reply on this.</p>\n"
        },
        {
            "tags": [
                "amazon-ec2",
                "ami",
                "ec2-ami"
            ],
            "owner": {
                "reputation": 9570,
                "user_id": 2051454,
                "user_type": "registered",
                "accept_rate": 60,
                "profile_image": "https://i.stack.imgur.com/sZBFW.jpg?s=128&g=1",
                "display_name": "Engineer Dollery",
                "link": "https://stackoverflow.com/users/2051454/engineer-dollery"
            },
            "is_answered": false,
            "view_count": 14,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1524599405,
            "creation_date": 1524436308,
            "last_edit_date": 1524500351,
            "question_id": 49971359,
            "body_markdown": "I&#39;m doing something simple -- installing a docker beta on trusty ubuntu using the amazon ebs builder.\r\n\r\n    ==&gt; Builds finished. The artifacts of successful builds are:\r\n    --&gt; : AMIs were created:\r\n    eu-west-2: ami-xxxxxxxx\r\n\r\nBut, I go look and, to my horror, I have no custom amis at all. I&#39;ve changed region to check if it has turned up somewhere else. It hasn&#39;t. I&#39;ve just confirmed that there&#39;s nothing weird about the aws account this is using either -- it&#39;s my root account.\r\n\r\nI&#39;ve included the packer.json and a minimal provision.sh for your entertainment.\r\n\r\npacker.json\r\n\r\n    {\r\n      &quot;variables&quot;: {\r\n        &quot;aws_access_key&quot;: &quot;{{env `AWS_ACCESS_KEY_ID`}}&quot;,\r\n        &quot;aws_secret_key&quot;: &quot;{{env `AWS_SECRET_ACCESS_KEY`}}&quot;,\r\n        &quot;ami_base_name&quot;: &quot;xxxxxxxxxxxxxxxx&quot;\r\n      },\r\n      &quot;builders&quot;: [\r\n        {\r\n          &quot;name&quot;: &quot;{{user `name`}}&quot;,\r\n          &quot;access_key&quot;: &quot;{{user `aws_access_key`}}&quot;,\r\n          &quot;secret_key&quot;: &quot;{{user `aws_secret_key`}}&quot;,\r\n          &quot;region&quot;: &quot;eu-west-2&quot;,\r\n          &quot;source_ami_filter&quot;: {\r\n            &quot;filters&quot;: {\r\n              &quot;virtualization-type&quot;: &quot;hvm&quot;,\r\n              &quot;name&quot;: &quot;ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*&quot;,\r\n              &quot;root-device-type&quot;: &quot;ebs&quot;\r\n            },\r\n            &quot;owners&quot;: [&quot;099720109477&quot;],\r\n            &quot;most_recent&quot;: true\r\n          },\r\n          &quot;instance_type&quot;: &quot;t2.micro&quot;,\r\n          &quot;ssh_username&quot;: &quot;ubuntu&quot;,\r\n          &quot;ssh_timeout&quot;: &quot;10m&quot;,\r\n          &quot;ami_name&quot;: &quot;{{user `ami_base_name`}}_{{timestamp}}&quot;,\r\n          &quot;ami_description&quot;: &quot;{{user `ami_base_name`}} AMI&quot;,\r\n          &quot;run_tags&quot;: {\r\n            &quot;ami-create&quot;: &quot;{{user `ami_base_name`}}&quot;\r\n          },\r\n          &quot;tags&quot;: {\r\n            &quot;ami&quot;: &quot;{{user `ami_base_name`}}&quot;,\r\n    \t&quot;owner&quot;: &quot;xxxxxxxxxxxxxxx&quot;\r\n          },\r\n          &quot;associate_public_ip_address&quot;: true,\r\n          &quot;type&quot;: &quot;amazon-ebs&quot;\r\n        }\r\n      ],\r\n      &quot;provisioners&quot;: [\r\n        {\r\n          &quot;type&quot;: &quot;file&quot;,\r\n          &quot;source&quot;: &quot;provision.sh&quot;,\r\n          &quot;destination&quot;: &quot;/tmp/provision.sh&quot;\r\n        },\r\n        {\r\n          &quot;type&quot;: &quot;shell&quot;,\r\n          &quot;execute_command&quot;: &quot;echo &#39;packer&#39; | sudo -S sh -c &#39;{{ .Vars }} {{ .Path }}&#39;&quot;,\r\n          &quot;inline&quot;: [\r\n            &quot;cd /tmp&quot;,\r\n            &quot;chmod +x provision.sh&quot;,\r\n            &quot;./provision.sh&quot;\r\n          ]\r\n        }\r\n      ]\r\n    }\r\n\r\nprovision.sh\r\n\r\n    #!/bin/sh\r\n    echo &quot;provisioning...&quot;\r\n\r\n\r\nAnd here&#39;s most of the output of the command, with boring bits removed:\r\n\r\n\r\n    ==&gt; : Prevalidating AMI Name: xxxxxxxxxxxxxxx\r\n        : Found Image ID: ami-506e8f37\r\n    ==&gt; : Creating temporary keypair: packer_xxxxxxx\r\n    ==&gt; : Creating temporary security group for this instance: packer_xxxxx\r\n    ==&gt; : Authorizing access to port 22 from 0.0.0.0/0 in the temporary security group...\r\n    ==&gt; : Launching a source AWS instance...\r\n    ==&gt; : Adding tags to source instance\r\n        : Adding tag: &quot;ami-create&quot;: &quot;xxxxxxxxxxxxxxxxx&quot;\r\n        : Adding tag: &quot;Name&quot;: &quot;Packer Builder&quot;\r\n        : Instance ID: i-09ef0118a46e2ceb4\r\n    ==&gt; : Waiting for instance (i-09ef0118a46e2ceb4) to become ready...\r\n    ==&gt; : Waiting for SSH to become available...\r\n    ==&gt; : Connected to SSH!\r\n    ==&gt; : Uploading provision.sh =&gt; /tmp/provision.sh\r\n    ==&gt; : Provisioning with shell script: /tmp/packer-shell114009819\r\n        : root\r\n       ...\r\n        : Provisioner running... done.\r\n    ==&gt; : Stopping the source instance...\r\n        : Stopping instance, attempt 1\r\n    ==&gt; : Waiting for the instance to stop...\r\n    ==&gt; : Creating the AMI: xxxxxxxxxxxxxx\r\n        : AMI: ami-a78e6dc0\r\n    ==&gt; : Waiting for AMI to become ready...\r\n    ==&gt; : Modifying attributes on AMI (ami-a78e6dc0)...\r\n        : Modifying: description\r\n    ==&gt; : Modifying attributes on snapshot (snap-00e365c51c5960394)...\r\n    ==&gt; : Adding tags to AMI (ami-a78e6dc0)...\r\n    ==&gt; : Tagging snapshot: snap-00e365c51c5960394\r\n    ==&gt; : Creating AMI tags\r\n        : Adding tag: &quot;ami&quot;: &quot;xxxxxxxxxxxxxxxxx&quot;\r\n        : Adding tag: &quot;owner&quot;: &quot;xxxxxxxxxxxxxxxxxxxxxxx&quot;\r\n    ==&gt; : Creating snapshot tags\r\n    ==&gt; : Terminating the source AWS instance...\r\n    ==&gt; : Cleaning up any extra volumes...\r\n    ==&gt; : No volumes to clean up, skipping\r\n    ==&gt; : Deleting temporary security group...\r\n    ==&gt; : Deleting temporary keypair...\r\n    Build &#39;&#39; finished.\r\n    \r\n    ==&gt; Builds finished. The artifacts of successful builds are:\r\n    --&gt; : AMIs were created:\r\n    eu-west-2: ami-a78e6dc0\r\n\r\n\r\nWhat am I missing (apart from the AMI ;)\r\n",
            "link": "https://stackoverflow.com/questions/49971359/packer-finishes-successfully-but-ami-is-missing-in-aws",
            "title": "Packer finishes successfully, but AMI is missing in AWS",
            "body": "<p>I'm doing something simple -- installing a docker beta on trusty ubuntu using the amazon ebs builder.</p>\n\n<pre><code>==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; : AMIs were created:\neu-west-2: ami-xxxxxxxx\n</code></pre>\n\n<p>But, I go look and, to my horror, I have no custom amis at all. I've changed region to check if it has turned up somewhere else. It hasn't. I've just confirmed that there's nothing weird about the aws account this is using either -- it's my root account.</p>\n\n<p>I've included the packer.json and a minimal provision.sh for your entertainment.</p>\n\n<p>packer.json</p>\n\n<pre><code>{\n  \"variables\": {\n    \"aws_access_key\": \"{{env `AWS_ACCESS_KEY_ID`}}\",\n    \"aws_secret_key\": \"{{env `AWS_SECRET_ACCESS_KEY`}}\",\n    \"ami_base_name\": \"xxxxxxxxxxxxxxxx\"\n  },\n  \"builders\": [\n    {\n      \"name\": \"{{user `name`}}\",\n      \"access_key\": \"{{user `aws_access_key`}}\",\n      \"secret_key\": \"{{user `aws_secret_key`}}\",\n      \"region\": \"eu-west-2\",\n      \"source_ami_filter\": {\n        \"filters\": {\n          \"virtualization-type\": \"hvm\",\n          \"name\": \"ubuntu/images/*ubuntu-xenial-16.04-amd64-server-*\",\n          \"root-device-type\": \"ebs\"\n        },\n        \"owners\": [\"099720109477\"],\n        \"most_recent\": true\n      },\n      \"instance_type\": \"t2.micro\",\n      \"ssh_username\": \"ubuntu\",\n      \"ssh_timeout\": \"10m\",\n      \"ami_name\": \"{{user `ami_base_name`}}_{{timestamp}}\",\n      \"ami_description\": \"{{user `ami_base_name`}} AMI\",\n      \"run_tags\": {\n        \"ami-create\": \"{{user `ami_base_name`}}\"\n      },\n      \"tags\": {\n        \"ami\": \"{{user `ami_base_name`}}\",\n    \"owner\": \"xxxxxxxxxxxxxxx\"\n      },\n      \"associate_public_ip_address\": true,\n      \"type\": \"amazon-ebs\"\n    }\n  ],\n  \"provisioners\": [\n    {\n      \"type\": \"file\",\n      \"source\": \"provision.sh\",\n      \"destination\": \"/tmp/provision.sh\"\n    },\n    {\n      \"type\": \"shell\",\n      \"execute_command\": \"echo 'packer' | sudo -S sh -c '{{ .Vars }} {{ .Path }}'\",\n      \"inline\": [\n        \"cd /tmp\",\n        \"chmod +x provision.sh\",\n        \"./provision.sh\"\n      ]\n    }\n  ]\n}\n</code></pre>\n\n<p>provision.sh</p>\n\n<pre><code>#!/bin/sh\necho \"provisioning...\"\n</code></pre>\n\n<p>And here's most of the output of the command, with boring bits removed:</p>\n\n<pre><code>==&gt; : Prevalidating AMI Name: xxxxxxxxxxxxxxx\n    : Found Image ID: ami-506e8f37\n==&gt; : Creating temporary keypair: packer_xxxxxxx\n==&gt; : Creating temporary security group for this instance: packer_xxxxx\n==&gt; : Authorizing access to port 22 from 0.0.0.0/0 in the temporary security group...\n==&gt; : Launching a source AWS instance...\n==&gt; : Adding tags to source instance\n    : Adding tag: \"ami-create\": \"xxxxxxxxxxxxxxxxx\"\n    : Adding tag: \"Name\": \"Packer Builder\"\n    : Instance ID: i-09ef0118a46e2ceb4\n==&gt; : Waiting for instance (i-09ef0118a46e2ceb4) to become ready...\n==&gt; : Waiting for SSH to become available...\n==&gt; : Connected to SSH!\n==&gt; : Uploading provision.sh =&gt; /tmp/provision.sh\n==&gt; : Provisioning with shell script: /tmp/packer-shell114009819\n    : root\n   ...\n    : Provisioner running... done.\n==&gt; : Stopping the source instance...\n    : Stopping instance, attempt 1\n==&gt; : Waiting for the instance to stop...\n==&gt; : Creating the AMI: xxxxxxxxxxxxxx\n    : AMI: ami-a78e6dc0\n==&gt; : Waiting for AMI to become ready...\n==&gt; : Modifying attributes on AMI (ami-a78e6dc0)...\n    : Modifying: description\n==&gt; : Modifying attributes on snapshot (snap-00e365c51c5960394)...\n==&gt; : Adding tags to AMI (ami-a78e6dc0)...\n==&gt; : Tagging snapshot: snap-00e365c51c5960394\n==&gt; : Creating AMI tags\n    : Adding tag: \"ami\": \"xxxxxxxxxxxxxxxxx\"\n    : Adding tag: \"owner\": \"xxxxxxxxxxxxxxxxxxxxxxx\"\n==&gt; : Creating snapshot tags\n==&gt; : Terminating the source AWS instance...\n==&gt; : Cleaning up any extra volumes...\n==&gt; : No volumes to clean up, skipping\n==&gt; : Deleting temporary security group...\n==&gt; : Deleting temporary keypair...\nBuild '' finished.\n\n==&gt; Builds finished. The artifacts of successful builds are:\n--&gt; : AMIs were created:\neu-west-2: ami-a78e6dc0\n</code></pre>\n\n<p>What am I missing (apart from the AMI ;)</p>\n"
        },
        {
            "tags": [
                "java",
                "arrays",
                "performance",
                "for-loop",
                "compare"
            ],
            "owner": {
                "reputation": 544,
                "user_id": 6138345,
                "user_type": "registered",
                "profile_image": "https://i.stack.imgur.com/zhoys.jpg?s=128&g=1",
                "display_name": "nima_moradi",
                "link": "https://stackoverflow.com/users/6138345/nima-moradi"
            },
            "is_answered": false,
            "view_count": 51,
            "answer_count": 2,
            "score": 1,
            "last_activity_date": 1524599403,
            "creation_date": 1523563825,
            "last_edit_date": 1524599403,
            "question_id": 49805098,
            "body_markdown": "i working in school Al project it all work well but little slow.\r\n&lt;br&gt; in my project i have overridden equals method.\r\n\r\n    @Override\r\n        public boolean equals(Object obj) {\r\n            if (obj == null) {\r\n                return false;\r\n            } else if (obj instanceof map) {\r\n                map m = (map) obj;\r\n                for (int i = 0; i &lt; mapSize; i++) {\r\n                    for (int j = 0; j &lt; mapSize; j++) {\r\n                        if (m.board[i][j] != board[i][j])\r\n                            return false;\r\n                    }\r\n                }\r\n            }\r\n            return true;\r\n        } \r\n\r\ni figure out if i write for loops in a decussate way to check first half of array 80% of times get pass in first half and 20% remaining i will check afterward . so i assumed it will work 50 % faster \r\n \r\n\r\n     for (int i = 0; i &lt; mapSize; i++) {\r\n                    for (int j = i%2 ; j &lt; mapSize; j+=2) {\r\n                        if (m.board[i][j] != board[i][j])\r\n                            return false;\r\n                    }\r\n                }\r\n\r\n    for (int i = 0; i &lt; mapSize; i++) {\r\n                        for (int j = (i+1)%2 ; j &lt; mapSize; j+=2) {\r\n                            if (m.board[i][j] != board[i][j])\r\n                                return false;\r\n                        }\r\n                    }\r\n\r\nafter dividing the for loop and couple of test it seemed it getting slower i don&#39;t why only. why it take too mush time and how to make it faster &lt;br&gt;\r\nthanks in advance \r\n",
            "link": "https://stackoverflow.com/questions/49805098/make-object-compare-faster",
            "title": "make object compare faster",
            "body": "<p>i working in school Al project it all work well but little slow.\n<br> in my project i have overridden equals method.</p>\n\n<pre><code>@Override\n    public boolean equals(Object obj) {\n        if (obj == null) {\n            return false;\n        } else if (obj instanceof map) {\n            map m = (map) obj;\n            for (int i = 0; i &lt; mapSize; i++) {\n                for (int j = 0; j &lt; mapSize; j++) {\n                    if (m.board[i][j] != board[i][j])\n                        return false;\n                }\n            }\n        }\n        return true;\n    } \n</code></pre>\n\n<p>i figure out if i write for loops in a decussate way to check first half of array 80% of times get pass in first half and 20% remaining i will check afterward . so i assumed it will work 50 % faster </p>\n\n<pre><code> for (int i = 0; i &lt; mapSize; i++) {\n                for (int j = i%2 ; j &lt; mapSize; j+=2) {\n                    if (m.board[i][j] != board[i][j])\n                        return false;\n                }\n            }\n\nfor (int i = 0; i &lt; mapSize; i++) {\n                    for (int j = (i+1)%2 ; j &lt; mapSize; j+=2) {\n                        if (m.board[i][j] != board[i][j])\n                            return false;\n                    }\n                }\n</code></pre>\n\n<p>after dividing the for loop and couple of test it seemed it getting slower i don't why only. why it take too mush time and how to make it faster <br>\nthanks in advance </p>\n"
        },
        {
            "tags": [
                "python",
                "unit-testing",
                "amazon-web-services",
                "aws-lambda"
            ],
            "owner": {
                "reputation": 1112,
                "user_id": 996719,
                "user_type": "registered",
                "accept_rate": 49,
                "profile_image": "https://www.gravatar.com/avatar/ac4a6c04cc463e0a52c5e43382ecbdfd?s=128&d=identicon&r=PG",
                "display_name": "Titus Pullo",
                "link": "https://stackoverflow.com/users/996719/titus-pullo"
            },
            "is_answered": true,
            "view_count": 528,
            "answer_count": 4,
            "score": 2,
            "last_activity_date": 1524599400,
            "creation_date": 1516124642,
            "question_id": 48287018,
            "body_markdown": "I have a python script that I run on AWS. The script contains the `lambda_handler(event, context)` function that is called by AWS. Now, I&#39;d like to create a new lambda function that acts as unit test.\r\nA typical unit test schema is defined as:\r\n\r\n    import unittest\r\n\r\n    def my_function(a):\r\n        return a + 1\r\n\r\n    class Test(unittest.TestCase):\r\n\r\n        def test_correct(self):\r\n            self.assertEqual( my_function(1), 2)\r\n\r\n    if __name__ == &#39;__main__&#39;:\r\n        unittest.main()\r\n\r\nIn AWS, the `lambda_handler(event, context)` function is called. How can I make the `unittest_lambda_handler(event, context)` to perform the unit test?\r\n\r\nSo I am guessing my code (in the unit test script) should look like:\r\n\r\nimport main_lambda_function\r\nimport unittest\r\n\r\n    def unittest_lambda_handler(event, context):\r\n\t     #what should this function do?\r\n\r\n    class MyTest(unittest.TestCase):\r\n\t     def return_type(self,event, context):\r\n            self.assertTrue(isinstance(main_lambda_function.lambda_handler(event, context),int))\r\n  \r\nIs this the correct approach?If so, what should `unittest_lambda_handler` do?\r\n",
            "link": "https://stackoverflow.com/questions/48287018/unittest-on-aws-lambda",
            "title": "Unittest on AWS Lambda",
            "body": "<p>I have a python script that I run on AWS. The script contains the <code>lambda_handler(event, context)</code> function that is called by AWS. Now, I'd like to create a new lambda function that acts as unit test.\nA typical unit test schema is defined as:</p>\n\n<pre><code>import unittest\n\ndef my_function(a):\n    return a + 1\n\nclass Test(unittest.TestCase):\n\n    def test_correct(self):\n        self.assertEqual( my_function(1), 2)\n\nif __name__ == '__main__':\n    unittest.main()\n</code></pre>\n\n<p>In AWS, the <code>lambda_handler(event, context)</code> function is called. How can I make the <code>unittest_lambda_handler(event, context)</code> to perform the unit test?</p>\n\n<p>So I am guessing my code (in the unit test script) should look like:</p>\n\n<p>import main_lambda_function\nimport unittest</p>\n\n<pre><code>def unittest_lambda_handler(event, context):\n     #what should this function do?\n\nclass MyTest(unittest.TestCase):\n     def return_type(self,event, context):\n        self.assertTrue(isinstance(main_lambda_function.lambda_handler(event, context),int))\n</code></pre>\n\n<p>Is this the correct approach?If so, what should <code>unittest_lambda_handler</code> do?</p>\n"
        },
        {
            "tags": [
                "ms-access",
                "office365"
            ],
            "owner": {
                "reputation": 92,
                "user_id": 3216034,
                "user_type": "registered",
                "accept_rate": 26,
                "profile_image": "https://www.gravatar.com/avatar/eb061981ca8df90d46d66da96a0b0577?s=128&d=identicon&r=PG&f=1",
                "display_name": "jpl458",
                "link": "https://stackoverflow.com/users/3216034/jpl458"
            },
            "is_answered": false,
            "view_count": 20,
            "answer_count": 2,
            "score": 0,
            "last_activity_date": 1524599396,
            "creation_date": 1524496018,
            "last_edit_date": 1524540377,
            "question_id": 49984315,
            "body_markdown": "Creating a new ACCESS application. When I first open ACCESS and create new database it works fine, but is saved on the PC as Database71.  If I save it as with a new name to the desk top, or try to rename it it won&#39;t open with the following message:  WARNING: It is impossible to determine that this content came from a reliable source.  I have swept the web and tried several of the solutions,  involving the ACCESS Options and the trust center. but none of them work. \r\nUsing OFFICE 365 and Windows 10.\r\n\r\njpl458",
            "link": "https://stackoverflow.com/questions/49984315/cant-rename-access-database-or-move-it",
            "title": "CAn&#39;t rename ACCESS database or move it",
            "body": "<p>Creating a new ACCESS application. When I first open ACCESS and create new database it works fine, but is saved on the PC as Database71.  If I save it as with a new name to the desk top, or try to rename it it won't open with the following message:  WARNING: It is impossible to determine that this content came from a reliable source.  I have swept the web and tried several of the solutions,  involving the ACCESS Options and the trust center. but none of them work. \nUsing OFFICE 365 and Windows 10.</p>\n\n<p>jpl458</p>\n"
        },
        {
            "tags": [
                "linux",
                "bash",
                "wget",
                "brace-expansion"
            ],
            "owner": {
                "user_type": "does_not_exist",
                "display_name": "user9406898"
            },
            "is_answered": true,
            "view_count": 27,
            "accepted_answer_id": 50008876,
            "answer_count": 3,
            "score": 1,
            "last_activity_date": 1524599396,
            "creation_date": 1524594388,
            "question_id": 50008543,
            "body_markdown": "I am currently trying to scrape images of a website with a little script I&#39;ve made :\r\n\r\n    for url in $my_url/{1..100}&#39;.png&#39; \r\n    do\r\n        wget &quot;$url&quot; || break\r\n    done\r\n\r\nThe fast is that sometimes, images are named 1.png or 01.png or 001.png\r\n\r\nSo I would like to try the download of the images with each name to not miss any pictures.\r\n\r\nSomething like :\r\n\r\n    for url in $my_url/{1..100}{01..100}{001..100}&#39;.png&#39; \r\n\r\nThanks for the help !",
            "link": "https://stackoverflow.com/questions/50008543/multiple-brace-expansion-in-wget-url",
            "title": "Multiple brace expansion in wget URL",
            "body": "<p>I am currently trying to scrape images of a website with a little script I've made :</p>\n\n<pre><code>for url in $my_url/{1..100}'.png' \ndo\n    wget \"$url\" || break\ndone\n</code></pre>\n\n<p>The fast is that sometimes, images are named 1.png or 01.png or 001.png</p>\n\n<p>So I would like to try the download of the images with each name to not miss any pictures.</p>\n\n<p>Something like :</p>\n\n<pre><code>for url in $my_url/{1..100}{01..100}{001..100}'.png' \n</code></pre>\n\n<p>Thanks for the help !</p>\n"
        },
        {
            "tags": [
                "arrays",
                "mongodb",
                "mongodb-query"
            ],
            "owner": {
                "reputation": 9006,
                "user_id": 52954,
                "user_type": "registered",
                "accept_rate": 78,
                "profile_image": "https://www.gravatar.com/avatar/49313a97f8c3f50e05b6c7ce6d3e8ecd?s=128&d=identicon&r=PG",
                "display_name": "LiorH",
                "link": "https://stackoverflow.com/users/52954/liorh"
            },
            "is_answered": true,
            "view_count": 65802,
            "accepted_answer_id": 4669702,
            "answer_count": 9,
            "score": 133,
            "last_activity_date": 1524599396,
            "creation_date": 1294838019,
            "last_edit_date": 1524461918,
            "question_id": 4669178,
            "body_markdown": "I have a Mongo document which holds an array of elements.\r\n\r\nI&#39;d like to reset the `.handled` attribute of all objects in the array where `.profile` = XX.\r\n\r\nThe document is in the following form: \r\n\r\n    {\r\n        &quot;_id&quot;: ObjectId(&quot;4d2d8deff4e6c1d71fc29a07&quot;),\r\n        &quot;user_id&quot;: &quot;714638ba-2e08-2168-2b99-00002f3d43c0&quot;,\r\n        &quot;events&quot;: [{\r\n                &quot;handled&quot;: 1,\r\n                &quot;profile&quot;: 10,\r\n                &quot;data&quot;: &quot;.....&quot;\r\n            } {\r\n                &quot;handled&quot;: 1,\r\n                &quot;profile&quot;: 10,\r\n                &quot;data&quot;: &quot;.....&quot;\r\n            } {\r\n                &quot;handled&quot;: 1,\r\n                &quot;profile&quot;: 20,\r\n                &quot;data&quot;: &quot;.....&quot;\r\n            }\r\n            ...\r\n        ]\r\n    }\r\n\r\nso, I tried the following:\r\n\r\n    .update({&quot;events.profile&quot;:10},{$set:{&quot;events.$.handled&quot;:0}},false,true)\r\n\r\nHowever it updates only the **first** matched array element in each document. (That&#39;s the defined behaviour for [$ - the positional operator][1].)\r\n\r\nHow can I update **all** matched array elements?\r\n\r\n\r\n  [1]: http://www.mongodb.org/display/DOCS/Updating#Updating-The%24positionaloperator",
            "link": "https://stackoverflow.com/questions/4669178/how-to-update-multiple-array-elements-in-mongodb",
            "title": "How to Update Multiple Array Elements in mongodb",
            "body": "<p>I have a Mongo document which holds an array of elements.</p>\n\n<p>I'd like to reset the <code>.handled</code> attribute of all objects in the array where <code>.profile</code> = XX.</p>\n\n<p>The document is in the following form: </p>\n\n<pre><code>{\n    \"_id\": ObjectId(\"4d2d8deff4e6c1d71fc29a07\"),\n    \"user_id\": \"714638ba-2e08-2168-2b99-00002f3d43c0\",\n    \"events\": [{\n            \"handled\": 1,\n            \"profile\": 10,\n            \"data\": \".....\"\n        } {\n            \"handled\": 1,\n            \"profile\": 10,\n            \"data\": \".....\"\n        } {\n            \"handled\": 1,\n            \"profile\": 20,\n            \"data\": \".....\"\n        }\n        ...\n    ]\n}\n</code></pre>\n\n<p>so, I tried the following:</p>\n\n<pre><code>.update({\"events.profile\":10},{$set:{\"events.$.handled\":0}},false,true)\n</code></pre>\n\n<p>However it updates only the <strong>first</strong> matched array element in each document. (That's the defined behaviour for <a href=\"http://www.mongodb.org/display/DOCS/Updating#Updating-The%24positionaloperator\" rel=\"nofollow noreferrer\">$ - the positional operator</a>.)</p>\n\n<p>How can I update <strong>all</strong> matched array elements?</p>\n"
        },
        {
            "tags": [
                "c++",
                "sfml"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9687656,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/b5b8f235833288d676407b6bcb5d1f8d?s=128&d=identicon&r=PG&f=1",
                "display_name": "B4039",
                "link": "https://stackoverflow.com/users/9687656/b4039"
            },
            "is_answered": false,
            "view_count": 20,
            "answer_count": 0,
            "score": -3,
            "last_activity_date": 1524599396,
            "creation_date": 1524599396,
            "question_id": 50009744,
            "body_markdown": "I&#39;ve implemented a basic Keyboard Input manager:\r\n\r\n    #include &quot;UserIO.h&quot;\r\n    \r\n    \r\n    UserIO::UserIO(GameManager* game, Physics* physics)\r\n    {\r\n    \tthis-&gt;game = game;\r\n    \tthis-&gt;physics = physics;\r\n    }\r\n    \r\n    UserIO::~UserIO()\r\n    {\r\n    }\r\n    \r\n    void UserIO::checkInput()\r\n    {\r\n    \tif (sf::Keyboard::isKeyPressed(sf::Keyboard::Left))\r\n    \t{\r\n    \t\tphysics-&gt;moveLeft();\r\n    \t}\r\n    \r\n    \tif (sf::Keyboard::isKeyPressed(sf::Keyboard::Right))\r\n    \t{\r\n    \t\tphysics-&gt;moveRight();\r\n    \r\n    \t}\r\n    \r\n    \tif (sf::Keyboard::isKeyPressed(sf::Keyboard::Up))\r\n    \t{\r\n    \t\tphysics-&gt;moveUp();\r\n    \t}\r\n    \r\n    \tif (sf::Keyboard::isKeyPressed(sf::Keyboard::Down))\r\n    \t{\r\n    \t\tphysics-&gt;moveDown();\r\n    \t}\r\n    \r\n    }\r\n\r\nThe physics manager increments the player&#39;s velocity as follows:\r\n\r\n    void Physics::moveRight()\r\n    {\r\n    \t//ball-&gt;setVelX(ball-&gt;getVelx()+0.005f);\r\n    \tball-&gt;setX(ball-&gt;getVelx() + 0.001);\r\n    \r\n    }\r\n    \r\n    void Physics::moveUp()\r\n    {\r\n    \tball-&gt;setVelY(ball-&gt;getVely() + 0.001f);\r\n    }\r\n    \r\n    void Physics::moveLeft()\r\n    {\r\n    \tball-&gt;setVelX(ball-&gt;getVelx() - 0.001f);\r\n    }\r\n    \r\n    void Physics::moveDown()\r\n    {\r\n    \tball-&gt;setVelY(ball-&gt;getVely() - 0.001f);\r\n    }\r\n    \r\n    void Physics::updatePlayerPos()\r\n    {\r\n    \t//dampen velocity\r\n    \tball-&gt;setVelX(ball-&gt;getVelx() * dampen);\r\n    \tball-&gt;setVelY(ball-&gt;getVely() * dampen);\r\n    \r\n    \tball-&gt;setX(ball-&gt;getX() + ball-&gt;getVelx());\r\n    \tball-&gt;setY(ball-&gt;getY() + ball-&gt;getVely());\r\n    }\r\n\r\nAnd my main loop:\r\n\r\n    while (window-&gt;isOpen())\r\n    \t{\r\n    \t\t\r\n    \t\t\r\n    \t\twindow-&gt;clear();\r\n    \t\tuser-&gt;checkInput();\r\n    \t\tphysics-&gt;tick();\r\n    \t\trender-&gt;draw();\r\n    \t\r\n    \t}\r\n\r\nThe idea is for the player object (circle) to accelerate as one of the keys is pressed. The &quot;tick&quot; method calls &quot;updatePlayerPos&quot; to calculate displacement.\r\n\r\nA &quot;dampen&quot; variable ensures the ball decelerates while no keys are pressed.\r\n\r\nWhen running the program, SFML seems somewhat receptive to keyboard strokes, however the motion of the ball is completely erratic: while a key is pressed, the ball will remain stationary for a while, and then zoom off. I&#39;ve tried a different combination of velocity values and nothing improves. I also tried removing the acceleration aspect altogether, and having the ball&#39;s displacement proportional to the length of time the key is pressed (e.g. if &quot;right key&quot; is pressed, add 0.1 to x). Sadly no change.\r\n\r\nI&#39;m now starting to think the issue lies with the game loop and/or SFML, as opposed to the physics. Is it possible that keystrokes are being missed? Would a fixed timestep help the situation?\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50009744/sfml-keyboard-input",
            "title": "SFML Keyboard Input",
            "body": "<p>I've implemented a basic Keyboard Input manager:</p>\n\n<pre><code>#include \"UserIO.h\"\n\n\nUserIO::UserIO(GameManager* game, Physics* physics)\n{\n    this-&gt;game = game;\n    this-&gt;physics = physics;\n}\n\nUserIO::~UserIO()\n{\n}\n\nvoid UserIO::checkInput()\n{\n    if (sf::Keyboard::isKeyPressed(sf::Keyboard::Left))\n    {\n        physics-&gt;moveLeft();\n    }\n\n    if (sf::Keyboard::isKeyPressed(sf::Keyboard::Right))\n    {\n        physics-&gt;moveRight();\n\n    }\n\n    if (sf::Keyboard::isKeyPressed(sf::Keyboard::Up))\n    {\n        physics-&gt;moveUp();\n    }\n\n    if (sf::Keyboard::isKeyPressed(sf::Keyboard::Down))\n    {\n        physics-&gt;moveDown();\n    }\n\n}\n</code></pre>\n\n<p>The physics manager increments the player's velocity as follows:</p>\n\n<pre><code>void Physics::moveRight()\n{\n    //ball-&gt;setVelX(ball-&gt;getVelx()+0.005f);\n    ball-&gt;setX(ball-&gt;getVelx() + 0.001);\n\n}\n\nvoid Physics::moveUp()\n{\n    ball-&gt;setVelY(ball-&gt;getVely() + 0.001f);\n}\n\nvoid Physics::moveLeft()\n{\n    ball-&gt;setVelX(ball-&gt;getVelx() - 0.001f);\n}\n\nvoid Physics::moveDown()\n{\n    ball-&gt;setVelY(ball-&gt;getVely() - 0.001f);\n}\n\nvoid Physics::updatePlayerPos()\n{\n    //dampen velocity\n    ball-&gt;setVelX(ball-&gt;getVelx() * dampen);\n    ball-&gt;setVelY(ball-&gt;getVely() * dampen);\n\n    ball-&gt;setX(ball-&gt;getX() + ball-&gt;getVelx());\n    ball-&gt;setY(ball-&gt;getY() + ball-&gt;getVely());\n}\n</code></pre>\n\n<p>And my main loop:</p>\n\n<pre><code>while (window-&gt;isOpen())\n    {\n\n\n        window-&gt;clear();\n        user-&gt;checkInput();\n        physics-&gt;tick();\n        render-&gt;draw();\n\n    }\n</code></pre>\n\n<p>The idea is for the player object (circle) to accelerate as one of the keys is pressed. The \"tick\" method calls \"updatePlayerPos\" to calculate displacement.</p>\n\n<p>A \"dampen\" variable ensures the ball decelerates while no keys are pressed.</p>\n\n<p>When running the program, SFML seems somewhat receptive to keyboard strokes, however the motion of the ball is completely erratic: while a key is pressed, the ball will remain stationary for a while, and then zoom off. I've tried a different combination of velocity values and nothing improves. I also tried removing the acceleration aspect altogether, and having the ball's displacement proportional to the length of time the key is pressed (e.g. if \"right key\" is pressed, add 0.1 to x). Sadly no change.</p>\n\n<p>I'm now starting to think the issue lies with the game loop and/or SFML, as opposed to the physics. Is it possible that keystrokes are being missed? Would a fixed timestep help the situation?</p>\n"
        },
        {
            "tags": [
                "sql",
                "sql-server",
                "tsql",
                "ado.net",
                "concurrency"
            ],
            "owner": {
                "reputation": 320159,
                "user_id": 27535,
                "user_type": "registered",
                "accept_rate": 89,
                "profile_image": "https://www.gravatar.com/avatar/4190f05bc2af559580aa220d0139e9f8?s=128&d=identicon&r=PG",
                "display_name": "gbn",
                "link": "https://stackoverflow.com/users/27535/gbn"
            },
            "is_answered": true,
            "view_count": 165511,
            "accepted_answer_id": 1547539,
            "answer_count": 12,
            "community_owned_date": 1370592427,
            "score": 245,
            "last_activity_date": 1524599380,
            "creation_date": 1254063149,
            "last_edit_date": 1495540046,
            "question_id": 1483732,
            "body_markdown": "Inspired by [this question][1] where there are differing views on SET NOCOUNT...\r\n\r\n&gt; Should we use SET NOCOUNT ON for SQL Server? If not, why not?\r\n\r\n**What it does** Edit 6, on 22 Jul 2011\r\n\r\nIt suppresses the &quot;xx rows affected&quot; message after any DML. This is a resultset and when sent, the client must process it. It&#39;s tiny, but measurable (see answers below)\r\n\r\nFor triggers etc, the client will receive multiple &quot;xx rows affected&quot; and this causes all manner of errors for some ORMs, MS Access, JPA etc (see edits below)\r\n\r\n\r\n**Background:**\r\n\r\nGeneral accepted best practice (I thought until this question) is to use `SET NOCOUNT ON` in triggers and stored procedures in SQL Server. We use it everywhere and a quick google shows plenty of SQL Server MVPs agreeing too.\r\n\r\nMSDN says this can break a [.net SQLDataAdapter][2].\r\n\r\nNow, this means to me that the SQLDataAdapter is limited to utterly simply CRUD processing because it expects the &quot;n rows affected&quot; message to match. So, I can&#39;t use:\r\n\r\n- IF EXISTS to avoid duplicates (no rows affected message) *Note: use with caution*\r\n- WHERE NOT EXISTS (less rows then expected\r\n- Filter out trivial updates (eg no data actually changes)\r\n- Do any table access before (such as logging)\r\n- Hide complexity or denormlisation\r\n- etc\r\n\r\nIn the question marc_s (who knows his SQL stuff) says do not use it. This differs to what I think (and I regard myself as somewhat competent at SQL too).\r\n\r\nIt&#39;s possible I&#39;m missing something (feel free to point out the obvious), but what do you folks out there think?\r\n\r\nNote: it&#39;s been years since I saw this error because I don&#39;t use SQLDataAdapter nowadays.\r\n\r\n**Edits after comments and questions:**\r\n\r\nEdit: More thoughts...\r\n\r\nWe have multiple clients: one may use a C# SQLDataAdaptor, another may use nHibernate from Java. These can be affected in different ways with `SET NOCOUNT ON`.\r\n\r\nIf you regard stored procs as methods, then it&#39;s bad form (anti-pattern) to assume some internal processing works a certain way for your own purposes.\r\n\r\nEdit 2: a [trigger breaking nHibernate question][3], where `SET NOCOUNT ON` can not be set\r\n\r\n(and no, it&#39;s not a duplicate of [this][4])\r\n\r\nEdit 3: Yet more info, thanks to my MVP colleague\r\n\r\n- [KB 240882][5], issue causing disconnects on SQL 2000 and earlier\r\n- [Demo of performance gain][6] \r\n\r\nEdit 4: 13 May 2011\r\n\r\n[Breaks Linq 2 SQL too when not specified?][7]\r\n\r\nEdit 5: 14 Jun 2011\r\n\r\nBreaks JPA, stored proc with table variables: https://stackoverflow.com/q/6344631/27535\r\n\r\nEdit 6: 15 Aug 2011\r\n\r\nThe SSMS &quot;Edit rows&quot; data grid requires SET NOCOUNT ON: https://stackoverflow.com/q/7067329/27535\r\n\r\nEdit 7: 07 Mar 2013\r\n\r\nMore in depth details from @RemusRusanu:&lt;br&gt; https://stackoverflow.com/questions/1915405/does-set-nocount-on-really-make-that-much-of-a-performance-difference/1918085#1918085\r\n\r\n  [1]: https://stackoverflow.com/questions/1483383/is-this-stored-procedure-thread-safe-or-whatever-the-equiv-is-on-sql-server\r\n  [2]: http://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqldataadapter.aspx\r\n  [3]: https://stackoverflow.com/questions/1354362\r\n  [4]: https://stackoverflow.com/questions/995589/set-nocount-off-or-return-rowcount\r\n  [5]: http://support.microsoft.com/?scid=kb%3Ben-us%3B240882&amp;x=4&amp;y=9\r\n  [6]: http://sqlmag.com/sql-server-2000/seeing-believing\r\n  [7]: https://stackoverflow.com/q/5880413/27535",
            "link": "https://stackoverflow.com/questions/1483732/set-nocount-on-usage",
            "title": "SET NOCOUNT ON usage",
            "body": "<p>Inspired by <a href=\"https://stackoverflow.com/questions/1483383/is-this-stored-procedure-thread-safe-or-whatever-the-equiv-is-on-sql-server\">this question</a> where there are differing views on SET NOCOUNT...</p>\n\n<blockquote>\n  <p>Should we use SET NOCOUNT ON for SQL Server? If not, why not?</p>\n</blockquote>\n\n<p><strong>What it does</strong> Edit 6, on 22 Jul 2011</p>\n\n<p>It suppresses the \"xx rows affected\" message after any DML. This is a resultset and when sent, the client must process it. It's tiny, but measurable (see answers below)</p>\n\n<p>For triggers etc, the client will receive multiple \"xx rows affected\" and this causes all manner of errors for some ORMs, MS Access, JPA etc (see edits below)</p>\n\n<p><strong>Background:</strong></p>\n\n<p>General accepted best practice (I thought until this question) is to use <code>SET NOCOUNT ON</code> in triggers and stored procedures in SQL Server. We use it everywhere and a quick google shows plenty of SQL Server MVPs agreeing too.</p>\n\n<p>MSDN says this can break a <a href=\"http://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqldataadapter.aspx\" rel=\"noreferrer\">.net SQLDataAdapter</a>.</p>\n\n<p>Now, this means to me that the SQLDataAdapter is limited to utterly simply CRUD processing because it expects the \"n rows affected\" message to match. So, I can't use:</p>\n\n<ul>\n<li>IF EXISTS to avoid duplicates (no rows affected message) <em>Note: use with caution</em></li>\n<li>WHERE NOT EXISTS (less rows then expected</li>\n<li>Filter out trivial updates (eg no data actually changes)</li>\n<li>Do any table access before (such as logging)</li>\n<li>Hide complexity or denormlisation</li>\n<li>etc</li>\n</ul>\n\n<p>In the question marc_s (who knows his SQL stuff) says do not use it. This differs to what I think (and I regard myself as somewhat competent at SQL too).</p>\n\n<p>It's possible I'm missing something (feel free to point out the obvious), but what do you folks out there think?</p>\n\n<p>Note: it's been years since I saw this error because I don't use SQLDataAdapter nowadays.</p>\n\n<p><strong>Edits after comments and questions:</strong></p>\n\n<p>Edit: More thoughts...</p>\n\n<p>We have multiple clients: one may use a C# SQLDataAdaptor, another may use nHibernate from Java. These can be affected in different ways with <code>SET NOCOUNT ON</code>.</p>\n\n<p>If you regard stored procs as methods, then it's bad form (anti-pattern) to assume some internal processing works a certain way for your own purposes.</p>\n\n<p>Edit 2: a <a href=\"https://stackoverflow.com/questions/1354362\">trigger breaking nHibernate question</a>, where <code>SET NOCOUNT ON</code> can not be set</p>\n\n<p>(and no, it's not a duplicate of <a href=\"https://stackoverflow.com/questions/995589/set-nocount-off-or-return-rowcount\">this</a>)</p>\n\n<p>Edit 3: Yet more info, thanks to my MVP colleague</p>\n\n<ul>\n<li><a href=\"http://support.microsoft.com/?scid=kb%3Ben-us%3B240882&amp;x=4&amp;y=9\" rel=\"noreferrer\">KB 240882</a>, issue causing disconnects on SQL 2000 and earlier</li>\n<li><a href=\"http://sqlmag.com/sql-server-2000/seeing-believing\" rel=\"noreferrer\">Demo of performance gain</a> </li>\n</ul>\n\n<p>Edit 4: 13 May 2011</p>\n\n<p><a href=\"https://stackoverflow.com/q/5880413/27535\">Breaks Linq 2 SQL too when not specified?</a></p>\n\n<p>Edit 5: 14 Jun 2011</p>\n\n<p>Breaks JPA, stored proc with table variables: <a href=\"https://stackoverflow.com/q/6344631/27535\">Does JPA 2.0 support SQL Server table variables?</a></p>\n\n<p>Edit 6: 15 Aug 2011</p>\n\n<p>The SSMS \"Edit rows\" data grid requires SET NOCOUNT ON: <a href=\"https://stackoverflow.com/q/7067329/27535\">Update trigger with GROUP BY</a></p>\n\n<p>Edit 7: 07 Mar 2013</p>\n\n<p>More in depth details from @RemusRusanu:<br> <a href=\"https://stackoverflow.com/questions/1915405/does-set-nocount-on-really-make-that-much-of-a-performance-difference/1918085#1918085\">Does SET NOCOUNT ON really make that much of a performance difference</a></p>\n"
        },
        {
            "tags": [
                "python",
                "file",
                "filenames",
                "wildcard"
            ],
            "owner": {
                "reputation": 56,
                "user_id": 619187,
                "user_type": "unregistered",
                "profile_image": "https://www.gravatar.com/avatar/a7003bf55cd3ecf7061cbc10090ceb75?s=128&d=identicon&r=PG",
                "display_name": "greg",
                "link": "https://stackoverflow.com/users/619187/greg"
            },
            "is_answered": true,
            "view_count": 30521,
            "answer_count": 4,
            "score": 11,
            "last_activity_date": 1524599372,
            "creation_date": 1297840349,
            "last_edit_date": 1447226133,
            "question_id": 5013532,
            "body_markdown": "I have a directory of text files that all have the extension `.txt`. My goal is to print the contents of the text file. I wish to be able use the wildcard `*.txt` to specify the file name I wish to open (I&#39;m thinking along the lines of something like `F:\\text\\*.txt`?), split the lines of the text file, then print the output.\r\n\r\nHere is an example of what I want to do, but I want to be able to change `somefile` when executing my command.\r\n\r\n\r\n\r\n    f = open(&#39;F:\\text\\somefile.txt&#39;, &#39;r&#39;)\r\n    for line in f:\r\n        print line,\r\n\r\n\r\nI had checked out the glob module earlier, but I couldn&#39;t figure out how to actually do anything to the files.  Here is what I came up with, not working. \r\n\r\n    filepath = &quot;F:\\irc\\as\\*.txt&quot;\r\n    txt = glob.glob(filepath)\r\n\r\n    lines = string.split(txt, &#39;\\n&#39;) #AttributeError: &#39;list&#39; object has no attribute &#39;split&#39;\r\n    print lines",
            "link": "https://stackoverflow.com/questions/5013532/open-file-by-filename-wildcard",
            "title": "Open file by filename wildcard",
            "body": "<p>I have a directory of text files that all have the extension <code>.txt</code>. My goal is to print the contents of the text file. I wish to be able use the wildcard <code>*.txt</code> to specify the file name I wish to open (I'm thinking along the lines of something like <code>F:\\text\\*.txt</code>?), split the lines of the text file, then print the output.</p>\n\n<p>Here is an example of what I want to do, but I want to be able to change <code>somefile</code> when executing my command.</p>\n\n<pre><code>f = open('F:\\text\\somefile.txt', 'r')\nfor line in f:\n    print line,\n</code></pre>\n\n<p>I had checked out the glob module earlier, but I couldn't figure out how to actually do anything to the files.  Here is what I came up with, not working. </p>\n\n<pre><code>filepath = \"F:\\irc\\as\\*.txt\"\ntxt = glob.glob(filepath)\n\nlines = string.split(txt, '\\n') #AttributeError: 'list' object has no attribute 'split'\nprint lines\n</code></pre>\n"
        },
        {
            "tags": [
                "api",
                "google-cloud-platform",
                "google-cloud-functions"
            ],
            "owner": {
                "reputation": 9,
                "user_id": 4904920,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/5a63a0e92e75bf06f28d25bc45b96545?s=128&d=identicon&r=PG&f=1",
                "display_name": "handsomebob10",
                "link": "https://stackoverflow.com/users/4904920/handsomebob10"
            },
            "is_answered": false,
            "view_count": 22,
            "closed_date": 1524629822,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524599362,
            "creation_date": 1524596820,
            "last_edit_date": 1524599362,
            "question_id": 50009161,
            "body_markdown": "Is there a way to apply an api that isnt a google api such as Twilio to google cloud functions?\r\n\r\n[enter image description here][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/v0WzP.png",
            "link": "https://stackoverflow.com/questions/50009161/can-an-external-api-be-integrated-to-google-cloud-functions",
            "closed_reason": "off-topic",
            "title": "Can an external API be integrated to google cloud functions?",
            "body": "<p>Is there a way to apply an api that isnt a google api such as Twilio to google cloud functions?</p>\n\n<p><a href=\"https://i.stack.imgur.com/v0WzP.png\" rel=\"nofollow noreferrer\">enter image description here</a></p>\n"
        },
        {
            "tags": [
                "python",
                "pandas",
                "amazon-web-services",
                "amazon-s3",
                "aws-lambda"
            ],
            "owner": {
                "reputation": 16,
                "user_id": 9607674,
                "user_type": "registered",
                "profile_image": "https://lh6.googleusercontent.com/-HGDKg6VZW0g/AAAAAAAAAAI/AAAAAAAAAyE/gfCj8Vf4vbs/photo.jpg?sz=128",
                "display_name": "Dave D",
                "link": "https://stackoverflow.com/users/9607674/dave-d"
            },
            "is_answered": false,
            "view_count": 44,
            "answer_count": 0,
            "score": 1,
            "last_activity_date": 1524599337,
            "creation_date": 1524515607,
            "last_edit_date": 1524599337,
            "question_id": 49989510,
            "body_markdown": "I am looking for some advice on this project.  My thought was to use Python and a Lambda to aggregate the data and respond to the website.  The main parameters are date ranges and can be dynamic.\r\n\r\n**Project Requirements:**\r\n\r\n - Read data from monthly return files stored in JSON (each file contains roughly 3000 securities and is 1.6 MB in size)\r\n - Aggregate the data into various buckets displaying counts and returns for each bucket (for our purposes here lets say the buckets are Sectors and Market Cap ranges which can vary)\r\n - Display aggregated data on a website\r\n\r\n**Issue I face**\r\nI have successfully implemted this in an AWS Lambda, however in testing requests that are 20 years of data (and yes I get them), I begin to hit the memory limits in AWS Lambda.\r\n\r\n**Process I used:**\r\nAll files are stored in S3, so I use the boto3 library to obtain the files, reading them into memory.  This is still small and not of any real significance.\r\n\r\nI use `json.loads` to convert the files into a pandas dataframe.  I was loading all of the files into one large dataframe. - This is where the it runs out of memory.\r\n\r\nI then pass the dataframe to custom aggregations using `groupby` to get my results.  This part is not as fast as I would like but does the job of getting what I need.\r\n\r\nThe end result dataframe that is then converted back into JSON and is less than 500 MB.\r\n\r\nThis entire process when it works locally outside the lambda is about 40 seconds.  \r\nI have tried running this with threads and processing single frames at once but the performance degrades to about 1 min 30 seconds.  \r\n\r\nWhile I would rather not scrap everything and start over, I am willing to do so if there is a more efficient way to handle this.  The old process did everything inside of node.js without the use of a lambda and took almost 3 minutes to generate.\r\n\r\n**Code currently used**\r\nI had to clean this a little to pull out some items but here is the code used.\r\nRead data from S3 into JSON this will result in a list of string data.\r\n\r\n     while not q.empty():\r\n                fkey = q.get()\r\n            \r\n\r\n                try:\r\n                    obj = self.s3.Object(bucket_name=bucket,key=fkey[1])\r\n                    json_data = obj.get()[&#39;Body&#39;].read().decode(&#39;utf-8&#39;)\r\n                    \r\n                    results[fkey[1]] = json_data\r\n                except Exception as e:\r\n                    results[fkey[1]] = str(e)\r\n                q.task_done()\r\n\r\nLoop through the JSON files to build a dataframe for working\r\n\r\n    for k,v in s3Data.items():        \r\n            lstdf.append(buildDataframefromJson(k,v))\r\n        \r\n    def buildDataframefromJson(key, json_data):\r\n        \r\n        tmpdf = pd.DataFrame(columns=[&#39;ticker&#39;,&#39;totalReturn&#39;,&#39;isExcluded&#39;,&#39;marketCapStartUsd&#39;,\r\n                                      &#39;category&#39;,&#39;marketCapBand&#39;,&#39;peGreaterThanMarket&#39;, &#39;Month&#39;,&#39;epsUsd&#39;]\r\n                             )\r\n         #Read the json into a dataframe\r\n        tmpdf = pd.read_json(json_data,\r\n                             dtype={\r\n                                &#39;ticker&#39;:str,\r\n                                &#39;totalReturn&#39;:np.float32,\r\n                                &#39;isExcluded&#39;:np.bool,\r\n                                &#39;marketCapStartUsd&#39;:np.float32,\r\n                                &#39;category&#39;:str,\r\n                                &#39;marketCapBand&#39;:str,\r\n                                &#39;peGreaterThanMarket&#39;:np.bool,\r\n                                &#39;epsUsd&#39;:np.float32\r\n                                })[[&#39;ticker&#39;,&#39;totalReturn&#39;,&#39;isExcluded&#39;,&#39;marketCapStartUsd&#39;,&#39;category&#39;,\r\n                            &#39;marketCapBand&#39;,&#39;peGreaterThanMarket&#39;,&#39;epsUsd&#39;]]\r\n        dtTmp = datetime.strptime(key.split(&#39;/&#39;)[3], &quot;%m-%Y&quot;)\r\n        dtTmp = datetime.strptime(str(dtTmp.year) + &#39;-&#39;+ str(dtTmp.month),&#39;%Y-%m&#39;)\r\n        tmpdf.insert(0,&#39;Month&#39;,dtTmp, allow_duplicates=True)\r\n    \r\n        return tmpdf",
            "link": "https://stackoverflow.com/questions/49989510/hitting-aws-lambda-memory-limit-in-python",
            "title": "Hitting AWS Lambda Memory limit in Python",
            "body": "<p>I am looking for some advice on this project.  My thought was to use Python and a Lambda to aggregate the data and respond to the website.  The main parameters are date ranges and can be dynamic.</p>\n\n<p><strong>Project Requirements:</strong></p>\n\n<ul>\n<li>Read data from monthly return files stored in JSON (each file contains roughly 3000 securities and is 1.6 MB in size)</li>\n<li>Aggregate the data into various buckets displaying counts and returns for each bucket (for our purposes here lets say the buckets are Sectors and Market Cap ranges which can vary)</li>\n<li>Display aggregated data on a website</li>\n</ul>\n\n<p><strong>Issue I face</strong>\nI have successfully implemted this in an AWS Lambda, however in testing requests that are 20 years of data (and yes I get them), I begin to hit the memory limits in AWS Lambda.</p>\n\n<p><strong>Process I used:</strong>\nAll files are stored in S3, so I use the boto3 library to obtain the files, reading them into memory.  This is still small and not of any real significance.</p>\n\n<p>I use <code>json.loads</code> to convert the files into a pandas dataframe.  I was loading all of the files into one large dataframe. - This is where the it runs out of memory.</p>\n\n<p>I then pass the dataframe to custom aggregations using <code>groupby</code> to get my results.  This part is not as fast as I would like but does the job of getting what I need.</p>\n\n<p>The end result dataframe that is then converted back into JSON and is less than 500 MB.</p>\n\n<p>This entire process when it works locally outside the lambda is about 40 seconds.<br>\nI have tried running this with threads and processing single frames at once but the performance degrades to about 1 min 30 seconds.  </p>\n\n<p>While I would rather not scrap everything and start over, I am willing to do so if there is a more efficient way to handle this.  The old process did everything inside of node.js without the use of a lambda and took almost 3 minutes to generate.</p>\n\n<p><strong>Code currently used</strong>\nI had to clean this a little to pull out some items but here is the code used.\nRead data from S3 into JSON this will result in a list of string data.</p>\n\n<pre><code> while not q.empty():\n            fkey = q.get()\n\n\n            try:\n                obj = self.s3.Object(bucket_name=bucket,key=fkey[1])\n                json_data = obj.get()['Body'].read().decode('utf-8')\n\n                results[fkey[1]] = json_data\n            except Exception as e:\n                results[fkey[1]] = str(e)\n            q.task_done()\n</code></pre>\n\n<p>Loop through the JSON files to build a dataframe for working</p>\n\n<pre><code>for k,v in s3Data.items():        \n        lstdf.append(buildDataframefromJson(k,v))\n\ndef buildDataframefromJson(key, json_data):\n\n    tmpdf = pd.DataFrame(columns=['ticker','totalReturn','isExcluded','marketCapStartUsd',\n                                  'category','marketCapBand','peGreaterThanMarket', 'Month','epsUsd']\n                         )\n     #Read the json into a dataframe\n    tmpdf = pd.read_json(json_data,\n                         dtype={\n                            'ticker':str,\n                            'totalReturn':np.float32,\n                            'isExcluded':np.bool,\n                            'marketCapStartUsd':np.float32,\n                            'category':str,\n                            'marketCapBand':str,\n                            'peGreaterThanMarket':np.bool,\n                            'epsUsd':np.float32\n                            })[['ticker','totalReturn','isExcluded','marketCapStartUsd','category',\n                        'marketCapBand','peGreaterThanMarket','epsUsd']]\n    dtTmp = datetime.strptime(key.split('/')[3], \"%m-%Y\")\n    dtTmp = datetime.strptime(str(dtTmp.year) + '-'+ str(dtTmp.month),'%Y-%m')\n    tmpdf.insert(0,'Month',dtTmp, allow_duplicates=True)\n\n    return tmpdf\n</code></pre>\n"
        },
        {
            "tags": [
                "codeigniter",
                "codeigniter-2"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9692846,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/eeef8f8ba39b0ec2ed03c8d769bcf1f4?s=128&d=identicon&r=PG&f=1",
                "display_name": "Ancy",
                "link": "https://stackoverflow.com/users/9692846/ancy"
            },
            "is_answered": false,
            "view_count": 29,
            "answer_count": 1,
            "score": -1,
            "last_activity_date": 1524599336,
            "creation_date": 1524594608,
            "last_edit_date": 1524594914,
            "question_id": 50008596,
            "body_markdown": "I have completed a project work on codeigniter which is done in localhost. I had uploaded the work on to a server and the website was working fine. After some days I moved into another server, when I checked the website the home page is working fine and there is no error in database connectivity but the login page in the website is not working. It is not showing a error page but a white blank page.\r\n\r\nI had checked the version number. Its showing as CI 2.1 and PHP 5.6. Is it the problm of version ? Can anyone say a solution for this ? I checked a stackoverflow link like this https://stackoverflow.com/questions/29230694/codeigniter-shows-blank-page-with-no-error But I didnt get a solution .Should I upgrade the version and how it is to be done ?",
            "link": "https://stackoverflow.com/questions/50008596/how-to-correct-the-login-page-page-which-shows-blank-page-on-login-in-codeignite",
            "title": "How to correct the login page page which shows blank page on login in codeigniter?",
            "body": "<p>I have completed a project work on codeigniter which is done in localhost. I had uploaded the work on to a server and the website was working fine. After some days I moved into another server, when I checked the website the home page is working fine and there is no error in database connectivity but the login page in the website is not working. It is not showing a error page but a white blank page.</p>\n\n<p>I had checked the version number. Its showing as CI 2.1 and PHP 5.6. Is it the problm of version ? Can anyone say a solution for this ? I checked a stackoverflow link like this <a href=\"https://stackoverflow.com/questions/29230694/codeigniter-shows-blank-page-with-no-error\">Codeigniter shows blank page with no error</a> But I didnt get a solution .Should I upgrade the version and how it is to be done ?</p>\n"
        },
        {
            "tags": [
                "c#"
            ],
            "owner": {
                "reputation": 14,
                "user_id": 9277965,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/85280151ef3a6050ba778d952ca15a2e?s=128&d=identicon&r=PG&f=1",
                "display_name": "dguth8",
                "link": "https://stackoverflow.com/users/9277965/dguth8"
            },
            "is_answered": true,
            "view_count": 72,
            "answer_count": 3,
            "score": 1,
            "last_activity_date": 1524599326,
            "creation_date": 1524354464,
            "last_edit_date": 1524599326,
            "question_id": 49961186,
            "body_markdown": "With string interpolation, how do you handle variables piped into a command that contain spaces in them? For example, if you have a variable that has spaces in it (like a UNC path), how do you handle that? \r\n\r\nThis code works when no spaces are present in the &quot;filePath&quot; variable (i.e.; \\\\ServerName\\testfile.txt):\r\n\r\nEx: System.Diagnostics.Process.Start(&quot;net.exe&quot;, $&quot;use X: \\\\\\\\{filePath} {pwd /USER:{usr}&quot;).WaitForExit();\r\n\r\nAs soon as you encounter a path that has spaces in it, however, the command above no longer works, because it&#39;s unable to find the path. Normally, I would apply quotes around a path containing spaces, to counter this (in other languages like PowerShell). How do you do something similar with C# interpolation.",
            "link": "https://stackoverflow.com/questions/49961186/c-variables-inside-of-quotes",
            "title": "C# variables inside of quotes",
            "body": "<p>With string interpolation, how do you handle variables piped into a command that contain spaces in them? For example, if you have a variable that has spaces in it (like a UNC path), how do you handle that? </p>\n\n<p>This code works when no spaces are present in the \"filePath\" variable (i.e.; \\ServerName\\testfile.txt):</p>\n\n<p>Ex: System.Diagnostics.Process.Start(\"net.exe\", $\"use X: \\\\{filePath} {pwd /USER:{usr}\").WaitForExit();</p>\n\n<p>As soon as you encounter a path that has spaces in it, however, the command above no longer works, because it's unable to find the path. Normally, I would apply quotes around a path containing spaces, to counter this (in other languages like PowerShell). How do you do something similar with C# interpolation.</p>\n"
        },
        {
            "tags": [
                "mysql"
            ],
            "owner": {
                "reputation": 46,
                "user_id": 2953276,
                "user_type": "registered",
                "accept_rate": 71,
                "profile_image": "https://www.gravatar.com/avatar/e3c301509913798445384c384f8e8e82?s=128&d=identicon&r=PG&f=1",
                "display_name": "Stangn99",
                "link": "https://stackoverflow.com/users/2953276/stangn99"
            },
            "is_answered": false,
            "view_count": 19,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1524599325,
            "creation_date": 1524495244,
            "question_id": 49984078,
            "body_markdown": "I&#39;m trying to tackle this issue with my limited knowledge of SQL. \r\n\r\nI have one table with several rows of employees. Each employee reports to a MGRID. The data can go as 7-8 levels deep, with an employee reporting to a manager, manager to sr. manager, sr. manager to director, and so on. \r\n\r\nMy table looks like this:\r\n\r\n\r\n    EMPID\tNAME\t\tMGRID\r\n    1\t\tAlex\t\t8\r\n    2\t\tJane\t\t9\r\n    3\t\tBob\t\t    10\r\n    4\t\tShack\t\t11\r\n    5\t\tChris\t\t8\r\n    6\t\tSarah\t\t10\r\n    7\t\tJames\t\t8\r\n    8\t\tMichelle\t11\r\n    9\t\tAna\t\t    11\t\r\n    10\t\tSteve\t\t11\r\n    11\t\tRon\t\t    NULL. &lt;= CEO\r\n    12\t\tMike\t\t3\r\n    13\t\tJenn\t\t3\t\r\n\r\n\r\nMy ultimate goal is to output something that looks like this (multi-level)\r\n\r\n    Ron\r\n    \tShack\r\n    \tSteve\r\n    \t\tBob\r\n    \t\t\tMike\r\n    \t\t\tJenn\r\n    \t\tSarah\r\n    \tAna\r\n    \t\tJane\r\n    \tMichelle\r\n    \t\tJames\r\n    \t\tChris\t\r\n    \t\tAlex\r\n    \t\tChris\r\n\t\r\nCurrently on a mysql development environment with version 5.6, and local environment with ver 8.0. \r\n\r\nThanks. ",
            "link": "https://stackoverflow.com/questions/49984078/mysql-creating-a-tree-org-structure-from-flat-table",
            "title": "MYSQL Creating a tree/org structure from flat table",
            "body": "<p>I'm trying to tackle this issue with my limited knowledge of SQL. </p>\n\n<p>I have one table with several rows of employees. Each employee reports to a MGRID. The data can go as 7-8 levels deep, with an employee reporting to a manager, manager to sr. manager, sr. manager to director, and so on. </p>\n\n<p>My table looks like this:</p>\n\n<pre><code>EMPID   NAME        MGRID\n1       Alex        8\n2       Jane        9\n3       Bob         10\n4       Shack       11\n5       Chris       8\n6       Sarah       10\n7       James       8\n8       Michelle    11\n9       Ana         11  \n10      Steve       11\n11      Ron         NULL. &lt;= CEO\n12      Mike        3\n13      Jenn        3   \n</code></pre>\n\n<p>My ultimate goal is to output something that looks like this (multi-level)</p>\n\n<pre><code>Ron\n    Shack\n    Steve\n        Bob\n            Mike\n            Jenn\n        Sarah\n    Ana\n        Jane\n    Michelle\n        James\n        Chris   \n        Alex\n        Chris\n</code></pre>\n\n<p>Currently on a mysql development environment with version 5.6, and local environment with ver 8.0. </p>\n\n<p>Thanks. </p>\n"
        },
        {
            "tags": [
                "ios",
                "uistackview",
                "stackview"
            ],
            "owner": {
                "reputation": 703,
                "user_id": 2859206,
                "user_type": "registered",
                "accept_rate": 80,
                "profile_image": "https://i.stack.imgur.com/KXUgP.jpg?s=128&g=1",
                "display_name": "DrWhat",
                "link": "https://stackoverflow.com/users/2859206/drwhat"
            },
            "is_answered": true,
            "view_count": 322,
            "accepted_answer_id": 33914915,
            "answer_count": 3,
            "score": 1,
            "last_activity_date": 1524599316,
            "creation_date": 1447260506,
            "question_id": 33655539,
            "body_markdown": "I&#39;ve a fairly difficult layout design that might be easier using nested stack views in iOS. BUT, I&#39;m having problems controlling the size or distribution of stacks nested inside other stacks. One part of the layout, for example, looks OK-ish if I set Distribution to Fit Equally:\r\n[![enter image description here][1]][1]\r\n\r\nBUT, what I really want is the photo and container to be about 1/3 the width of the text field stack. If I set Distribution to Fit Proportionally, the stack with the image (which doesn&#39;t change size) and container spreadout and squash the text against the side of the display. Everything I read suggests to reduce the Content Compression Resistance Priority. I&#39;ve tried this on the image, the container and on the stack view itself, but it doesn&#39;t do much.\r\n\r\nCould someone please point me in the right direction to control the relative widths of stacks nested inside other stacks?\r\n\r\n  [1]: http://i.stack.imgur.com/mQKpg.png",
            "link": "https://stackoverflow.com/questions/33655539/how-do-i-control-the-relative-widths-of-uistackviews-nested-within-another-stack",
            "title": "How do I control the relative widths of UIStackViews nested within another stack view?",
            "body": "<p>I've a fairly difficult layout design that might be easier using nested stack views in iOS. BUT, I'm having problems controlling the size or distribution of stacks nested inside other stacks. One part of the layout, for example, looks OK-ish if I set Distribution to Fit Equally:\n<a href=\"https://i.stack.imgur.com/mQKpg.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/mQKpg.png\" alt=\"enter image description here\"></a></p>\n\n<p>BUT, what I really want is the photo and container to be about 1/3 the width of the text field stack. If I set Distribution to Fit Proportionally, the stack with the image (which doesn't change size) and container spreadout and squash the text against the side of the display. Everything I read suggests to reduce the Content Compression Resistance Priority. I've tried this on the image, the container and on the stack view itself, but it doesn't do much.</p>\n\n<p>Could someone please point me in the right direction to control the relative widths of stacks nested inside other stacks?</p>\n"
        },
        {
            "tags": [
                "c#",
                "asp.net",
                "asp.net-mvc",
                "rest",
                "json.net"
            ],
            "owner": {
                "reputation": 29,
                "user_id": 6661813,
                "user_type": "registered",
                "accept_rate": 57,
                "profile_image": "https://www.gravatar.com/avatar/6d57127f63de373bff3e2b4263f921fe?s=128&d=identicon&r=PG&f=1",
                "display_name": "Niteesh Kumar",
                "link": "https://stackoverflow.com/users/6661813/niteesh-kumar"
            },
            "is_answered": true,
            "view_count": 68,
            "answer_count": 2,
            "score": 0,
            "last_activity_date": 1524599307,
            "creation_date": 1520792975,
            "last_edit_date": 1520822324,
            "question_id": 49223558,
            "body_markdown": "I have a C# class and data table.\r\n\r\nDataTable:\r\n\r\n    +---------+-----------------+---------------+---------+---------+--------------+\r\n    | Pers_Id | Pers_First_Name | Pers_Last_Name| OrderNu | OrderId |  Pers_Update |\r\n    +---------+-----------------+---------------+---------+---------+--------------+\r\n    | 1       |       ABC       |        Ln     |   76454 |  1      |   2018-03-25 |\r\n    +---------+-----------------+---------------+---------+---------+--------------+\r\n    | 1       |       ABC       |        Ln     |   76578 |  2      |   2018-03-25 |\r\n    +---------+-----------------+---------------+---------+---------+--------------+\r\n                   \r\n\r\nClass:\r\n\r\n&lt;!-- language: c# --&gt;\r\n\r\n    public class Person\r\n    {\r\n        public int Pers_Id { get; set; }\r\n        public string Pers_First_Name { get; set; }\r\n        public string Pers_Last_Name { get; set; }\r\n        public DateTime Pers_Update { get; set; }\r\n        public List&lt;Order&gt; Order_List { get; set; }\r\n        \r\n        public class Order\r\n        {\r\n            public int OrderID { get; set; }\r\n            public string OrderNu { get; set; }\r\n        }\r\n    }\r\n\r\nI need to bind this class from data table and need to convert it into json object for rest API response in asp .net web API.\r\n\r\nWhen i am binding i am getting json duplicate but result should be like this \r\n\r\n{\r\n  &quot;Pers_Id&quot;: 1,\r\n  &quot;Pers_First_Name&quot;: &quot;ABC&quot;,\r\n  &quot;Pers_Last_Name&quot;: &quot;LN&quot;,\r\n  &quot;Pers_Update&quot;: &quot;&quot;,\r\n  &quot;Order_List&quot;: [\r\n    {\r\n      &quot;OrderID&quot;: &quot;1&quot;,\r\n      &quot;OrderNu&quot;: &quot;76454&quot;\r\n    },\r\n    {\r\n      &quot;OrderID&quot;: &quot;2&quot;,\r\n      &quot;OrderNu&quot;: &quot;76578&quot;\r\n    }\r\n  ]\r\n}",
            "link": "https://stackoverflow.com/questions/49223558/convert-mvc-c-model-class-to-json-from-datatable",
            "title": "convert mvc c# model class to json from datatable",
            "body": "<p>I have a C# class and data table.</p>\n\n<p>DataTable:</p>\n\n<pre><code>+---------+-----------------+---------------+---------+---------+--------------+\n| Pers_Id | Pers_First_Name | Pers_Last_Name| OrderNu | OrderId |  Pers_Update |\n+---------+-----------------+---------------+---------+---------+--------------+\n| 1       |       ABC       |        Ln     |   76454 |  1      |   2018-03-25 |\n+---------+-----------------+---------------+---------+---------+--------------+\n| 1       |       ABC       |        Ln     |   76578 |  2      |   2018-03-25 |\n+---------+-----------------+---------------+---------+---------+--------------+\n</code></pre>\n\n<p>Class:</p>\n\n<pre class=\"lang-cs prettyprint-override\"><code>public class Person\n{\n    public int Pers_Id { get; set; }\n    public string Pers_First_Name { get; set; }\n    public string Pers_Last_Name { get; set; }\n    public DateTime Pers_Update { get; set; }\n    public List&lt;Order&gt; Order_List { get; set; }\n\n    public class Order\n    {\n        public int OrderID { get; set; }\n        public string OrderNu { get; set; }\n    }\n}\n</code></pre>\n\n<p>I need to bind this class from data table and need to convert it into json object for rest API response in asp .net web API.</p>\n\n<p>When i am binding i am getting json duplicate but result should be like this </p>\n\n<p>{\n  \"Pers_Id\": 1,\n  \"Pers_First_Name\": \"ABC\",\n  \"Pers_Last_Name\": \"LN\",\n  \"Pers_Update\": \"\",\n  \"Order_List\": [\n    {\n      \"OrderID\": \"1\",\n      \"OrderNu\": \"76454\"\n    },\n    {\n      \"OrderID\": \"2\",\n      \"OrderNu\": \"76578\"\n    }\n  ]\n}</p>\n"
        },
        {
            "tags": [
                "c#",
                "generics"
            ],
            "owner": {
                "reputation": 1231,
                "user_id": 60795,
                "user_type": "registered",
                "accept_rate": 89,
                "profile_image": "https://i.stack.imgur.com/2pOUu.jpg?s=128&g=1",
                "display_name": "Mouk",
                "link": "https://stackoverflow.com/users/60795/mouk"
            },
            "is_answered": true,
            "view_count": 1787,
            "answer_count": 1,
            "score": 3,
            "last_activity_date": 1524599305,
            "creation_date": 1257636768,
            "last_edit_date": 1524599305,
            "question_id": 1694813,
            "body_markdown": "A co-worker of mine asked me last week if it were possible in C# to extend a generic class from its generic parameter.  He said it was possible in C++.\r\nWhat he wanted makes actually sense. He wanted a generic decorator to annotate an arbitrary class with additional information. Something like:\r\n\r\n\tpublic class Decorator&lt;T&gt; : T\r\n\t{\r\n\t\tpublic object AdditionalInformation {get:set;}\r\n\t}\r\n\r\nSo that he can now use this generic decorator everywhere instead of T.\r\n\r\nThe most similar thing I could come with was a container class with the original object, the additional information and an implicit conversion.\r\n\r\n\r\n\tpublic class Decorator&lt;T&gt;\r\n\t{\r\n\t\tprivate readonly T _instance;\r\n\t\t\r\n\t\tpublic Decorator(T instance)\r\n\t\t{\r\n\t\t\t_instance = instance;\r\n\t\t}\r\n\r\n\t\tpublic T Instance\r\n\t\t{\r\n\t\t\tget { return _instance; }\r\n\t\t}\r\n\t\tpublic object AdditionalInformation { get; set; }\r\n\r\n\t\tpublic static implicit operator T(Decorator&lt;T&gt; deco)\r\n\t\t{\r\n\t\t\treturn deco._instance;\r\n\t\t}\r\n\t}\r\n\r\nBut this is not the same because the implicit conversion is only one way. He cannot use it, for example, as a return type of a method because the additional information would be lost after he implicit conversion.\r\n\r\nDoes anybody have a better idea?",
            "link": "https://stackoverflow.com/questions/1694813/decorator-with-generic-base-class",
            "title": "Decorator with generic base class",
            "body": "<p>A co-worker of mine asked me last week if it were possible in C# to extend a generic class from its generic parameter.  He said it was possible in C++.\nWhat he wanted makes actually sense. He wanted a generic decorator to annotate an arbitrary class with additional information. Something like:</p>\n\n<pre><code>public class Decorator&lt;T&gt; : T\n{\n    public object AdditionalInformation {get:set;}\n}\n</code></pre>\n\n<p>So that he can now use this generic decorator everywhere instead of T.</p>\n\n<p>The most similar thing I could come with was a container class with the original object, the additional information and an implicit conversion.</p>\n\n<pre><code>public class Decorator&lt;T&gt;\n{\n    private readonly T _instance;\n\n    public Decorator(T instance)\n    {\n        _instance = instance;\n    }\n\n    public T Instance\n    {\n        get { return _instance; }\n    }\n    public object AdditionalInformation { get; set; }\n\n    public static implicit operator T(Decorator&lt;T&gt; deco)\n    {\n        return deco._instance;\n    }\n}\n</code></pre>\n\n<p>But this is not the same because the implicit conversion is only one way. He cannot use it, for example, as a return type of a method because the additional information would be lost after he implicit conversion.</p>\n\n<p>Does anybody have a better idea?</p>\n"
        },
        {
            "tags": [
                "linux",
                "amazon-web-services",
                "centos",
                "packer",
                "hashicorp"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9225789,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/c804f5a585008c798edb9fdbe9408420?s=128&d=identicon&r=PG&f=1",
                "display_name": "lonecoder",
                "link": "https://stackoverflow.com/users/9225789/lonecoder"
            },
            "is_answered": false,
            "view_count": 14,
            "closed_date": 1524640000,
            "answer_count": 1,
            "score": -1,
            "last_activity_date": 1524599291,
            "creation_date": 1524569200,
            "question_id": 50000517,
            "body_markdown": "I have an existing installation of packer 1.0.0 on my ec2 instance which i want to upgrade to 1.2.2. How do i upgrade packer to this version. If uninstalling and re installing is the only option, what is the way to uninstall packer. The output of command ***which packer*** on my ec2 instance is the following ***/usr/local/packer/latest/packer***",
            "link": "https://stackoverflow.com/questions/50000517/how-do-i-upgrade-packer-on-an-aws-ec2-instance",
            "closed_reason": "off-topic",
            "title": "How do I upgrade packer on an aws ec2 instance",
            "body": "<p>I have an existing installation of packer 1.0.0 on my ec2 instance which i want to upgrade to 1.2.2. How do i upgrade packer to this version. If uninstalling and re installing is the only option, what is the way to uninstall packer. The output of command <strong><em>which packer</em></strong> on my ec2 instance is the following <strong><em>/usr/local/packer/latest/packer</em></strong></p>\n"
        },
        {
            "tags": [
                "python"
            ],
            "owner": {
                "reputation": 373,
                "user_id": 734263,
                "user_type": "registered",
                "accept_rate": 63,
                "profile_image": "https://www.gravatar.com/avatar/d7c74225ad53248f4adaa3ca4803c5a1?s=128&d=identicon&r=PG",
                "display_name": "xxbidiao",
                "link": "https://stackoverflow.com/users/734263/xxbidiao"
            },
            "is_answered": false,
            "view_count": 42,
            "answer_count": 0,
            "score": -1,
            "last_activity_date": 1524599289,
            "creation_date": 1524599289,
            "question_id": 50009721,
            "body_markdown": "What should I do to ensure that in a given scope, every `=` deep copies the object instead of only coping reference?\r\n\r\n    with always_deepcopy() as flag:\r\n        object_a = object_b #ensures deep copy, regardless of other assumptions like scope; equivalent to object_a = copy.deepcopy(object_b)\r\n\r\n    def always_deepcopy():\r\n        return &quot;What should I do here?&quot;)\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50009721/python-always-deep-copy-in-a-given-scope",
            "title": "Python: Always deep copy in a given scope?",
            "body": "<p>What should I do to ensure that in a given scope, every <code>=</code> deep copies the object instead of only coping reference?</p>\n\n<pre><code>with always_deepcopy() as flag:\n    object_a = object_b #ensures deep copy, regardless of other assumptions like scope; equivalent to object_a = copy.deepcopy(object_b)\n\ndef always_deepcopy():\n    return \"What should I do here?\")\n</code></pre>\n"
        },
        {
            "tags": [
                "apache-flink",
                "flink-streaming",
                "flink-cep"
            ],
            "owner": {
                "reputation": 19,
                "user_id": 4949562,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-Y31OH53Sj8s/AAAAAAAAAAI/AAAAAAAAAJc/9l6Y4VOH7BA/photo.jpg?sz=128",
                "display_name": "Debajyoti Pathak",
                "link": "https://stackoverflow.com/users/4949562/debajyoti-pathak"
            },
            "is_answered": false,
            "view_count": 47,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1524599287,
            "creation_date": 1522738652,
            "last_edit_date": 1522739924,
            "question_id": 49624021,
            "body_markdown": "I have a simple program using flink CEP library to detect multiple failed login from a file of log records. My application uses Event time and I am doing a keyBy on the logged in &#39;user&#39;. \r\n\r\nThe program works fine when I set the StreamExecutionEnvironment parallelism to 1. It fails when parallelism is anything else. I am unable to understand why.\r\n\r\nI can see that all records related to a particular user is going to the same thread, so why the issue. Also see that the records are on many occasions not in event time order (not sure if that is a problem) but I couldn&#39;t find anything in the api to let me sort the records by event time within a window.\r\n    \r\n    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\r\n    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\r\n    env.getConfig().setAutoWatermarkInterval(1000);\r\n    env.setParallelism(1); //tried with 1 &amp; 4\r\n    .....    \r\n    DataStream&lt;LogEvent&gt; inputLogEventStream = env\r\n        .readFile(format, FILEPATH, FileProcessingMode.PROCESS_CONTINUOUSLY, 1000)\r\n        .map(new MapToLogEvents())\t\t\t\t\r\n        .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;LogEvent&gt;(Time.seconds(0)) {\r\n                public long extractTimestamp(LogEvent element) {\r\n                        return element.getTimeLong();\r\n                }\r\n        })\r\n        .keyBy(new KeySelector&lt;LogEvent, String&gt;() {\r\n                public String getKey(LogEvent le) throws Exception {\r\n                        return le.getUser();\r\n                }\r\n        });\r\n    inputLogEventStream.print();\r\n    \r\n    Pattern&lt;LogEvent, ?&gt; mflPattern = Pattern.&lt;LogEvent&gt; begin(&quot;mfl&quot;)\r\n        .subtype(LogEvent.class).where(\r\n                new SimpleCondition&lt;LogEvent&gt;() {\r\n                        public boolean filter(LogEvent logEvent) {\r\n                                if (logEvent.getResult().equalsIgnoreCase(&quot;failed&quot;)) { return true; }\r\n                                return false;\r\n                        }\r\n        })\r\n        .timesOrMore(3).within(Time.seconds(60));\r\n        \r\n    PatternStream&lt;LogEvent&gt; mflPatternStream = CEP.pattern(inputLogEventStream, mflPattern);\r\n        \r\n    DataStream&lt;Threat&gt; outputMflStream = mflPatternStream.select(\r\n        new PatternSelectFunction&lt;LogEvent, Threat&gt;() {\r\n                public Threat select(Map&lt;String, List&lt;LogEvent&gt;&gt; logEventsMap) {\r\n                        return new Threat(&quot;MULTIPLE FAILED LOGINS detected!&quot;);\r\n                }\r\n        });\r\n    outputMflStream.print();\r\n\r\nAlso reproduced below are print outputs when:\r\n\r\nparallelism = 1 (It **detected pattern** successfully)\r\n\r\n    04/03/2018 12:03:53\tSource: Custom File Source(1/1) switched to RUNNING \r\n    04/03/2018 12:03:53\tSelectCepOperator -&gt; Sink: Unnamed(1/1) switched to RUNNING \r\n    04/03/2018 12:03:53\tSplit Reader: Custom File Source -&gt; Map -&gt; Timestamps/Watermarks(1/1) switched to RUNNING \r\n    04/03/2018 12:03:53\tSink: Unnamed(1/1) switched to RUNNING \r\n    LogEvent [recordType=base18, eventCategory=login, user=paul, machine=laptop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:08Z, timeLong=1522103408000]\r\n    LogEvent [recordType=base19, eventCategory=login, user=deb, machine=desktop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:03Z, timeLong=1522103403000]\r\n    LogEvent [recordType=base20, eventCategory=login, user=deb, machine=desktop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:05Z, timeLong=1522103405000]\r\n    LogEvent [recordType=base21, eventCategory=login, user=deb, machine=desktop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:06Z, timeLong=1522103406000]\r\n    **THREAT** ==&gt; MULTIPLE FAILED LOGINS detected!\r\n\r\nparallelism = 4 (It **failed to detect** pattern)\r\n\r\n    04/03/2018 12:05:33\tSplit Reader: Custom File Source -&gt; Map -&gt; Timestamps/Watermarks(3/4) switched to RUNNING \r\n    04/03/2018 12:05:33\tSplit Reader: Custom File Source -&gt; Map -&gt; Timestamps/Watermarks(2/4) switched to RUNNING \r\n    04/03/2018 12:05:33\tSink: Unnamed(2/4) switched to RUNNING \r\n    04/03/2018 12:05:33\tSelectCepOperator -&gt; Sink: Unnamed(2/4) switched to RUNNING \r\n    04/03/2018 12:05:33\tSink: Unnamed(3/4) switched to RUNNING \r\n    04/03/2018 12:05:33\tSelectCepOperator -&gt; Sink: Unnamed(3/4) switched to RUNNING \r\n    2&gt; LogEvent [recordType=base18, eventCategory=login, user=paul, machine=laptop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:08Z, timeLong=1522103408000]\r\n    3&gt; LogEvent [recordType=base21, eventCategory=login, user=deb, machine=desktop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:06Z, timeLong=1522103406000]\r\n    3&gt; LogEvent [recordType=base20, eventCategory=login, user=deb, machine=desktop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:05Z, timeLong=1522103405000]\r\n    3&gt; LogEvent [recordType=base19, eventCategory=login, user=deb, machine=desktop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:03Z, timeLong=1522103403000]",
            "link": "https://stackoverflow.com/questions/49624021/flink-cep-unable-to-detect-pattern-when-parallelism-1",
            "title": "Flink CEP unable to detect pattern when parallelism &gt; 1",
            "body": "<p>I have a simple program using flink CEP library to detect multiple failed login from a file of log records. My application uses Event time and I am doing a keyBy on the logged in 'user'. </p>\n\n<p>The program works fine when I set the StreamExecutionEnvironment parallelism to 1. It fails when parallelism is anything else. I am unable to understand why.</p>\n\n<p>I can see that all records related to a particular user is going to the same thread, so why the issue. Also see that the records are on many occasions not in event time order (not sure if that is a problem) but I couldn't find anything in the api to let me sort the records by event time within a window.</p>\n\n<pre><code>final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);\nenv.getConfig().setAutoWatermarkInterval(1000);\nenv.setParallelism(1); //tried with 1 &amp; 4\n.....    \nDataStream&lt;LogEvent&gt; inputLogEventStream = env\n    .readFile(format, FILEPATH, FileProcessingMode.PROCESS_CONTINUOUSLY, 1000)\n    .map(new MapToLogEvents())              \n    .assignTimestampsAndWatermarks(new BoundedOutOfOrdernessTimestampExtractor&lt;LogEvent&gt;(Time.seconds(0)) {\n            public long extractTimestamp(LogEvent element) {\n                    return element.getTimeLong();\n            }\n    })\n    .keyBy(new KeySelector&lt;LogEvent, String&gt;() {\n            public String getKey(LogEvent le) throws Exception {\n                    return le.getUser();\n            }\n    });\ninputLogEventStream.print();\n\nPattern&lt;LogEvent, ?&gt; mflPattern = Pattern.&lt;LogEvent&gt; begin(\"mfl\")\n    .subtype(LogEvent.class).where(\n            new SimpleCondition&lt;LogEvent&gt;() {\n                    public boolean filter(LogEvent logEvent) {\n                            if (logEvent.getResult().equalsIgnoreCase(\"failed\")) { return true; }\n                            return false;\n                    }\n    })\n    .timesOrMore(3).within(Time.seconds(60));\n\nPatternStream&lt;LogEvent&gt; mflPatternStream = CEP.pattern(inputLogEventStream, mflPattern);\n\nDataStream&lt;Threat&gt; outputMflStream = mflPatternStream.select(\n    new PatternSelectFunction&lt;LogEvent, Threat&gt;() {\n            public Threat select(Map&lt;String, List&lt;LogEvent&gt;&gt; logEventsMap) {\n                    return new Threat(\"MULTIPLE FAILED LOGINS detected!\");\n            }\n    });\noutputMflStream.print();\n</code></pre>\n\n<p>Also reproduced below are print outputs when:</p>\n\n<p>parallelism = 1 (It <strong>detected pattern</strong> successfully)</p>\n\n<pre><code>04/03/2018 12:03:53 Source: Custom File Source(1/1) switched to RUNNING \n04/03/2018 12:03:53 SelectCepOperator -&gt; Sink: Unnamed(1/1) switched to RUNNING \n04/03/2018 12:03:53 Split Reader: Custom File Source -&gt; Map -&gt; Timestamps/Watermarks(1/1) switched to RUNNING \n04/03/2018 12:03:53 Sink: Unnamed(1/1) switched to RUNNING \nLogEvent [recordType=base18, eventCategory=login, user=paul, machine=laptop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:08Z, timeLong=1522103408000]\nLogEvent [recordType=base19, eventCategory=login, user=deb, machine=desktop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:03Z, timeLong=1522103403000]\nLogEvent [recordType=base20, eventCategory=login, user=deb, machine=desktop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:05Z, timeLong=1522103405000]\nLogEvent [recordType=base21, eventCategory=login, user=deb, machine=desktop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:06Z, timeLong=1522103406000]\n**THREAT** ==&gt; MULTIPLE FAILED LOGINS detected!\n</code></pre>\n\n<p>parallelism = 4 (It <strong>failed to detect</strong> pattern)</p>\n\n<pre><code>04/03/2018 12:05:33 Split Reader: Custom File Source -&gt; Map -&gt; Timestamps/Watermarks(3/4) switched to RUNNING \n04/03/2018 12:05:33 Split Reader: Custom File Source -&gt; Map -&gt; Timestamps/Watermarks(2/4) switched to RUNNING \n04/03/2018 12:05:33 Sink: Unnamed(2/4) switched to RUNNING \n04/03/2018 12:05:33 SelectCepOperator -&gt; Sink: Unnamed(2/4) switched to RUNNING \n04/03/2018 12:05:33 Sink: Unnamed(3/4) switched to RUNNING \n04/03/2018 12:05:33 SelectCepOperator -&gt; Sink: Unnamed(3/4) switched to RUNNING \n2&gt; LogEvent [recordType=base18, eventCategory=login, user=paul, machine=laptop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:08Z, timeLong=1522103408000]\n3&gt; LogEvent [recordType=base21, eventCategory=login, user=deb, machine=desktop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:06Z, timeLong=1522103406000]\n3&gt; LogEvent [recordType=base20, eventCategory=login, user=deb, machine=desktop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:05Z, timeLong=1522103405000]\n3&gt; LogEvent [recordType=base19, eventCategory=login, user=deb, machine=desktop1, result=failed, eventCount=1, dataBytes=100, time=2018-03-26T22:30:03Z, timeLong=1522103403000]\n</code></pre>\n"
        },
        {
            "tags": [
                "iis",
                "url-rewriting",
                "url-rewrite-module"
            ],
            "owner": {
                "reputation": 10,
                "user_id": 5376930,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/142ea065788206d4ee32f78cd6b23de3?s=128&d=identicon&r=PG&f=1",
                "display_name": "Valter Beretta",
                "link": "https://stackoverflow.com/users/5376930/valter-beretta"
            },
            "is_answered": false,
            "view_count": 12,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524599283,
            "creation_date": 1524586354,
            "last_edit_date": 1524599283,
            "question_id": 50006407,
            "body_markdown": "I need help to setup an Url Rewrite in IIS with an url in 1 field of pattern.\r\nThis what I would like to setup:\r\n\r\nInput url: \r\n\r\n    http://www.example.com/Trk/14966/2/address@email.xy/http://www.example2.com/aa/bb\r\n\r\nRewrite url: \r\n\r\n    http://www.example.com/public/Trk/Mypage.php?fCodImc=14966&amp;fMitCod=2&amp;fMitMai=address@email.xy&amp;fRef=http://www.example2.com/aa/bb\r\n\r\nWhat i tried, but not work:\r\n\r\nPattern:\r\n\r\n    ^trk/([_0-9a-z-]+)/([_0-9a-z-]+)/([^@]+@[^@]+)/(.*)\r\n\r\nRewrite:\r\n\r\n    /Public/Trk/mypage.php?fCodImc={R:1}&amp;fMitCod={R:2}&amp;fMitMai={R:3}&amp;fRef={R:4}\r\n\r\nThe problem is in paticular to find the exact way to pass the last parameter {R:4}\r\n\r\nAny idea?\r\nThankyou\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50006407/iis-url-rewrite-with-url-in-pattern",
            "title": "IIS url rewrite with url in pattern",
            "body": "<p>I need help to setup an Url Rewrite in IIS with an url in 1 field of pattern.\nThis what I would like to setup:</p>\n\n<p>Input url: </p>\n\n<pre><code>http://www.example.com/Trk/14966/2/address@email.xy/http://www.example2.com/aa/bb\n</code></pre>\n\n<p>Rewrite url: </p>\n\n<pre><code>http://www.example.com/public/Trk/Mypage.php?fCodImc=14966&amp;fMitCod=2&amp;fMitMai=address@email.xy&amp;fRef=http://www.example2.com/aa/bb\n</code></pre>\n\n<p>What i tried, but not work:</p>\n\n<p>Pattern:</p>\n\n<pre><code>^trk/([_0-9a-z-]+)/([_0-9a-z-]+)/([^@]+@[^@]+)/(.*)\n</code></pre>\n\n<p>Rewrite:</p>\n\n<pre><code>/Public/Trk/mypage.php?fCodImc={R:1}&amp;fMitCod={R:2}&amp;fMitMai={R:3}&amp;fRef={R:4}\n</code></pre>\n\n<p>The problem is in paticular to find the exact way to pass the last parameter {R:4}</p>\n\n<p>Any idea?\nThankyou</p>\n"
        },
        {
            "tags": [
                "database",
                "design-patterns",
                "scalability",
                "system-design"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9672883,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
                "display_name": "Mark Musk",
                "link": "https://stackoverflow.com/users/9672883/mark-musk"
            },
            "is_answered": false,
            "view_count": 27,
            "closed_date": 1524676403,
            "answer_count": 0,
            "score": -1,
            "last_activity_date": 1524599275,
            "creation_date": 1524540987,
            "last_edit_date": 1524599275,
            "question_id": 49993041,
            "body_markdown": "Suppose I have a database locally and I want to send data to a remote database. How would I know that there is a connection between the remote and local databases and know when there is no connection between them? In case of no connection, the data would be written to the logs and when the connection comes back up we would sort the data according to their timestamp locally and then send it to the remote server.\r\n\r\nI was thinking of keep pinging the remote database, if we get a response to the ping then it would mean that the remote database is accepting requests otherwise there is no connection. But this would spam the server with lots of requests. Is there any better way?",
            "link": "https://stackoverflow.com/questions/49993041/send-data-from-local-to-remote-database",
            "closed_reason": "too broad",
            "title": "Send data from local to remote database",
            "body": "<p>Suppose I have a database locally and I want to send data to a remote database. How would I know that there is a connection between the remote and local databases and know when there is no connection between them? In case of no connection, the data would be written to the logs and when the connection comes back up we would sort the data according to their timestamp locally and then send it to the remote server.</p>\n\n<p>I was thinking of keep pinging the remote database, if we get a response to the ping then it would mean that the remote database is accepting requests otherwise there is no connection. But this would spam the server with lots of requests. Is there any better way?</p>\n"
        },
        {
            "tags": [
                "jms",
                "activemq"
            ],
            "owner": {
                "reputation": 3563,
                "user_id": 39334,
                "user_type": "registered",
                "accept_rate": 81,
                "profile_image": "https://graph.facebook.com/520402061/picture?type=large",
                "display_name": "stolsvik",
                "link": "https://stackoverflow.com/users/39334/stolsvik"
            },
            "is_answered": false,
            "view_count": 13,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524599264,
            "creation_date": 1524599264,
            "question_id": 50009716,
            "body_markdown": "Is there a way to check an ActiveMQ server is still up, without actually receiving or sending or doing a transaction? I am employing a *&quot;Best Effort 1 Phase Commit&quot;* logic - i.e. not using XA, but instead commit db right before committing the JMS, assuming that the JMS commit will go through unless the server has gone down (as opposed to the SQL commit, which can fail due to e.g. constraint violations).\r\n\r\nThis is very close to perfectly good enough. However, if I could, right before committing the database transaction, check whether the ActiveMQ server actually was running right now, I would tighten the potential crash-window just a tad bit more.\r\n\r\nA way to send some kind of &quot;ping&quot; through the connection would be exactly what I was looking for, e.g. get hold of some kind of status from the actual server (not just ask the connection object whether *it believes* it has a connection to the server). For a SQL Server, &quot;SELECT 1&quot; is pretty nice for this scenario. I guess I could send a message to a topic without any consumers, but this seems a bit heavy handed. It would be no problem to employ methods on the actual ActiveMQConnection object (as opposed to purely the JMS API).",
            "link": "https://stackoverflow.com/questions/50009716/activemq-actively-check-connection-for-server-liveliness",
            "title": "ActiveMQ actively check connection for server liveliness",
            "body": "<p>Is there a way to check an ActiveMQ server is still up, without actually receiving or sending or doing a transaction? I am employing a <em>\"Best Effort 1 Phase Commit\"</em> logic - i.e. not using XA, but instead commit db right before committing the JMS, assuming that the JMS commit will go through unless the server has gone down (as opposed to the SQL commit, which can fail due to e.g. constraint violations).</p>\n\n<p>This is very close to perfectly good enough. However, if I could, right before committing the database transaction, check whether the ActiveMQ server actually was running right now, I would tighten the potential crash-window just a tad bit more.</p>\n\n<p>A way to send some kind of \"ping\" through the connection would be exactly what I was looking for, e.g. get hold of some kind of status from the actual server (not just ask the connection object whether <em>it believes</em> it has a connection to the server). For a SQL Server, \"SELECT 1\" is pretty nice for this scenario. I guess I could send a message to a topic without any consumers, but this seems a bit heavy handed. It would be no problem to employ methods on the actual ActiveMQConnection object (as opposed to purely the JMS API).</p>\n"
        },
        {
            "tags": [
                "javascript",
                "react-native"
            ],
            "owner": {
                "reputation": 1902,
                "user_id": 4398966,
                "user_type": "registered",
                "accept_rate": 64,
                "profile_image": "https://i.stack.imgur.com/Md0LP.jpg?s=128&g=1",
                "display_name": "DCR",
                "link": "https://stackoverflow.com/users/4398966/dcr"
            },
            "is_answered": true,
            "view_count": 24,
            "accepted_answer_id": 50009503,
            "answer_count": 2,
            "score": 0,
            "last_activity_date": 1524599259,
            "creation_date": 1524597117,
            "last_edit_date": 1524597724,
            "question_id": 50009223,
            "body_markdown": "If I have a flat object then this works: \r\n\r\n     let stateCopy={...this.state}\r\n\r\n     Object.entries(dictionary).map(([key,value])=&gt;{\r\n     stateCopy.key = value.toString())\r\n     })\r\n\r\nIs there a way to do this if dictionary contains a nested object.  Suppose a dictionary looks like:\r\n\r\n    dictionary={left:{name:&#39;WORK&#39;,\r\n                      min:2,\r\n                      sec:0,}\r\n                start:true}\r\n\r\nI need some way of updating stateCopy, i.e\r\n\r\n    stateCopy.left.name=&#39;WORK&#39; \r\n    stateCopy.left.min=2 \r\n    stateCopy.left.sec=0 \r\n    stateCopy.start=true ",
            "link": "https://stackoverflow.com/questions/50009223/how-to-get-keys-of-nested-object",
            "title": "how to get keys of nested object",
            "body": "<p>If I have a flat object then this works: </p>\n\n<pre><code> let stateCopy={...this.state}\n\n Object.entries(dictionary).map(([key,value])=&gt;{\n stateCopy.key = value.toString())\n })\n</code></pre>\n\n<p>Is there a way to do this if dictionary contains a nested object.  Suppose a dictionary looks like:</p>\n\n<pre><code>dictionary={left:{name:'WORK',\n                  min:2,\n                  sec:0,}\n            start:true}\n</code></pre>\n\n<p>I need some way of updating stateCopy, i.e</p>\n\n<pre><code>stateCopy.left.name='WORK' \nstateCopy.left.min=2 \nstateCopy.left.sec=0 \nstateCopy.start=true \n</code></pre>\n"
        },
        {
            "tags": [
                "tensorflow",
                "object-detection",
                "object-detection-api"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 6879309,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/f2976253ee4efd3c259ff51b1975b756?s=128&d=identicon&r=PG&f=1",
                "display_name": "Mohamed Hedeya",
                "link": "https://stackoverflow.com/users/6879309/mohamed-hedeya"
            },
            "is_answered": false,
            "view_count": 12,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524599247,
            "creation_date": 1524599247,
            "question_id": 50009709,
            "body_markdown": "I was trying to use tensorflow object detection API to fine tune the mask_rcnn_inception_resnet_v2_atrous_coco model and use it to train on the MIO-TCD dataset. I converted the MIO-TCD dataset into TFRecord.\r\n\r\nHowever, I got stuck with the following InvalidArgumentError:\r\n\r\n    INFO:tensorflow:Error reported to Coordinator: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [5]\r\n             [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](Loss/BoxClassifierLoss/assert_equal_2/All/_155, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_157, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/shape/_147)]]\r\n             [[Node: FirstStageFeatureExtractor/InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/read/_225 = _Recv[client_terminated=false, recv_device=&quot;/job:localhost/replica:0/task:0/device:GPU:0&quot;, send_device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;, send_device_incarnation=1, tensor_name=&quot;edge_2248_FirstStageFeatureExtractor/InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/read&quot;, tensor_type=DT_FLOAT, _device=&quot;/job:localhost/replica:0/task:0/device:GPU:0&quot;]()]]\r\n    \r\n    Caused by op &#39;Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert&#39;, defined at:\r\n      File &quot;train.py&quot;, line 167, in &lt;module&gt;\r\n        tf.app.run()\r\n      File &quot;C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py&quot;, line 124, in run\r\n        _sys.exit(main(argv))\r\n      File &quot;train.py&quot;, line 163, in main\r\n        worker_job_name, is_chief, FLAGS.train_dir)\r\n      File &quot;C:\\Users\\hedey\\models\\research\\object_detection\\trainer.py&quot;, line 246, in train\r\n        clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])\r\n      File &quot;C:\\Users\\hedey\\models\\research\\deployment\\model_deploy.py&quot;, line 193, in create_clones\r\n        outputs = model_fn(*args, **kwargs)\r\n      File &quot;C:\\Users\\hedey\\models\\research\\object_detection\\trainer.py&quot;, line 181, in _create_losses\r\n        losses_dict = detection_model.loss(prediction_dict, true_image_shapes)\r\n      File &quot;C:\\Users\\hedey\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py&quot;, line 1580, in loss\r\n        groundtruth_masks_list,\r\n      File &quot;C:\\Users\\hedey\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py&quot;, line 1813, in _loss_box_classifier\r\n        groundtruth_boxlists, groundtruth_masks_list)\r\n      File &quot;C:\\Users\\hedey\\models\\research\\object_detection\\core\\target_assigner.py&quot;, line 447, in batch_assign_targets\r\n        anchors, gt_boxes, gt_class_targets, gt_weights)\r\n      File &quot;C:\\Users\\hedey\\models\\research\\object_detection\\core\\target_assigner.py&quot;, line 151, in assign\r\n        groundtruth_boxes.get())[:1])\r\n      File &quot;C:\\Users\\hedey\\models\\research\\object_detection\\utils\\shape_utils.py&quot;, line 279, in assert_shape_equal\r\n        return tf.assert_equal(shape_a, shape_b)\r\n      File &quot;C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\check_ops.py&quot;, line 392, in assert_equal\r\n        return control_flow_ops.Assert(condition, data, summarize=summarize)\r\n      File &quot;C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py&quot;, line 118, in wrapped\r\n        return _add_should_use_warning(fn(*args, **kwargs))\r\n      File &quot;C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py&quot;, line 169, in Assert\r\n        condition, data, summarize, name=&quot;Assert&quot;)\r\n      File &quot;C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py&quot;, line 48, in _assert\r\n        name=name)\r\n      File &quot;C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py&quot;, line 787, in _apply_op_helper\r\n        op_def=op_def)\r\n      File &quot;C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py&quot;, line 3160, in create_op\r\n        op_def=op_def)\r\n      File &quot;C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py&quot;, line 1625, in __init__\r\n        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n    \r\n    InvalidArgumentError (see above for traceback): assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [5]\r\n             [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](Loss/BoxClassifierLoss/assert_equal_2/All/_155, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_157, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/shape/_147)]]\r\n             [[Node: FirstStageFeatureExtractor/InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/read/_225 = _Recv[client_terminated=false, recv_device=&quot;/job:localhost/replica:0/task:0/device:GPU:0&quot;, send_device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;, send_device_incarnation=1, tensor_name=&quot;edge_2248_FirstStageFeatureExtractor/InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/read&quot;, tensor_type=DT_FLOAT, _device=&quot;/job:localhost/replica:0/task:0/device:GPU:0&quot;]()]]\r\n    Traceback (most recent call last):\r\n      File &quot;C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py&quot;, line 1350, in _do_call\r\n        return fn(*args)\r\n      File &quot;C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py&quot;, line 1329, in _run_fn\r\n        status, run_metadata)\r\n      File &quot;C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py&quot;, line 473, in __exit__\r\n        c_api.TF_GetCode(self.status.status))\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [5]\r\n             [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](Loss/BoxClassifierLoss/assert_equal_2/All/_155, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_157, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/shape/_147)]]\r\n             [[Node: FirstStageFeatureExtractor/InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/read/_225 = _Recv[client_terminated=false, recv_device=&quot;/job:localhost/replica:0/task:0/device:GPU:0&quot;, send_device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;, send_device_incarnation=1, tensor_name=&quot;edge_2248_FirstStageFeatureExtractor/InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/read&quot;, tensor_type=DT_FLOAT, _device=&quot;/job:localhost/replica:0/task:0/device:GPU:0&quot;]()]]\r\n\r\nI found that other people posted about the same problem in more than one of the github issues. The following is an example, and I already commented there and was advised to post about it on stackoverflow:\r\nhttps://github.com/tensorflow/models/issues/3972#issuecomment-381535604\r\nhttps://github.com/tensorflow/models/issues/3972#issuecomment-381535604",
            "link": "https://stackoverflow.com/questions/50009709/assertion-failed-error-when-using-tensorflow-object-detection-api-to-fine-tune-t",
            "title": "assertion failed error when using tensorflow object detection API to fine tune the mask_rcnn_inception_resnet_v2_atrous_coco model",
            "body": "<p>I was trying to use tensorflow object detection API to fine tune the mask_rcnn_inception_resnet_v2_atrous_coco model and use it to train on the MIO-TCD dataset. I converted the MIO-TCD dataset into TFRecord.</p>\n\n<p>However, I got stuck with the following InvalidArgumentError:</p>\n\n<pre><code>INFO:tensorflow:Error reported to Coordinator: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [5]\n         [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Loss/BoxClassifierLoss/assert_equal_2/All/_155, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_157, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/shape/_147)]]\n         [[Node: FirstStageFeatureExtractor/InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/read/_225 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_2248_FirstStageFeatureExtractor/InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/read\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert', defined at:\n  File \"train.py\", line 167, in &lt;module&gt;\n    tf.app.run()\n  File \"C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\", line 124, in run\n    _sys.exit(main(argv))\n  File \"train.py\", line 163, in main\n    worker_job_name, is_chief, FLAGS.train_dir)\n  File \"C:\\Users\\hedey\\models\\research\\object_detection\\trainer.py\", line 246, in train\n    clones = model_deploy.create_clones(deploy_config, model_fn, [input_queue])\n  File \"C:\\Users\\hedey\\models\\research\\deployment\\model_deploy.py\", line 193, in create_clones\n    outputs = model_fn(*args, **kwargs)\n  File \"C:\\Users\\hedey\\models\\research\\object_detection\\trainer.py\", line 181, in _create_losses\n    losses_dict = detection_model.loss(prediction_dict, true_image_shapes)\n  File \"C:\\Users\\hedey\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 1580, in loss\n    groundtruth_masks_list,\n  File \"C:\\Users\\hedey\\models\\research\\object_detection\\meta_architectures\\faster_rcnn_meta_arch.py\", line 1813, in _loss_box_classifier\n    groundtruth_boxlists, groundtruth_masks_list)\n  File \"C:\\Users\\hedey\\models\\research\\object_detection\\core\\target_assigner.py\", line 447, in batch_assign_targets\n    anchors, gt_boxes, gt_class_targets, gt_weights)\n  File \"C:\\Users\\hedey\\models\\research\\object_detection\\core\\target_assigner.py\", line 151, in assign\n    groundtruth_boxes.get())[:1])\n  File \"C:\\Users\\hedey\\models\\research\\object_detection\\utils\\shape_utils.py\", line 279, in assert_shape_equal\n    return tf.assert_equal(shape_a, shape_b)\n  File \"C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\check_ops.py\", line 392, in assert_equal\n    return control_flow_ops.Assert(condition, data, summarize=summarize)\n  File \"C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 118, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 169, in Assert\n    condition, data, summarize, name=\"Assert\")\n  File \"C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\ops\\gen_logging_ops.py\", line 48, in _assert\n    name=name)\n  File \"C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [5]\n         [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Loss/BoxClassifierLoss/assert_equal_2/All/_155, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_157, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/shape/_147)]]\n         [[Node: FirstStageFeatureExtractor/InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/read/_225 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_2248_FirstStageFeatureExtractor/InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/read\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\nTraceback (most recent call last):\n  File \"C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1350, in _do_call\n    return fn(*args)\n  File \"C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1329, in _run_fn\n    status, run_metadata)\n  File \"C:\\Users\\hedey\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 473, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [] [Condition x == y did not hold element-wise:] [x (Loss/BoxClassifierLoss/assert_equal_2/x:0) = ] [0] [y (Loss/BoxClassifierLoss/assert_equal_2/y:0) = ] [5]\n         [[Node: Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert = Assert[T=[DT_STRING, DT_STRING, DT_STRING, DT_INT32, DT_STRING, DT_INT32], summarize=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Loss/BoxClassifierLoss/assert_equal_2/All/_155, Loss/RPNLoss/assert_equal/Assert/Assert/data_0, Loss/RPNLoss/assert_equal/Assert/Assert/data_1, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_2, Loss/BoxClassifierLoss/assert_equal_2/x/_157, Loss/BoxClassifierLoss/assert_equal_2/Assert/Assert/data_4, Loss/RPNLoss/ones_1/shape/_147)]]\n         [[Node: FirstStageFeatureExtractor/InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/read/_225 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device_incarnation=1, tensor_name=\"edge_2248_FirstStageFeatureExtractor/InceptionResnetV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/read\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]()]]\n</code></pre>\n\n<p>I found that other people posted about the same problem in more than one of the github issues. The following is an example, and I already commented there and was advised to post about it on stackoverflow:\n<a href=\"https://github.com/tensorflow/models/issues/3972#issuecomment-381535604\" rel=\"nofollow noreferrer\">https://github.com/tensorflow/models/issues/3972#issuecomment-381535604</a>\n<a href=\"https://github.com/tensorflow/models/issues/3972#issuecomment-381535604\" rel=\"nofollow noreferrer\">https://github.com/tensorflow/models/issues/3972#issuecomment-381535604</a></p>\n"
        },
        {
            "tags": [
                "file",
                "rpm",
                "rpmbuild",
                "buildroot"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9692922,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-XdUIqdMkCWA/AAAAAAAAAAI/AAAAAAAAAAA/4252rscbv5M/photo.jpg?sz=128",
                "display_name": "Steve Johnson",
                "link": "https://stackoverflow.com/users/9692922/steve-johnson"
            },
            "is_answered": false,
            "view_count": 19,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524599239,
            "creation_date": 1524596174,
            "last_edit_date": 1524599239,
            "question_id": 50009000,
            "body_markdown": "The system where I used to manually build rpms has been shut down.  I don&#39;t know the version of rpmbuild which it used, but the new system has 4.11.3 running under CentOS 7.3.\r\n\r\nMy basic command is:\r\n\r\n    rpmbuild --define &#39;_topdir myTopDir&#39; -bb myspecfile.spec\r\n\r\nIn short, I don&#39;t do anything fancy until the rpm is installed on a target system, although the tarball is unbundled to a separate partition due to partition size limitations on some of the target systems.  It&#39;s unbundled into `%{myInstallDir}`, which is defined using a `%define` at the top of the spec file.\r\n\r\nFor the most part, all works as it used to.\r\n\r\nWhen it gets to the `%files` section, however, it prepends a directory, which never happened before.\r\n\r\nThe %files section is pretty simple:\r\n\r\n    %defattr(-,%{myUser},%{myGroup})\r\n    %{myInstallDir}/*\r\n\r\nIt used to look for files in `%{myInstallDir}` - exactly what I want. Tried and true for installs, upgrades, uninstalls, etc.\r\n\r\nNow it complains that it can&#39;t find anything in `%{_topdir}/BUILDROOT/myApp.x86_64/%{myInstallDir}`\r\n\r\nI&#39;ve tried setting buildroot to the empty string or `%{myInstallDir}` at the command line, but rpmbuild complains about the empty string and searches `%{myInstallDir}/%{myInstallDir}` in the second case.\r\n\r\nIt wants to prepend `%{_topdir}/BUILDROOT/myApp.x86_64` or some other path such as &quot;/&quot; to whatever I tell it.  How do I get it to stop prepending a path?\r\n\r\nThanks.\r\n\r\n",
            "link": "https://stackoverflow.com/questions/50009000/rpm-files-section-prepending-buildroot",
            "title": "rpm %files section prepending BUILDROOT",
            "body": "<p>The system where I used to manually build rpms has been shut down.  I don't know the version of rpmbuild which it used, but the new system has 4.11.3 running under CentOS 7.3.</p>\n\n<p>My basic command is:</p>\n\n<pre><code>rpmbuild --define '_topdir myTopDir' -bb myspecfile.spec\n</code></pre>\n\n<p>In short, I don't do anything fancy until the rpm is installed on a target system, although the tarball is unbundled to a separate partition due to partition size limitations on some of the target systems.  It's unbundled into <code>%{myInstallDir}</code>, which is defined using a <code>%define</code> at the top of the spec file.</p>\n\n<p>For the most part, all works as it used to.</p>\n\n<p>When it gets to the <code>%files</code> section, however, it prepends a directory, which never happened before.</p>\n\n<p>The %files section is pretty simple:</p>\n\n<pre><code>%defattr(-,%{myUser},%{myGroup})\n%{myInstallDir}/*\n</code></pre>\n\n<p>It used to look for files in <code>%{myInstallDir}</code> - exactly what I want. Tried and true for installs, upgrades, uninstalls, etc.</p>\n\n<p>Now it complains that it can't find anything in <code>%{_topdir}/BUILDROOT/myApp.x86_64/%{myInstallDir}</code></p>\n\n<p>I've tried setting buildroot to the empty string or <code>%{myInstallDir}</code> at the command line, but rpmbuild complains about the empty string and searches <code>%{myInstallDir}/%{myInstallDir}</code> in the second case.</p>\n\n<p>It wants to prepend <code>%{_topdir}/BUILDROOT/myApp.x86_64</code> or some other path such as \"/\" to whatever I tell it.  How do I get it to stop prepending a path?</p>\n\n<p>Thanks.</p>\n"
        }
    ],
    "has_more": true,
    "quota_max": 300,
    "quota_remaining": 294
}