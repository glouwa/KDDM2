{
    "items": [
        {
            "tags": [
                "architecture",
                "rabbitmq",
                "message-queue",
                "amqp"
            ],
            "owner": {
                "reputation": 7249,
                "user_id": 175057,
                "user_type": "registered",
                "accept_rate": 82,
                "profile_image": "https://i.stack.imgur.com/epnyD.jpg?s=128&g=1",
                "display_name": "contactmatt",
                "link": "https://stackoverflow.com/users/175057/contactmatt"
            },
            "is_answered": false,
            "view_count": 10,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524204052,
            "creation_date": 1524204052,
            "question_id": 49934981,
            "body_markdown": "I&#39;m architecting RabbitMQ into our solution, and I&#39;m curious on how to handle processing and acknowledging messages efficiently, while still performing &quot;real&quot; work in the consumer code that can range from 5-10 seconds. (More work than the [samples][1] ever delve into).\r\n\r\n[![enter image description here][2]][2]\r\n\r\nAbove is an example of what I&#39;m aiming to process. A message in my `twitter.tweet_cmd_q` queue that has all the parameters needed in the message body, for the consumer to make the actual Twitter API request, and save those results to the DB.\r\n\r\nHowever, I&#39;m running into two problems here:\r\n\r\n 1. I&#39;ll be processing thousands of records a minute - I can&#39;t possibly take 5-10 seconds before acknowledging that message. Is it &quot;normal&quot; for the consumer code to process all the needed work before acknowledging the message? (i.e. I could see ack&#39;ing the message, and throwing the actual work to be done in another thread for processing. - though this would then require it&#39;s own form of &quot;thread&quot; management so that the system does undergo too much load).\r\n 2. Would [RPC calls][3] benefit me at all here, in this situation that involves querying the data and saving it to the DB?\r\n\r\n 3. Is the best way to deal with this scalability, just to create more worker instances for round robin handling?\r\n\r\n\r\n  [1]: https://www.rabbitmq.com/tutorials/tutorial-two-dotnet.html\r\n  [2]: https://i.stack.imgur.com/j3jBC.png\r\n  [3]: https://www.rabbitmq.com/tutorials/tutorial-six-dotnet.html",
            "link": "https://stackoverflow.com/questions/49934981/scaling-slightly-long-running-consumer-work-rabbitmq",
            "title": "Scaling slightly long running Consumer work - RabbitMQ",
            "body": "<p>I'm architecting RabbitMQ into our solution, and I'm curious on how to handle processing and acknowledging messages efficiently, while still performing \"real\" work in the consumer code that can range from 5-10 seconds. (More work than the <a href=\"https://www.rabbitmq.com/tutorials/tutorial-two-dotnet.html\" rel=\"nofollow noreferrer\">samples</a> ever delve into).</p>\n\n<p><a href=\"https://i.stack.imgur.com/j3jBC.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/j3jBC.png\" alt=\"enter image description here\"></a></p>\n\n<p>Above is an example of what I'm aiming to process. A message in my <code>twitter.tweet_cmd_q</code> queue that has all the parameters needed in the message body, for the consumer to make the actual Twitter API request, and save those results to the DB.</p>\n\n<p>However, I'm running into two problems here:</p>\n\n<ol>\n<li>I'll be processing thousands of records a minute - I can't possibly take 5-10 seconds before acknowledging that message. Is it \"normal\" for the consumer code to process all the needed work before acknowledging the message? (i.e. I could see ack'ing the message, and throwing the actual work to be done in another thread for processing. - though this would then require it's own form of \"thread\" management so that the system does undergo too much load).</li>\n<li><p>Would <a href=\"https://www.rabbitmq.com/tutorials/tutorial-six-dotnet.html\" rel=\"nofollow noreferrer\">RPC calls</a> benefit me at all here, in this situation that involves querying the data and saving it to the DB?</p></li>\n<li><p>Is the best way to deal with this scalability, just to create more worker instances for round robin handling?</p></li>\n</ol>\n"
        },
        {
            "tags": [
                "javascript",
                "reactjs"
            ],
            "owner": {
                "reputation": 37,
                "user_id": 8023463,
                "user_type": "registered",
                "accept_rate": 33,
                "profile_image": "https://i.stack.imgur.com/wYe5W.jpg?s=128&g=1",
                "display_name": "jimc3",
                "link": "https://stackoverflow.com/users/8023463/jimc3"
            },
            "is_answered": false,
            "view_count": 14,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1524204032,
            "creation_date": 1524203591,
            "question_id": 49934888,
            "body_markdown": "I just started my journey into the world of React and I love it so far. I finally started to get a grasp of the basics and tried my hand in creating a simple program.\r\nThis program simply takes what you are typing in a text box and displays it live in a header tag. I can&#39;t seem to figure out why this isn&#39;t working. I get the text to appear in the console, but can&#39;t figure out why it isn&#39;t displaying in my &lt;h2&gt; tag. Thank you in advance for the help. Any tips are greatly appreciated.\r\n\r\n\r\n    class Header extends Component{\r\n        render(){\r\n            return(\r\n                &lt;div className = &quot;header&quot;&gt;\r\n                    &lt;h1&gt;Enter Your Name&lt;/h1&gt;\r\n                &lt;/div&gt;\r\n            )\r\n        }\r\n    }\r\n\r\n    class Input extends Component{\r\n        render(){\r\n            return(\r\n                &lt;div className = &quot;input_container&quot;&gt;\r\n                    &lt;input onChange = {this.props.onChange} className = &quot;input_field&quot; type = &quot;text&quot; placeholder = &quot;Name...&quot;&gt;&lt;/input&gt;\r\n                &lt;/div&gt;\r\n            )\r\n        }\r\n    }\r\n\r\n    class App extends Component {\r\n        state = {\r\n            inputValue : &quot;&quot;\r\n        }\r\n    \r\n        onChange = (e) =&gt; {\r\n            console.log(e.target.value)\r\n            this.setState = ({inputValue: e.target.value});\r\n        }\r\n    \r\n        render(){\r\n            return(\r\n                &lt;div&gt;\r\n                &lt;Columns /&gt;\r\n                &lt;Input\r\n                    onChange = {this.onChange}/&gt;\r\n                &lt;h2&gt;{this.state.inputValue}&lt;/h2&gt;\r\n            &lt;/div&gt;\r\n            )\r\n        }\r\n    }",
            "link": "https://stackoverflow.com/questions/49934888/show-text-in-a-header-tag-as-it-is-being-typed-in-an-input-tag",
            "title": "Show text in a header tag as it is being typed in an input tag",
            "body": "<p>I just started my journey into the world of React and I love it so far. I finally started to get a grasp of the basics and tried my hand in creating a simple program.\nThis program simply takes what you are typing in a text box and displays it live in a header tag. I can't seem to figure out why this isn't working. I get the text to appear in the console, but can't figure out why it isn't displaying in my  tag. Thank you in advance for the help. Any tips are greatly appreciated.</p>\n\n<pre><code>class Header extends Component{\n    render(){\n        return(\n            &lt;div className = \"header\"&gt;\n                &lt;h1&gt;Enter Your Name&lt;/h1&gt;\n            &lt;/div&gt;\n        )\n    }\n}\n\nclass Input extends Component{\n    render(){\n        return(\n            &lt;div className = \"input_container\"&gt;\n                &lt;input onChange = {this.props.onChange} className = \"input_field\" type = \"text\" placeholder = \"Name...\"&gt;&lt;/input&gt;\n            &lt;/div&gt;\n        )\n    }\n}\n\nclass App extends Component {\n    state = {\n        inputValue : \"\"\n    }\n\n    onChange = (e) =&gt; {\n        console.log(e.target.value)\n        this.setState = ({inputValue: e.target.value});\n    }\n\n    render(){\n        return(\n            &lt;div&gt;\n            &lt;Columns /&gt;\n            &lt;Input\n                onChange = {this.onChange}/&gt;\n            &lt;h2&gt;{this.state.inputValue}&lt;/h2&gt;\n        &lt;/div&gt;\n        )\n    }\n}\n</code></pre>\n"
        },
        {
            "tags": [
                "c",
                "visual-studio-2013",
                "linked-list"
            ],
            "owner": {
                "reputation": 16,
                "user_id": 9672569,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/b440723321ca533309e3e1ca3b5ffcff?s=128&d=identicon&r=PG&f=1",
                "display_name": "KKK",
                "link": "https://stackoverflow.com/users/9672569/kkk"
            },
            "is_answered": false,
            "view_count": 32,
            "answer_count": 0,
            "score": 3,
            "last_activity_date": 1524204027,
            "creation_date": 1524190048,
            "last_edit_date": 1524204027,
            "question_id": 49932828,
            "body_markdown": "In the following code (that inserts only process names that still not are in list), the trouble is that I&#39;m not able to print all items while the next node is different of `NULL`. I&#39;m catching the exception that says **STATUS_ACCESS_VIOLATION (0xC0000005)**. This trouble is present on two commented lines below.\r\n\r\nHow to fix this?\r\n\r\n    #include &quot;stdafx.h&quot;\r\n    #include &lt;conio.h&gt;\r\n    #include &lt;windows.h&gt;\r\n    #include &lt;Winternl.h&gt;\r\n    \r\n    #pragma comment(lib,&quot;ntdll.lib&quot;)\r\n    \r\n    typedef struct _SYSTEM_PROCESS_INFO\r\n    {\r\n    \tULONG                   NextEntryOffset;\r\n    \tULONG                   NumberOfThreads;\r\n    \tLARGE_INTEGER           Reserved[3];\r\n    \tLARGE_INTEGER           CreateTime;\r\n    \tLARGE_INTEGER           UserTime;\r\n    \tLARGE_INTEGER           KernelTime;\r\n    \tUNICODE_STRING          ImageName;\r\n    \tULONG                   BasePriority;\r\n    \tHANDLE                  ProcessId;\r\n    \tHANDLE                  InheritedFromProcessId;\r\n    }SYSTEM_PROCESS_INFO, *PSYSTEM_PROCESS_INFO;\r\n    \r\n    #pragma region LinkedList\r\n    \r\n    struct test_struct\r\n    {\r\n    \tUNICODE_STRING val;\r\n    \tstruct test_struct *next;\r\n    };\r\n    \r\n    struct test_struct *head = NULL;\r\n    struct test_struct *curr = NULL;\r\n    \r\n    struct test_struct* create_list(UNICODE_STRING val)\r\n    {\r\n    \tprintf(&quot;\\n creating list with headnode as [%wZ]\\n&quot;, &amp;val);\r\n    \tstruct test_struct *ptr = (struct test_struct*)malloc(sizeof(struct test_struct));\r\n    \tif (NULL == ptr)\r\n    \t{\r\n    \t\tprintf(&quot;\\n Node creation failed \\n&quot;);\r\n    \t\treturn NULL;\r\n    \t}\r\n    \tptr-&gt;val = val;\r\n    \tptr-&gt;next = NULL;\r\n    \r\n    \thead = curr = ptr;\r\n    \treturn ptr;\r\n    }\r\n    \r\n    struct test_struct* add_to_list(UNICODE_STRING val, BOOLEAN add_to_end)\r\n    {\r\n    \tif (NULL == head)\r\n    \t{\r\n    \t\treturn (create_list(val));\r\n    \t}\r\n    \r\n    \tif (add_to_end)\r\n    \t\tprintf(&quot;\\n Adding node to end of list with value [%wZ]\\n&quot;, &amp;val);\r\n    \telse\r\n    \t\tprintf(&quot;\\n Adding node to beginning of list with value [%wZ]\\n&quot;, &amp;val);\r\n    \r\n    \tstruct test_struct *ptr = (struct test_struct*)malloc(sizeof(struct test_struct));\r\n    \tif (NULL == ptr)\r\n    \t{\r\n    \t\tprintf(&quot;\\n Node creation failed \\n&quot;);\r\n    \t\treturn NULL;\r\n    \t}\r\n    \tptr-&gt;val = val;\r\n    \tptr-&gt;next = NULL;\r\n    \r\n    \tif (add_to_end)\r\n    \t{\r\n    \t\tcurr-&gt;next = ptr;\r\n    \t\tcurr = ptr;\r\n    \t}\r\n    \telse\r\n    \t{\r\n    \t\tptr-&gt;next = head;\r\n    \t\thead = ptr;\r\n    \t}\r\n    \treturn ptr;\r\n    }\r\n    \r\n    struct test_struct* search_in_list(UNICODE_STRING val, struct test_struct **prev)\r\n    {\r\n    \tBOOLEAN(WINAPI * RtlEqualUnicodeString)(IN PCUNICODE_STRING String1, IN PCUNICODE_STRING String2, IN BOOLEAN CaseInSensitive);\r\n    \tHMODULE ntDll = GetModuleHandleA(&quot;ntdll&quot;);\r\n    \tRtlEqualUnicodeString = (BOOLEAN (WINAPI *)(IN PCUNICODE_STRING String1, IN PCUNICODE_STRING String2, IN BOOLEAN CaseInSensitive))GetProcAddress(ntDll, &quot;RtlEqualUnicodeString&quot;);\r\n    \t\r\n    \tstruct test_struct *ptr = head;\r\n    \tstruct test_struct *tmp = NULL;\r\n    \tBOOLEAN found = FALSE;\r\n    \r\n    \tprintf(&quot;\\n Searching the list for value [%wZ] \\n&quot;, &amp;val);\r\n    \r\n    \twhile (ptr != NULL)\r\n    \t{\r\n    \t\tif (RtlEqualUnicodeString(&amp;ptr-&gt;val, &amp;val, TRUE))\r\n    \t\t{\r\n    \t\t\tfound = TRUE;\r\n    \t\t\tbreak;\r\n    \t\t}\r\n    \t\telse\r\n    \t\t{\r\n    \t\t\ttmp = ptr;\r\n    \t\t\tptr = ptr-&gt;next;\r\n    \t\t}\r\n    \t}\r\n    \r\n    \tif (TRUE == found)\r\n    \t{\r\n    \t\tif (prev)\r\n    \t\t\t*prev = tmp;\r\n    \t\treturn ptr;\r\n    \t}\r\n    \telse\r\n    \t{\r\n    \t\treturn NULL;\r\n    \t}\r\n    }\r\n    \r\n    void dispose(struct test_struct *head)\r\n    {\r\n    \tstruct test_struct *cursor, *tmp;\r\n    \r\n    \tif (head != NULL)\r\n    \t{\r\n    \t\tcursor = head;\r\n    \r\n    \t\twhile (cursor != NULL)\r\n    \t\t{\r\n    \t\t\ttmp = cursor-&gt;next;\r\n    \t\t\t//printf(&quot;Releasing: %wZ \\n&quot;, &amp;cursor-&gt;val);\r\n    \t\t\tfree(cursor);\r\n    \t\t\tcursor = tmp;\r\n    \t\t}\r\n    \t}\r\n    }\r\n    \r\n    int filterException(int code, PEXCEPTION_POINTERS ex) {\r\n    \tprintf(&quot;Exception: %#X\\n&quot;, code);\r\n    \treturn EXCEPTION_EXECUTE_HANDLER;\r\n    }\r\n    \r\n    void print_list(void)\r\n    {\r\n    \tstruct test_struct *ptr = head;\r\n    \r\n    \tprintf(&quot;\\n -------Printing list Start------- \\n&quot;);\r\n    \twhile (ptr != NULL)\r\n    \t{\r\n    \t\t__try\r\n    \t\t{\r\n    \t\t\t//printf(&quot;\\n [%wZ] \\n&quot;, &amp;ptr-&gt;val);\r\n    \t\t\tptr = ptr-&gt;next;\r\n    \t\t}\r\n    \t\t__except (filterException(GetExceptionCode(), GetExceptionInformation())) \r\n    \t\t{\r\n    \t\t\tprintf(&quot;&quot;);\r\n    \t\t}\r\n    \t}\r\n    \tprintf(&quot;\\n -------Printing list End------- \\n&quot;);\r\n    \r\n    \treturn;\r\n    }\r\n    \r\n    #pragma endregion LinkedList\r\n    \r\n    int _tmain(int argc, _TCHAR* argv[])\r\n    {\r\n    \r\n    \tNTSTATUS status;\r\n    \tPVOID buffer;\r\n    \tPSYSTEM_PROCESS_INFO spi;\r\n    \r\n    \tstruct test_struct *ptr = NULL;\r\n    \r\n    \tbuffer = VirtualAlloc(NULL, 1024 * 1024, MEM_COMMIT | MEM_RESERVE, PAGE_READWRITE);\r\n    \r\n    \tif (!buffer)\r\n    \t{\r\n    \t\tprintf(&quot;\\nError: Unable to allocate memory for process list (%d)\\n&quot;, GetLastError());\r\n    \t\treturn -1;\r\n    \t}\r\n    \r\n    \tprintf(&quot;\\nProcess list allocated at address %#X\\n&quot;, buffer);\r\n    \tspi = (PSYSTEM_PROCESS_INFO)buffer;\r\n    \r\n    \tif (!NT_SUCCESS(status = NtQuerySystemInformation(SystemProcessInformation, spi, 1024 * 1024, NULL)))\r\n    \t{\r\n    \t\tprintf(&quot;\\nError: Unable to query process list (%#X)\\n&quot;, status);\r\n    \r\n    \t\tVirtualFree(buffer, 0, MEM_RELEASE);\r\n    \t\treturn -1;\r\n    \t}\r\n    \r\n    \twhile (spi-&gt;NextEntryOffset)\r\n    \t{\r\n    \t\tprintf(&quot;\\nProcess name: %wZ | Process ID: %d\\n&quot;, &amp;spi-&gt;ImageName, spi-&gt;ProcessId);\r\n    \r\n    \t\tptr = search_in_list(spi-&gt;ImageName, NULL);\r\n    \r\n    \t\tif (NULL == ptr)\r\n    \t\t{\r\n    \t\t\tadd_to_list(spi-&gt;ImageName, TRUE);\r\n    \t\t}\r\n    \t\telse\r\n    \t\t{\r\n    \t\t\tprintf(&quot;process already in list \\n&quot;);\r\n    \t\t}\r\n    \r\n    \t\tspi = (PSYSTEM_PROCESS_INFO)((LPBYTE)spi + spi-&gt;NextEntryOffset);\r\n    \t}\r\n    \tVirtualFree(buffer, 0, MEM_RELEASE);\r\n    \r\n    \tprint_list();\r\n    \tdispose(head);\r\n    \r\n    \t_getch();\r\n    \treturn 0;\r\n    }",
            "link": "https://stackoverflow.com/questions/49932828/linked-list-printf-access-violation-while-next-node-null",
            "title": "Linked List: printf() Access Violation while next node != NULL?",
            "body": "<p>In the following code (that inserts only process names that still not are in list), the trouble is that I'm not able to print all items while the next node is different of <code>NULL</code>. I'm catching the exception that says <strong>STATUS_ACCESS_VIOLATION (0xC0000005)</strong>. This trouble is present on two commented lines below.</p>\n\n<p>How to fix this?</p>\n\n<pre><code>#include \"stdafx.h\"\n#include &lt;conio.h&gt;\n#include &lt;windows.h&gt;\n#include &lt;Winternl.h&gt;\n\n#pragma comment(lib,\"ntdll.lib\")\n\ntypedef struct _SYSTEM_PROCESS_INFO\n{\n    ULONG                   NextEntryOffset;\n    ULONG                   NumberOfThreads;\n    LARGE_INTEGER           Reserved[3];\n    LARGE_INTEGER           CreateTime;\n    LARGE_INTEGER           UserTime;\n    LARGE_INTEGER           KernelTime;\n    UNICODE_STRING          ImageName;\n    ULONG                   BasePriority;\n    HANDLE                  ProcessId;\n    HANDLE                  InheritedFromProcessId;\n}SYSTEM_PROCESS_INFO, *PSYSTEM_PROCESS_INFO;\n\n#pragma region LinkedList\n\nstruct test_struct\n{\n    UNICODE_STRING val;\n    struct test_struct *next;\n};\n\nstruct test_struct *head = NULL;\nstruct test_struct *curr = NULL;\n\nstruct test_struct* create_list(UNICODE_STRING val)\n{\n    printf(\"\\n creating list with headnode as [%wZ]\\n\", &amp;val);\n    struct test_struct *ptr = (struct test_struct*)malloc(sizeof(struct test_struct));\n    if (NULL == ptr)\n    {\n        printf(\"\\n Node creation failed \\n\");\n        return NULL;\n    }\n    ptr-&gt;val = val;\n    ptr-&gt;next = NULL;\n\n    head = curr = ptr;\n    return ptr;\n}\n\nstruct test_struct* add_to_list(UNICODE_STRING val, BOOLEAN add_to_end)\n{\n    if (NULL == head)\n    {\n        return (create_list(val));\n    }\n\n    if (add_to_end)\n        printf(\"\\n Adding node to end of list with value [%wZ]\\n\", &amp;val);\n    else\n        printf(\"\\n Adding node to beginning of list with value [%wZ]\\n\", &amp;val);\n\n    struct test_struct *ptr = (struct test_struct*)malloc(sizeof(struct test_struct));\n    if (NULL == ptr)\n    {\n        printf(\"\\n Node creation failed \\n\");\n        return NULL;\n    }\n    ptr-&gt;val = val;\n    ptr-&gt;next = NULL;\n\n    if (add_to_end)\n    {\n        curr-&gt;next = ptr;\n        curr = ptr;\n    }\n    else\n    {\n        ptr-&gt;next = head;\n        head = ptr;\n    }\n    return ptr;\n}\n\nstruct test_struct* search_in_list(UNICODE_STRING val, struct test_struct **prev)\n{\n    BOOLEAN(WINAPI * RtlEqualUnicodeString)(IN PCUNICODE_STRING String1, IN PCUNICODE_STRING String2, IN BOOLEAN CaseInSensitive);\n    HMODULE ntDll = GetModuleHandleA(\"ntdll\");\n    RtlEqualUnicodeString = (BOOLEAN (WINAPI *)(IN PCUNICODE_STRING String1, IN PCUNICODE_STRING String2, IN BOOLEAN CaseInSensitive))GetProcAddress(ntDll, \"RtlEqualUnicodeString\");\n\n    struct test_struct *ptr = head;\n    struct test_struct *tmp = NULL;\n    BOOLEAN found = FALSE;\n\n    printf(\"\\n Searching the list for value [%wZ] \\n\", &amp;val);\n\n    while (ptr != NULL)\n    {\n        if (RtlEqualUnicodeString(&amp;ptr-&gt;val, &amp;val, TRUE))\n        {\n            found = TRUE;\n            break;\n        }\n        else\n        {\n            tmp = ptr;\n            ptr = ptr-&gt;next;\n        }\n    }\n\n    if (TRUE == found)\n    {\n        if (prev)\n            *prev = tmp;\n        return ptr;\n    }\n    else\n    {\n        return NULL;\n    }\n}\n\nvoid dispose(struct test_struct *head)\n{\n    struct test_struct *cursor, *tmp;\n\n    if (head != NULL)\n    {\n        cursor = head;\n\n        while (cursor != NULL)\n        {\n            tmp = cursor-&gt;next;\n            //printf(\"Releasing: %wZ \\n\", &amp;cursor-&gt;val);\n            free(cursor);\n            cursor = tmp;\n        }\n    }\n}\n\nint filterException(int code, PEXCEPTION_POINTERS ex) {\n    printf(\"Exception: %#X\\n\", code);\n    return EXCEPTION_EXECUTE_HANDLER;\n}\n\nvoid print_list(void)\n{\n    struct test_struct *ptr = head;\n\n    printf(\"\\n -------Printing list Start------- \\n\");\n    while (ptr != NULL)\n    {\n        __try\n        {\n            //printf(\"\\n [%wZ] \\n\", &amp;ptr-&gt;val);\n            ptr = ptr-&gt;next;\n        }\n        __except (filterException(GetExceptionCode(), GetExceptionInformation())) \n        {\n            printf(\"\");\n        }\n    }\n    printf(\"\\n -------Printing list End------- \\n\");\n\n    return;\n}\n\n#pragma endregion LinkedList\n\nint _tmain(int argc, _TCHAR* argv[])\n{\n\n    NTSTATUS status;\n    PVOID buffer;\n    PSYSTEM_PROCESS_INFO spi;\n\n    struct test_struct *ptr = NULL;\n\n    buffer = VirtualAlloc(NULL, 1024 * 1024, MEM_COMMIT | MEM_RESERVE, PAGE_READWRITE);\n\n    if (!buffer)\n    {\n        printf(\"\\nError: Unable to allocate memory for process list (%d)\\n\", GetLastError());\n        return -1;\n    }\n\n    printf(\"\\nProcess list allocated at address %#X\\n\", buffer);\n    spi = (PSYSTEM_PROCESS_INFO)buffer;\n\n    if (!NT_SUCCESS(status = NtQuerySystemInformation(SystemProcessInformation, spi, 1024 * 1024, NULL)))\n    {\n        printf(\"\\nError: Unable to query process list (%#X)\\n\", status);\n\n        VirtualFree(buffer, 0, MEM_RELEASE);\n        return -1;\n    }\n\n    while (spi-&gt;NextEntryOffset)\n    {\n        printf(\"\\nProcess name: %wZ | Process ID: %d\\n\", &amp;spi-&gt;ImageName, spi-&gt;ProcessId);\n\n        ptr = search_in_list(spi-&gt;ImageName, NULL);\n\n        if (NULL == ptr)\n        {\n            add_to_list(spi-&gt;ImageName, TRUE);\n        }\n        else\n        {\n            printf(\"process already in list \\n\");\n        }\n\n        spi = (PSYSTEM_PROCESS_INFO)((LPBYTE)spi + spi-&gt;NextEntryOffset);\n    }\n    VirtualFree(buffer, 0, MEM_RELEASE);\n\n    print_list();\n    dispose(head);\n\n    _getch();\n    return 0;\n}\n</code></pre>\n"
        },
        {
            "tags": [
                "python",
                "django",
                "django-rest-framework",
                "django-rest-swagger"
            ],
            "owner": {
                "reputation": 85,
                "user_id": 6289691,
                "user_type": "registered",
                "accept_rate": 50,
                "profile_image": "https://www.gravatar.com/avatar/3b8c87596ee2e75ecceff56501d10660?s=128&d=identicon&r=PG&f=1",
                "display_name": "Naella",
                "link": "https://stackoverflow.com/users/6289691/naella"
            },
            "is_answered": true,
            "view_count": 725,
            "accepted_answer_id": 45326499,
            "answer_count": 4,
            "score": 2,
            "last_activity_date": 1524204021,
            "creation_date": 1500968605,
            "last_edit_date": 1524204021,
            "question_id": 45296939,
            "body_markdown": "I&#39;m working with Django rest framework API, I am trying to make a filter by first_name or by last_name or by both of them.\r\nThis is my **ContactViewSet.py** :\r\n\r\n    class ContactViewSet(viewsets.ModelViewSet):\r\n        queryset = Contact.objects.all()\r\n        serializer_class = ContactSerializer\r\n        filter_backends = (DjangoFilterBackend, )\r\n        filter_fields = (&#39;first_name&#39;, &#39;last_name&#39;)\r\n        lookup_field = &#39;idContact&#39;\r\n\r\n\r\n\r\nMy DRF&#39;s settings : \r\n\r\n    REST_FRAMEWORK = {\r\n        &#39;DEFAULT_FILTER_BACKENDS&#39;: (&#39;django_filters.rest_framework.DjangoFilterBackend&#39;,),\r\n    }\r\n\r\n\r\nMy actuel request url looks like : \r\n\r\n\r\n    http://localhost:8000/api/v1/contacts/?first_name=Clair&amp;last_name=Test\r\n\r\n\r\nBut I&#39;m looking for something like this : \r\n\r\n\r\n    http://localhost:8000/api/v1/contacts/?first_name=Cl**&amp;last_name=Tes**\r\n\r\n\r\nAny help would be appreciated ..",
            "link": "https://stackoverflow.com/questions/45296939/django-filter-backend",
            "title": "Django Filter Backend",
            "body": "<p>I'm working with Django rest framework API, I am trying to make a filter by first_name or by last_name or by both of them.\nThis is my <strong>ContactViewSet.py</strong> :</p>\n\n<pre><code>class ContactViewSet(viewsets.ModelViewSet):\n    queryset = Contact.objects.all()\n    serializer_class = ContactSerializer\n    filter_backends = (DjangoFilterBackend, )\n    filter_fields = ('first_name', 'last_name')\n    lookup_field = 'idContact'\n</code></pre>\n\n<p>My DRF's settings : </p>\n\n<pre><code>REST_FRAMEWORK = {\n    'DEFAULT_FILTER_BACKENDS': ('django_filters.rest_framework.DjangoFilterBackend',),\n}\n</code></pre>\n\n<p>My actuel request url looks like : </p>\n\n<pre><code>http://localhost:8000/api/v1/contacts/?first_name=Clair&amp;last_name=Test\n</code></pre>\n\n<p>But I'm looking for something like this : </p>\n\n<pre><code>http://localhost:8000/api/v1/contacts/?first_name=Cl**&amp;last_name=Tes**\n</code></pre>\n\n<p>Any help would be appreciated ..</p>\n"
        },
        {
            "tags": [
                "cppcms"
            ],
            "owner": {
                "reputation": 6,
                "user_id": 6288639,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/171df9d7194a861574a29e201a35c255?s=128&d=identicon&r=PG&f=1",
                "display_name": "Vandana Chadha",
                "link": "https://stackoverflow.com/users/6288639/vandana-chadha"
            },
            "is_answered": false,
            "view_count": 9,
            "answer_count": 0,
            "score": 1,
            "last_activity_date": 1524204012,
            "creation_date": 1524146012,
            "last_edit_date": 1524204012,
            "question_id": 49922887,
            "body_markdown": "I am using cppcms ver 1.2, and want to include external css and js files as follows:\r\nmedia/css/bootstrap.min.css\r\nBut I get a 404 error for them, even though the files exist in the media folder in my root folder of the app.\r\n\r\nI tried copying the media folder manually to the  CMakeFiles folder. But that didnt work also.\r\n\r\nStill keep getting a 404 error for localhost:8080/media/style.css or any of the other css or js files. \r\n\r\nWhat is the document root of the cppcms, and if it is the root folder where the code is, then why isnt it able to read them?\r\n\r\nMy code is modelled on the examples/message_board sample.\r\n",
            "link": "https://stackoverflow.com/questions/49922887/cppcms-404-error-for-css-and-js-files",
            "title": "cppcms: 404 error for css and js files",
            "body": "<p>I am using cppcms ver 1.2, and want to include external css and js files as follows:\nmedia/css/bootstrap.min.css\nBut I get a 404 error for them, even though the files exist in the media folder in my root folder of the app.</p>\n\n<p>I tried copying the media folder manually to the  CMakeFiles folder. But that didnt work also.</p>\n\n<p>Still keep getting a 404 error for localhost:8080/media/style.css or any of the other css or js files. </p>\n\n<p>What is the document root of the cppcms, and if it is the root folder where the code is, then why isnt it able to read them?</p>\n\n<p>My code is modelled on the examples/message_board sample.</p>\n"
        },
        {
            "tags": [
                "python",
                "big-o"
            ],
            "owner": {
                "reputation": 409,
                "user_id": 6930377,
                "user_type": "registered",
                "accept_rate": 48,
                "profile_image": "https://www.gravatar.com/avatar/72345ae8dcbde9beac98f975d909e84a?s=128&d=identicon&r=PG&f=1",
                "display_name": "Dilli",
                "link": "https://stackoverflow.com/users/6930377/dilli"
            },
            "is_answered": false,
            "view_count": 29,
            "answer_count": 1,
            "score": 1,
            "last_activity_date": 1524204010,
            "creation_date": 1524201365,
            "question_id": 49934465,
            "body_markdown": "Suppose we could access yesterday&#39;s stock prices as a list, where:\r\n\r\nThe indices are the time in minutes past trade opening time, which was 9:30am local time.\r\nThe values are the price in dollars of Apple stock at that time.\r\nSo if the stock cost $500 at 10:30am, stock_prices_yesterday[60] = 500.\r\n\r\nWrite an efficient function that takes stock_prices_yesterday and returns the best profit I could have made from 1 purchase and 1 sale of 1 Apple stock yesterday.\r\n\r\n\r\n\r\nThe solution I came up with:\r\n\r\n    def get_best_stock_price(list):\r\n        first_minimum_value = min(list)\r\n        index_of_the_minimum_value = list.index(first_lowest_value)\r\n        new_list_excluding_all_the_unwanted_items = list[new_list_index:]\r\n        max_new = max(new_list)\r\n        return max_new - first_minimum_value\r\n\r\nThe solution they offered:\r\n\r\n    def get_max_profit(stock_prices_yesterday):\r\n      if len(stock_prices_yesterday) &lt; 2:\r\n          raise ValueError(&#39;Getting a profit requires at least 2 prices&#39;)\r\n      min_price  = stock_prices_yesterday[0]\r\n      max_profit = stock_prices_yesterday[1] - stock_prices_yesterday[0]\r\n      for current_time in xrange(1, len(stock_prices_yesterday)):\r\n          current_price = stock_prices_yesterday[current_time]\r\n          potential_profit = current_price - min_price\r\n          max_profit = max(max_profit, potential_profit)\r\n          min_price  = min(min_price, current_price)\r\n    \r\n      return max_profit\r\n\r\nIs my answer suffice in terms of what they offer? if not how can I improve it. And what is my function lacking?\r\n\r\nThe function they offer includes O(n) time and O(1)O(1) space in the language of Big O notation. ",
            "link": "https://stackoverflow.com/questions/49934465/which-function-is-faster-in-terms-of-time-complexity-space-python",
            "title": "Which function is faster in terms of time complexity/space python",
            "body": "<p>Suppose we could access yesterday's stock prices as a list, where:</p>\n\n<p>The indices are the time in minutes past trade opening time, which was 9:30am local time.\nThe values are the price in dollars of Apple stock at that time.\nSo if the stock cost $500 at 10:30am, stock_prices_yesterday[60] = 500.</p>\n\n<p>Write an efficient function that takes stock_prices_yesterday and returns the best profit I could have made from 1 purchase and 1 sale of 1 Apple stock yesterday.</p>\n\n<p>The solution I came up with:</p>\n\n<pre><code>def get_best_stock_price(list):\n    first_minimum_value = min(list)\n    index_of_the_minimum_value = list.index(first_lowest_value)\n    new_list_excluding_all_the_unwanted_items = list[new_list_index:]\n    max_new = max(new_list)\n    return max_new - first_minimum_value\n</code></pre>\n\n<p>The solution they offered:</p>\n\n<pre><code>def get_max_profit(stock_prices_yesterday):\n  if len(stock_prices_yesterday) &lt; 2:\n      raise ValueError('Getting a profit requires at least 2 prices')\n  min_price  = stock_prices_yesterday[0]\n  max_profit = stock_prices_yesterday[1] - stock_prices_yesterday[0]\n  for current_time in xrange(1, len(stock_prices_yesterday)):\n      current_price = stock_prices_yesterday[current_time]\n      potential_profit = current_price - min_price\n      max_profit = max(max_profit, potential_profit)\n      min_price  = min(min_price, current_price)\n\n  return max_profit\n</code></pre>\n\n<p>Is my answer suffice in terms of what they offer? if not how can I improve it. And what is my function lacking?</p>\n\n<p>The function they offer includes O(n) time and O(1)O(1) space in the language of Big O notation. </p>\n"
        },
        {
            "tags": [
                "tensorflow",
                "tensorflow-estimator"
            ],
            "owner": {
                "reputation": 86,
                "user_id": 8023137,
                "user_type": "registered",
                "accept_rate": 85,
                "profile_image": "https://i.stack.imgur.com/NRwxo.jpg?s=128&g=1",
                "display_name": "Rocket Pingu",
                "link": "https://stackoverflow.com/users/8023137/rocket-pingu"
            },
            "is_answered": false,
            "view_count": 39,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1524203997,
            "creation_date": 1524102987,
            "last_edit_date": 1524203048,
            "question_id": 49911525,
            "body_markdown": "I can train and evalaute a Tensorflow Estimator model without any problems. When I do prediction, this error arises:\r\n\r\n    InvalidArgumentError (see above for traceback): output_shape has incorrect number of elements: 68 should be: 2\r\n    \t [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\r\n\r\nAll of the model functions use the same architecture:\r\n\r\n    def _train_model_fn(features, labels, mode, params):\r\n        features = _network_fn(features, mode, params)\r\n    \r\n        outputs = _get_output(features, params[&quot;output_layer&quot;],\r\n                              params[&quot;num_classes&quot;])\r\n        predictions = {\r\n            &quot;outputs&quot;: outputs\r\n        }\r\n    \r\n        ... # loss initialization and whatnot\r\n\r\n    def _eval_model_fn(features, labels, mode, params):\r\n        features = _network_fn(features, mode, params)\r\n        outputs = _get_output(features, params[&quot;output_layer&quot;], params[&quot;num_classes&quot;])\r\n        predictions = {\r\n            &quot;outputs&quot;: outputs\r\n        }\r\n    \r\n        ... # loss initialization and whatnot\r\n\r\n\r\n    def _predict_model_fn(features, mode, params):\r\n        features = _network_fn(features, mode, params)\r\n        outputs = _get_output(features, params[&quot;output_layer&quot;], params[&quot;num_classes&quot;])\r\n        predictions = {\r\n            &quot;outputs&quot;: outputs\r\n        }\r\n    \r\n        ...\r\n\r\nHere&#39;s the predict code:\r\n\r\n    def predict(params, features, checkpoint_dir):\r\n        estimator = tf.estimator.Estimator(model_fn=_predict_model_fn,\r\n                                           params=params,\r\n                                           model_dir=checkpoint_dir)\r\n        predictions = estimator.predict(input_fn=_input_fn(features))\r\n        for i, p in enumerate(predictions):\r\n            print(i, p)\r\n\r\nI also checked the shapes given every time the input passes a layer when training, and the same thing for predicting. They give the same shapes:\r\n\r\nTraining:\r\n\r\n    conv2d [1, 358, 358, 16]\r\n    max_pool2d [1, 179, 179, 16]\r\n    collapse_to_rnn_dims [1, 179, 2864]\r\n    birnn [1, 179, 64]\r\n\r\nPrediction:\r\n\r\n    conv2d [1, 358, 358, 16]\r\n    max_pool2d [1, 179, 179, 16]\r\n    collapse_to_rnn_dims [1, 179, 2864]\r\n    birnn [1, 179, 64]\r\n\r\nHere are the `SparseTensor`s I passed to `sparse_to_dense`:\r\n\r\nTraining:\r\n\r\n    SparseTensor(indices=Tensor(&quot;CTCBeamSearchDecoder:0&quot;, shape=(?, 2), dtype=int64), values=Tensor(&quot;CTCBeamSearchDecoder:1&quot;, shape=(?,), dtype=int64), dense_shape=Tensor(&quot;CTCBeamSearchDecoder:2&quot;, shape=(2,), dtype=int64))\r\n\r\nEvaluation:\r\n\r\n    SparseTensor(indices=Tensor(&quot;CTCBeamSearchDecoder:0&quot;, shape=(?, 2), dtype=int64), values=Tensor(&quot;CTCBeamSearchDecoder:1&quot;, shape=(?,), dtype=int64), dense_shape=Tensor(&quot;CTCBeamSearchDecoder:2&quot;, shape=(2,), dtype=int64))\r\n\r\nPrediction:\r\n\r\n    SparseTensor(indices=Tensor(&quot;CTCBeamSearchDecoder:0&quot;, shape=(?, 2), dtype=int64), values=Tensor(&quot;CTCBeamSearchDecoder:1&quot;, shape=(?,), dtype=int64), dense_shape=Tensor(&quot;CTCBeamSearchDecoder:2&quot;, shape=(2,), dtype=int64))\r\n\r\n\r\nWhich are all pretty much the same.\r\n\r\nAny reason why this is happening? Shouldn&#39;t the `_predict_model_fn` work given that it follows the same architecture as that of the other `model_fn`s?\r\n\r\nHere&#39;s the full stacktrace:\r\n\r\n    INFO:tensorflow:Using default config.\r\n    INFO:tensorflow:Using config: {&#39;_log_step_count_steps&#39;: 100, &#39;_keep_checkpoint_max&#39;: 5, &#39;_task_type&#39;: &#39;worker&#39;, &#39;_is_chief&#39;: True, &#39;_service&#39;: None, &#39;_save_summary_steps&#39;: 100, &#39;_model_dir&#39;: &#39;checkpoint\\\\model-20180419-150303&#39;, &#39;_task_id&#39;: 0, &#39;_evaluation_master&#39;: &#39;&#39;, &#39;_tf_random_seed&#39;: None, &#39;_cluster_spec&#39;: &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x00000091F58B3080&gt;, &#39;_num_ps_replicas&#39;: 0, &#39;_master&#39;: &#39;&#39;, &#39;_save_checkpoints_secs&#39;: 600, &#39;_session_config&#39;: None, &#39;_save_checkpoints_steps&#39;: None, &#39;_keep_checkpoint_every_n_hours&#39;: 10000, &#39;_global_id_in_cluster&#39;: 0, &#39;_num_worker_replicas&#39;: 1}\r\n    INFO:tensorflow:Calling model_fn.\r\n    INFO:tensorflow:Done calling model_fn.\r\n    INFO:tensorflow:Graph was finalized.\r\n    INFO:tensorflow:Restoring parameters from checkpoint\\model-20180419-150303\\model.ckpt-1\r\n    INFO:tensorflow:Running local_init_op.\r\n    INFO:tensorflow:Done running local_init_op.\r\n    Process Process-2:\r\n    Traceback (most recent call last):\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py&quot;, line 1361, in _do_call\r\n        return fn(*args)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py&quot;, line 1340, in _run_fn\r\n        target_list, status, run_metadata)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py&quot;, line 516, in __exit__\r\n        c_api.TF_GetCode(self.status.status))\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: output_shape has incorrect number of elements: 68 should be: 2\r\n    \t [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\r\n    \r\n    During handling of the above exception, another exception occurred:\r\n    \r\n    Traceback (most recent call last):\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\multiprocessing\\process.py&quot;, line 249, in _bootstrap\r\n        self.run()\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\multiprocessing\\process.py&quot;, line 93, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File &quot;C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\train_ocr.py&quot;, line 42, in evaluate_model\r\n        evaluate(architecture_params, images, labels, checkpoint_dir)\r\n      File &quot;C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py&quot;, line 82, in evaluate\r\n        predict(params, features, checkpoint_dir)\r\n      File &quot;C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py&quot;, line 90, in predict\r\n        for i, p in enumerate(predictions):\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py&quot;, line 492, in predict\r\n        preds_evaluated = mon_sess.run(predictions)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py&quot;, line 546, in run\r\n        run_metadata=run_metadata)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py&quot;, line 1022, in run\r\n        run_metadata=run_metadata)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py&quot;, line 1113, in run\r\n        raise six.reraise(*original_exc_info)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\six.py&quot;, line 693, in reraise\r\n        raise value\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py&quot;, line 1098, in run\r\n        return self._sess.run(*args, **kwargs)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py&quot;, line 1170, in run\r\n        run_metadata=run_metadata)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py&quot;, line 950, in run\r\n        return self._sess.run(*args, **kwargs)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py&quot;, line 905, in run\r\n        run_metadata_ptr)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py&quot;, line 1137, in _run\r\n        feed_dict_tensor, options, run_metadata)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py&quot;, line 1355, in _do_run\r\n        options, run_metadata)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py&quot;, line 1374, in _do_call\r\n        raise type(e)(node_def, op, message)\r\n    tensorflow.python.framework.errors_impl.InvalidArgumentError: output_shape has incorrect number of elements: 68 should be: 2\r\n    \t [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\r\n    \r\n    Caused by op &#39;output&#39;, defined at:\r\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\multiprocessing\\spawn.py&quot;, line 106, in spawn_main\r\n        exitcode = _main(fd)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\multiprocessing\\spawn.py&quot;, line 119, in _main\r\n        return self._bootstrap()\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\multiprocessing\\process.py&quot;, line 249, in _bootstrap\r\n        self.run()\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\multiprocessing\\process.py&quot;, line 93, in run\r\n        self._target(*self._args, **self._kwargs)\r\n      File &quot;C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\train_ocr.py&quot;, line 42, in evaluate_model\r\n        evaluate(architecture_params, images, labels, checkpoint_dir)\r\n      File &quot;C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py&quot;, line 82, in evaluate\r\n        predict(params, features, checkpoint_dir)\r\n      File &quot;C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py&quot;, line 90, in predict\r\n        for i, p in enumerate(predictions):\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py&quot;, line 479, in predict\r\n        features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py&quot;, line 793, in _call_model_fn\r\n        model_fn_results = self._model_fn(features=features, **kwargs)\r\n      File &quot;C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py&quot;, line 217, in _predict_model_fn\r\n        outputs = _get_output(features, params[&quot;output_layer&quot;], params[&quot;num_classes&quot;])\r\n      File &quot;C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py&quot;, line 134, in _get_output\r\n        return _sparse_to_dense(decoded, name=&quot;output&quot;)\r\n      File &quot;C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py&quot;, line 38, in _sparse_to_dense\r\n        name=name)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py&quot;, line 791, in sparse_to_dense\r\n        name=name)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_sparse_ops.py&quot;, line 2401, in _sparse_to_dense\r\n        name=name)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py&quot;, line 787, in _apply_op_helper\r\n        op_def=op_def)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py&quot;, line 3271, in create_op\r\n        op_def=op_def)\r\n      File &quot;C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py&quot;, line 1650, in __init__\r\n        self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n    \r\n    InvalidArgumentError (see above for traceback): output_shape has incorrect number of elements: 68 should be: 2\r\n    \t [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\r\n\r\n**Update**\r\n\r\nI tried using the same architecture in a different training run, I encountered a different shap error:\r\n\r\n    InvalidArgumentError (see above for traceback): output_shape has incorrect number of elements: 69 should be: 2\r\n    \t [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=&quot;/job:localhost/replica:0/task:0/device:CPU:0&quot;](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\r\n\r\nApparently, the problem seems to lie in the `ctc_beam_search_decoder`. Switching to `ctc_greedy_decoder` doesn&#39;t help either. Why is it doing this?\r\n\r\n**More updates**\r\n\r\nI have uploaded the reproducible example: https://github.com/selcouthlyBlue/ShapeErrorReproduce\r\n",
            "link": "https://stackoverflow.com/questions/49911525/estimator-predict-has-shape-issues",
            "title": "Estimator.predict() has Shape Issues?",
            "body": "<p>I can train and evalaute a Tensorflow Estimator model without any problems. When I do prediction, this error arises:</p>\n\n<pre><code>InvalidArgumentError (see above for traceback): output_shape has incorrect number of elements: 68 should be: 2\n     [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\n</code></pre>\n\n<p>All of the model functions use the same architecture:</p>\n\n<pre><code>def _train_model_fn(features, labels, mode, params):\n    features = _network_fn(features, mode, params)\n\n    outputs = _get_output(features, params[\"output_layer\"],\n                          params[\"num_classes\"])\n    predictions = {\n        \"outputs\": outputs\n    }\n\n    ... # loss initialization and whatnot\n\ndef _eval_model_fn(features, labels, mode, params):\n    features = _network_fn(features, mode, params)\n    outputs = _get_output(features, params[\"output_layer\"], params[\"num_classes\"])\n    predictions = {\n        \"outputs\": outputs\n    }\n\n    ... # loss initialization and whatnot\n\n\ndef _predict_model_fn(features, mode, params):\n    features = _network_fn(features, mode, params)\n    outputs = _get_output(features, params[\"output_layer\"], params[\"num_classes\"])\n    predictions = {\n        \"outputs\": outputs\n    }\n\n    ...\n</code></pre>\n\n<p>Here's the predict code:</p>\n\n<pre><code>def predict(params, features, checkpoint_dir):\n    estimator = tf.estimator.Estimator(model_fn=_predict_model_fn,\n                                       params=params,\n                                       model_dir=checkpoint_dir)\n    predictions = estimator.predict(input_fn=_input_fn(features))\n    for i, p in enumerate(predictions):\n        print(i, p)\n</code></pre>\n\n<p>I also checked the shapes given every time the input passes a layer when training, and the same thing for predicting. They give the same shapes:</p>\n\n<p>Training:</p>\n\n<pre><code>conv2d [1, 358, 358, 16]\nmax_pool2d [1, 179, 179, 16]\ncollapse_to_rnn_dims [1, 179, 2864]\nbirnn [1, 179, 64]\n</code></pre>\n\n<p>Prediction:</p>\n\n<pre><code>conv2d [1, 358, 358, 16]\nmax_pool2d [1, 179, 179, 16]\ncollapse_to_rnn_dims [1, 179, 2864]\nbirnn [1, 179, 64]\n</code></pre>\n\n<p>Here are the <code>SparseTensor</code>s I passed to <code>sparse_to_dense</code>:</p>\n\n<p>Training:</p>\n\n<pre><code>SparseTensor(indices=Tensor(\"CTCBeamSearchDecoder:0\", shape=(?, 2), dtype=int64), values=Tensor(\"CTCBeamSearchDecoder:1\", shape=(?,), dtype=int64), dense_shape=Tensor(\"CTCBeamSearchDecoder:2\", shape=(2,), dtype=int64))\n</code></pre>\n\n<p>Evaluation:</p>\n\n<pre><code>SparseTensor(indices=Tensor(\"CTCBeamSearchDecoder:0\", shape=(?, 2), dtype=int64), values=Tensor(\"CTCBeamSearchDecoder:1\", shape=(?,), dtype=int64), dense_shape=Tensor(\"CTCBeamSearchDecoder:2\", shape=(2,), dtype=int64))\n</code></pre>\n\n<p>Prediction:</p>\n\n<pre><code>SparseTensor(indices=Tensor(\"CTCBeamSearchDecoder:0\", shape=(?, 2), dtype=int64), values=Tensor(\"CTCBeamSearchDecoder:1\", shape=(?,), dtype=int64), dense_shape=Tensor(\"CTCBeamSearchDecoder:2\", shape=(2,), dtype=int64))\n</code></pre>\n\n<p>Which are all pretty much the same.</p>\n\n<p>Any reason why this is happening? Shouldn't the <code>_predict_model_fn</code> work given that it follows the same architecture as that of the other <code>model_fn</code>s?</p>\n\n<p>Here's the full stacktrace:</p>\n\n<pre><code>INFO:tensorflow:Using default config.\nINFO:tensorflow:Using config: {'_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_service': None, '_save_summary_steps': 100, '_model_dir': 'checkpoint\\\\model-20180419-150303', '_task_id': 0, '_evaluation_master': '', '_tf_random_seed': None, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x00000091F58B3080&gt;, '_num_ps_replicas': 0, '_master': '', '_save_checkpoints_secs': 600, '_session_config': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_global_id_in_cluster': 0, '_num_worker_replicas': 1}\nINFO:tensorflow:Calling model_fn.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from checkpoint\\model-20180419-150303\\model.ckpt-1\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nProcess Process-2:\nTraceback (most recent call last):\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1361, in _do_call\n    return fn(*args)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1340, in _run_fn\n    target_list, status, run_metadata)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\", line 516, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.InvalidArgumentError: output_shape has incorrect number of elements: 68 should be: 2\n     [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\multiprocessing\\process.py\", line 249, in _bootstrap\n    self.run()\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\multiprocessing\\process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\train_ocr.py\", line 42, in evaluate_model\n    evaluate(architecture_params, images, labels, checkpoint_dir)\n  File \"C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py\", line 82, in evaluate\n    predict(params, features, checkpoint_dir)\n  File \"C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py\", line 90, in predict\n    for i, p in enumerate(predictions):\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 492, in predict\n    preds_evaluated = mon_sess.run(predictions)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 546, in run\n    run_metadata=run_metadata)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1022, in run\n    run_metadata=run_metadata)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1113, in run\n    raise six.reraise(*original_exc_info)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\six.py\", line 693, in reraise\n    raise value\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1098, in run\n    return self._sess.run(*args, **kwargs)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 1170, in run\n    run_metadata=run_metadata)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\", line 950, in run\n    return self._sess.run(*args, **kwargs)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 905, in run\n    run_metadata_ptr)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1137, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1355, in _do_run\n    options, run_metadata)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1374, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: output_shape has incorrect number of elements: 68 should be: 2\n     [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\n\nCaused by op 'output', defined at:\n  File \"&lt;string&gt;\", line 1, in &lt;module&gt;\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 106, in spawn_main\n    exitcode = _main(fd)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\multiprocessing\\spawn.py\", line 119, in _main\n    return self._bootstrap()\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\multiprocessing\\process.py\", line 249, in _bootstrap\n    self.run()\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\multiprocessing\\process.py\", line 93, in run\n    self._target(*self._args, **self._kwargs)\n  File \"C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\train_ocr.py\", line 42, in evaluate_model\n    evaluate(architecture_params, images, labels, checkpoint_dir)\n  File \"C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py\", line 82, in evaluate\n    predict(params, features, checkpoint_dir)\n  File \"C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py\", line 90, in predict\n    for i, p in enumerate(predictions):\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 479, in predict\n    features, None, model_fn_lib.ModeKeys.PREDICT, self.config)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 793, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py\", line 217, in _predict_model_fn\n    outputs = _get_output(features, params[\"output_layer\"], params[\"num_classes\"])\n  File \"C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py\", line 134, in _get_output\n    return _sparse_to_dense(decoded, name=\"output\")\n  File \"C:\\Users\\asus.11\\Documents\\Optimized_OCR\\trainer\\backend\\tf\\experiment_ops.py\", line 38, in _sparse_to_dense\n    name=name)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\sparse_ops.py\", line 791, in sparse_to_dense\n    name=name)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_sparse_ops.py\", line 2401, in _sparse_to_dense\n    name=name)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3271, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\asus.11\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1650, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): output_shape has incorrect number of elements: 68 should be: 2\n     [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\n</code></pre>\n\n<p><strong>Update</strong></p>\n\n<p>I tried using the same architecture in a different training run, I encountered a different shap error:</p>\n\n<pre><code>InvalidArgumentError (see above for traceback): output_shape has incorrect number of elements: 69 should be: 2\n     [[Node: output = SparseToDense[T=DT_INT32, Tindices=DT_INT32, validate_indices=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](ToInt32, ToInt32_1, ToInt32_2, bidirectional_rnn/bidirectional_rnn/fw/fw/time)]]\n</code></pre>\n\n<p>Apparently, the problem seems to lie in the <code>ctc_beam_search_decoder</code>. Switching to <code>ctc_greedy_decoder</code> doesn't help either. Why is it doing this?</p>\n\n<p><strong>More updates</strong></p>\n\n<p>I have uploaded the reproducible example: <a href=\"https://github.com/selcouthlyBlue/ShapeErrorReproduce\" rel=\"nofollow noreferrer\">https://github.com/selcouthlyBlue/ShapeErrorReproduce</a></p>\n"
        },
        {
            "tags": [
                "algorithm",
                "ionic-framework",
                "tinder"
            ],
            "owner": {
                "reputation": 235,
                "user_id": 3024827,
                "user_type": "registered",
                "accept_rate": 33,
                "profile_image": "https://graph.facebook.com/1275986857/picture?type=large",
                "display_name": "user3024827",
                "link": "https://stackoverflow.com/users/3024827/user3024827"
            },
            "is_answered": false,
            "view_count": 27,
            "closed_date": 1524214624,
            "answer_count": 0,
            "score": -1,
            "last_activity_date": 1524203985,
            "creation_date": 1524203985,
            "question_id": 49934974,
            "body_markdown": "I want to build a tinder style matching system for another topic.\r\n\r\nWhen you swipe left to someone, tider won&#39;t show that person to your again. How does it make sure this happens.\r\n\r\nDoes tinder store every &#39;no&#39; on a user record as an id, and this ID is looked for when a query is made. If it finds the ID it will filter it out.\r\n\r\nThis approach seems unefficient. Is there a better way an yone could theorise?\r\n\r\n",
            "link": "https://stackoverflow.com/questions/49934974/tinder-style-match-system",
            "closed_reason": "too broad",
            "title": "Tinder style match system",
            "body": "<p>I want to build a tinder style matching system for another topic.</p>\n\n<p>When you swipe left to someone, tider won't show that person to your again. How does it make sure this happens.</p>\n\n<p>Does tinder store every 'no' on a user record as an id, and this ID is looked for when a query is made. If it finds the ID it will filter it out.</p>\n\n<p>This approach seems unefficient. Is there a better way an yone could theorise?</p>\n"
        },
        {
            "tags": [
                "c++"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 8230572,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/5ca89642bfb535488509ed5bc867bdde?s=128&d=identicon&r=PG&f=1",
                "display_name": "Divya",
                "link": "https://stackoverflow.com/users/8230572/divya"
            },
            "is_answered": false,
            "view_count": 34,
            "closed_date": 1524203981,
            "answer_count": 0,
            "score": -5,
            "last_activity_date": 1524203970,
            "creation_date": 1524202815,
            "last_edit_date": 1524203970,
            "question_id": 49934735,
            "body_markdown": "I am doing a code review where the dev is trying to search a value in the container..\r\n\r\n    std::vector&lt;int&gt; indexOfNumber;\r\n    int numberToBesearched ;\r\n    \r\n    if(indexOfNumber.end() != std::find(indexOfNumber.begin(), indexOfNumber.end(), numberToBesearched ));\r\n\r\nI think this is rong and should be reversed .\r\nPlease suggest whether this is an expected way of doing this.. Will not be an expensive operation on a huge container moving the iterator too and fro.. ",
            "link": "https://stackoverflow.com/questions/49934735/correct-way-of-writing-a-code-for-searching-a-container",
            "closed_reason": "too broad",
            "title": "Correct way of writing a code for searching a container",
            "body": "<p>I am doing a code review where the dev is trying to search a value in the container..</p>\n\n<pre><code>std::vector&lt;int&gt; indexOfNumber;\nint numberToBesearched ;\n\nif(indexOfNumber.end() != std::find(indexOfNumber.begin(), indexOfNumber.end(), numberToBesearched ));\n</code></pre>\n\n<p>I think this is rong and should be reversed .\nPlease suggest whether this is an expected way of doing this.. Will not be an expensive operation on a huge container moving the iterator too and fro.. </p>\n"
        },
        {
            "tags": [
                "java",
                "scala",
                "apache-spark",
                "streaming"
            ],
            "owner": {
                "reputation": 5,
                "user_id": 5868625,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/62943da80e6fe9e5409d93d0d4371d23?s=128&d=identicon&r=PG&f=1",
                "display_name": "erikejan",
                "link": "https://stackoverflow.com/users/5868625/erikejan"
            },
            "is_answered": false,
            "view_count": 33,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1524203961,
            "creation_date": 1522955345,
            "last_edit_date": 1524203961,
            "question_id": 49679768,
            "body_markdown": "I&#39;m new to Apache Spark and currently working on a Structured Streaming pipeline. In the middle of the data processing I need to do a bit of finnicky manipulation that requires that _all_ of the data (so far) is present. The amount of data has been heavily reduced at this point in the pipeline and performing a `.collect()`-like action will not be a bottleneck. The operation I need to perform is basically putting all remaining elements in a HashSet and doing a series of tricky existence checks. After this, I need to &quot;re-enter&quot; the streaming-pipeline to perform various writes to csv-files.\r\n\r\nHowever, attempting to perform `collect()` on a streaming pipeline understandably results in an error message. Below is a barebones (and stupid) example that illustrates my problem:\r\n\r\n    // imports ...\r\n\r\n    val spark = SparkSession.builder\r\n                            .appName(&quot;StructuredNetworkWordCount&quot;)\r\n                            .getOrCreate()\r\n    val lines = spark.readStream\r\n                     .format(&quot;socket&quot;)\r\n                     .option(&quot;host&quot;, &quot;localhost&quot;)\r\n                     .option(&quot;port&quot;, 4444)\r\n                     .load()\r\n\r\n    import spark.implicits._\r\n    \r\n    // Split the lines into words\r\n    val words = lines.as[String].flatMap(_.split(&quot; &quot;))\r\n\r\n    // Won&#39;t work in a streaming context\r\n    val wordList = words.collectAsList()\r\n\r\n    // Perform some operations on the collected() data\r\n    val numWords = wordList.size\r\n    val doubledNum = numWords * 2\r\n    \r\n    // Somehow output doubledNum\r\n    val query = wordCounts.writeStream\r\n                          .outputMode(&quot;complete&quot;)\r\n                          .format(&quot;console&quot;)\r\n                          .start()\r\n\r\n    query.awaitTermination()\r\n\r\nAs I said, this will definitely not work, but illustrates my problem. I need to perform a `collect()`-like action _in the middle of every microbatch_ in order to have simultaneous access to all data that is left. How would I go about doing this? Are accumulators the only way to access all the cumulative data in all partitions in the middle of a streaming pipeline?\r\n\r\nThanks!",
            "link": "https://stackoverflow.com/questions/49679768/applying-collect-to-a-apache-spark-structured-streaming-dataset",
            "title": "Applying collect() to a Apache Spark structured streaming Dataset",
            "body": "<p>I'm new to Apache Spark and currently working on a Structured Streaming pipeline. In the middle of the data processing I need to do a bit of finnicky manipulation that requires that <em>all</em> of the data (so far) is present. The amount of data has been heavily reduced at this point in the pipeline and performing a <code>.collect()</code>-like action will not be a bottleneck. The operation I need to perform is basically putting all remaining elements in a HashSet and doing a series of tricky existence checks. After this, I need to \"re-enter\" the streaming-pipeline to perform various writes to csv-files.</p>\n\n<p>However, attempting to perform <code>collect()</code> on a streaming pipeline understandably results in an error message. Below is a barebones (and stupid) example that illustrates my problem:</p>\n\n<pre><code>// imports ...\n\nval spark = SparkSession.builder\n                        .appName(\"StructuredNetworkWordCount\")\n                        .getOrCreate()\nval lines = spark.readStream\n                 .format(\"socket\")\n                 .option(\"host\", \"localhost\")\n                 .option(\"port\", 4444)\n                 .load()\n\nimport spark.implicits._\n\n// Split the lines into words\nval words = lines.as[String].flatMap(_.split(\" \"))\n\n// Won't work in a streaming context\nval wordList = words.collectAsList()\n\n// Perform some operations on the collected() data\nval numWords = wordList.size\nval doubledNum = numWords * 2\n\n// Somehow output doubledNum\nval query = wordCounts.writeStream\n                      .outputMode(\"complete\")\n                      .format(\"console\")\n                      .start()\n\nquery.awaitTermination()\n</code></pre>\n\n<p>As I said, this will definitely not work, but illustrates my problem. I need to perform a <code>collect()</code>-like action <em>in the middle of every microbatch</em> in order to have simultaneous access to all data that is left. How would I go about doing this? Are accumulators the only way to access all the cumulative data in all partitions in the middle of a streaming pipeline?</p>\n\n<p>Thanks!</p>\n"
        },
        {
            "tags": [
                "hyperledger",
                "hyperledger-explorer"
            ],
            "owner": {
                "reputation": 61,
                "user_id": 6339960,
                "user_type": "registered",
                "profile_image": "https://lh4.googleusercontent.com/-Nrit3el3TBA/AAAAAAAAAAI/AAAAAAAAAB8/Z4ot0NGedq8/photo.jpg?sz=128",
                "display_name": "Christian Lim",
                "link": "https://stackoverflow.com/users/6339960/christian-lim"
            },
            "is_answered": false,
            "view_count": 43,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1524203959,
            "creation_date": 1523870751,
            "question_id": 49853848,
            "body_markdown": "I recently installed Hyperledger Explorer. When running Explorer, it returns nothing to the browser and gives the error:\r\n\r\n    postgres://hppoc:password@127.0.0.1:5432/fabricexplorer\r\n    Please open web browser to access ：http://localhost:8080/\r\n    [2018-04-16 08:15:18.542] [ERROR] Query - Error: No identity has been assigned to this client\r\n        at Client._getSigningIdentity (/home/ubuntu/blockchain-explorer/node_modules/fabric-client/lib/Client.js:1206:11)\r\n        at Channel.queryInfo (/home/ubuntu/blockchain-explorer/node_modules/fabric-client/lib/Channel.js:896:36)\r\n        at helper.getOrgAdmin.then (/home/ubuntu/blockchain-explorer/app/query.js:98:18)\r\n        at &lt;anonymous&gt;\r\n\r\nI tried to `console.log` the output of `blockchain-explorer/node_modules/fabric-client/lib/Client.js:1206:11` indeed the `admin` variable is `undefined`\r\n\r\nIt&#39;s pretty weird since I installed Composer before this and it runs perfectly fine. All the`crypto-config` uses the default settings provided by the Composer example.\r\n\r\nVersions (pretty much latest stable version):\r\n\r\n* OS: Ubuntu 16.04 LTS\r\n* Docker: 18.03.0-ce\r\n* Node: v8.11.1\r\n* Hyperledger Fabric: 1.1.0 \r\n* Hyperledger Composer: 0.19\r\n\r\nExplorer `config.json` pretty much default, no TLS:\r\n\r\n    {\r\n        &quot;network-config&quot;: {\r\n                &quot;org1&quot;: {\r\n                        &quot;name&quot;: &quot;hlfv1&quot;,\r\n                        &quot;mspid&quot;: &quot;Org1MSP&quot;,\r\n                        &quot;peer1&quot;: {\r\n                                &quot;requests&quot;: &quot;grpc://127.0.0.1:7051&quot;,\r\n                                &quot;events&quot;: &quot;grpc://127.0.0.1:7053&quot;,\r\n                                &quot;server-hostname&quot;: &quot;peer0.org1.example.com&quot;\r\n                        },\r\n                        &quot;admin&quot;: {\r\n                                &quot;key&quot;: &quot;/home/ubuntu/fabric-tools/fabric-scripts/hlfv11/composer/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp/keystore&quot;,\r\n                                &quot;cert&quot;: &quot;/home/ubuntu/fabric-tools/fabric-scripts/hlfv11/composer/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp/signcerts&quot;\r\n                        }\r\n                }\r\n        },\r\n        &quot;host&quot;: &quot;localhost&quot;,\r\n        &quot;port&quot;: &quot;8080&quot;,\r\n        &quot;channel&quot;: &quot;composerchannel&quot;,\r\n        &quot;keyValueStore&quot;: &quot;/tmp/fabric-client-kvs&quot;,\r\n        &quot;eventWaitTime&quot;: &quot;30000&quot;,\r\n        &quot;pg&quot;: {\r\n                &quot;host&quot;: &quot;127.0.0.1&quot;,\r\n                &quot;port&quot;: &quot;5432&quot;,\r\n                &quot;database&quot;: &quot;fabricexplorer&quot;,\r\n                &quot;username&quot;: &quot;hppoc&quot;,\r\n                &quot;passwd&quot;: &quot;password&quot;\r\n        },\r\n        &quot;license&quot;: &quot;Apache-2.0&quot;\r\n    }\r\n\r\nAnything I missed / hints? Thanks beforehand.",
            "link": "https://stackoverflow.com/questions/49853848/hyperledger-explorer-is-empty-with-query-error-no-identity-has-been-assigned",
            "title": "Hyperledger Explorer is empty with Query - Error: No identity has been assigned to this client",
            "body": "<p>I recently installed Hyperledger Explorer. When running Explorer, it returns nothing to the browser and gives the error:</p>\n\n<pre><code>postgres://hppoc:password@127.0.0.1:5432/fabricexplorer\nPlease open web browser to access ：http://localhost:8080/\n[2018-04-16 08:15:18.542] [ERROR] Query - Error: No identity has been assigned to this client\n    at Client._getSigningIdentity (/home/ubuntu/blockchain-explorer/node_modules/fabric-client/lib/Client.js:1206:11)\n    at Channel.queryInfo (/home/ubuntu/blockchain-explorer/node_modules/fabric-client/lib/Channel.js:896:36)\n    at helper.getOrgAdmin.then (/home/ubuntu/blockchain-explorer/app/query.js:98:18)\n    at &lt;anonymous&gt;\n</code></pre>\n\n<p>I tried to <code>console.log</code> the output of <code>blockchain-explorer/node_modules/fabric-client/lib/Client.js:1206:11</code> indeed the <code>admin</code> variable is <code>undefined</code></p>\n\n<p>It's pretty weird since I installed Composer before this and it runs perfectly fine. All the<code>crypto-config</code> uses the default settings provided by the Composer example.</p>\n\n<p>Versions (pretty much latest stable version):</p>\n\n<ul>\n<li>OS: Ubuntu 16.04 LTS</li>\n<li>Docker: 18.03.0-ce</li>\n<li>Node: v8.11.1</li>\n<li>Hyperledger Fabric: 1.1.0 </li>\n<li>Hyperledger Composer: 0.19</li>\n</ul>\n\n<p>Explorer <code>config.json</code> pretty much default, no TLS:</p>\n\n<pre><code>{\n    \"network-config\": {\n            \"org1\": {\n                    \"name\": \"hlfv1\",\n                    \"mspid\": \"Org1MSP\",\n                    \"peer1\": {\n                            \"requests\": \"grpc://127.0.0.1:7051\",\n                            \"events\": \"grpc://127.0.0.1:7053\",\n                            \"server-hostname\": \"peer0.org1.example.com\"\n                    },\n                    \"admin\": {\n                            \"key\": \"/home/ubuntu/fabric-tools/fabric-scripts/hlfv11/composer/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp/keystore\",\n                            \"cert\": \"/home/ubuntu/fabric-tools/fabric-scripts/hlfv11/composer/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp/signcerts\"\n                    }\n            }\n    },\n    \"host\": \"localhost\",\n    \"port\": \"8080\",\n    \"channel\": \"composerchannel\",\n    \"keyValueStore\": \"/tmp/fabric-client-kvs\",\n    \"eventWaitTime\": \"30000\",\n    \"pg\": {\n            \"host\": \"127.0.0.1\",\n            \"port\": \"5432\",\n            \"database\": \"fabricexplorer\",\n            \"username\": \"hppoc\",\n            \"passwd\": \"password\"\n    },\n    \"license\": \"Apache-2.0\"\n}\n</code></pre>\n\n<p>Anything I missed / hints? Thanks beforehand.</p>\n"
        },
        {
            "tags": [
                "ubuntu-14.04",
                "ubuntu-16.04",
                "ubuntu-server"
            ],
            "owner": {
                "reputation": 135,
                "user_id": 4809070,
                "user_type": "registered",
                "accept_rate": 87,
                "profile_image": "https://www.gravatar.com/avatar/de25dab19b78af326cc9a7734ff89ee1?s=128&d=identicon&r=PG&f=1",
                "display_name": "Athi",
                "link": "https://stackoverflow.com/users/4809070/athi"
            },
            "is_answered": false,
            "view_count": 6,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524203951,
            "creation_date": 1524203951,
            "question_id": 49934966,
            "body_markdown": "I use UBUNTU server version 17.4. Using VMware I accessed the server. I configured the Network interface.\r\nhttp://10.1.3.163:8080/ is not reached in client side.\r\nUsing SQLyog, I cant access the MYSQL. I checked Port 8080 &amp; 3306 is active connections in the server and I also checked the TELNET and it shows error like\r\n\r\n    telnet: unable to connect the remote host: invalid argument\r\n\r\nI granted privileges for all users in mysql\r\n\r\nI think issues to execute map address from server or map address from client.\r\n\r\nPlease help me to solve the issue.",
            "link": "https://stackoverflow.com/questions/49934966/cant-accessible-from-ubuntu-server-to-client",
            "title": "Cant accessible from Ubuntu Server to Client",
            "body": "<p>I use UBUNTU server version 17.4. Using VMware I accessed the server. I configured the Network interface.\n<a href=\"http://10.1.3.163:8080/\" rel=\"nofollow noreferrer\">http://10.1.3.163:8080/</a> is not reached in client side.\nUsing SQLyog, I cant access the MYSQL. I checked Port 8080 &amp; 3306 is active connections in the server and I also checked the TELNET and it shows error like</p>\n\n<pre><code>telnet: unable to connect the remote host: invalid argument\n</code></pre>\n\n<p>I granted privileges for all users in mysql</p>\n\n<p>I think issues to execute map address from server or map address from client.</p>\n\n<p>Please help me to solve the issue.</p>\n"
        },
        {
            "tags": [
                "node.js",
                "bigdata"
            ],
            "owner": {
                "reputation": 17,
                "user_id": 9539529,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/4b9ab21918e08e353280053a7404fbdf?s=128&d=identicon&r=PG&f=1",
                "display_name": "Ajay Poriya",
                "link": "https://stackoverflow.com/users/9539529/ajay-poriya"
            },
            "is_answered": false,
            "view_count": 11,
            "closed_date": 1524206051,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524203949,
            "creation_date": 1524203949,
            "question_id": 49934964,
            "body_markdown": "I wanted to learn big data I have quite good knowledge of Nodejs. Is there any possible way to learn big data with nodejs ? In java we have options like Hadoop and Scala.",
            "link": "https://stackoverflow.com/questions/49934964/is-there-any-possible-way-to-configure-node-with-bigdata",
            "closed_reason": "off-topic",
            "title": "Is there any possible way to configure Node with BIGData?",
            "body": "<p>I wanted to learn big data I have quite good knowledge of Nodejs. Is there any possible way to learn big data with nodejs ? In java we have options like Hadoop and Scala.</p>\n"
        },
        {
            "tags": [
                "amazon-s3",
                "unicode",
                "aws-sdk"
            ],
            "owner": {
                "reputation": 529,
                "user_id": 1882090,
                "user_type": "registered",
                "accept_rate": 60,
                "profile_image": "https://i.stack.imgur.com/ygKG0.jpg?s=128&g=1",
                "display_name": "fjanisze",
                "link": "https://stackoverflow.com/users/1882090/fjanisze"
            },
            "is_answered": false,
            "view_count": 36,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524203941,
            "creation_date": 1523638369,
            "last_edit_date": 1524203941,
            "question_id": 49821811,
            "body_markdown": "How to decode unicode object names from S3 using the Windows C++ AWS SDK?\r\n\r\nI&#39;m facing the weird issue of getting wrongly decoded object names when listing buckets with files having unicode names. What I do is that I run &#39;mbstowcs_s&#39; or &#39;MultiByteToWideChar&#39; (Attempted both) to decode the object names that I get from S3.\r\n\r\nAs I understand, the object names coming from S3 are in multibyte format so I convert it to wide chars string using these converting functions, this seems to not work and I always get some trash names while listing unicode files, but names which use standard symbols from the ASCII table are properly decoded.\r\n\r\nFor example, having the file &#39;ธงไชย แม็คอินไตย์.txt&#39; on my bucket, if I attempt to list it from S3 using ListObjectV2Request and I attempt to convert its name to wide chars using mbstowcs_s I always get some noise instead of a valid name. The same method works perfectly with ASCII name.\r\n\r\nAny suggestion?\r\n\r\n**EDIT 1:**\r\n\r\nHere are a bit more details on how I&#39;m attempting to decode the names (cutting off all the not relevant parts in order to focus on the decoding steps)\r\n\r\n1) First I get the list of objects from S3:\r\n    \r\n    pListObjectsOutcome = new Aws::S3::Model::ListObjectsV2Outcome(pS3Client-&gt;ListObjectsV2(*pObjectsRequest));\r\n    pObjectList = new (std::nothrow) Aws::Vector&lt;Aws::S3::Model::Object&gt;(pListObjectsOutcome-&gt;GetResult().GetContents());\r\n\r\n2) Iterate over the list and extract the names:\r\n\r\n    Aws::Vector&lt;Aws::S3::Model::Object&gt;::iterator it = pObjectList-&gt;begin();\r\n    //loop for each name\r\n    Aws::String name = it-&gt;GetKey();\r\n\r\n3) Now `name` contains one of the filenames, attempt to decode it:\r\n\r\n    PWCHAR* objectName = nullptr;\r\n    //Properly initialize objectName and szObjNameLen, then:\r\n    mbstowcs_s(&amp;szCount, objectName, szObjNameLen, name.c_str(), name.length());\r\n    //OR:\r\n    MultiByteToWideChar(CP_ACP, MB_PRECOMPOSED, name.c_str(), name.length(), objectName, szObjNameLen); //Using CP_UTF8 does not work as well\r\n\r\n\r\n4) Later on, try to create a file on disk:\r\n\r\n    //Prepare the PATH for objectName and create the file...\r\n    hFile = CreateFile(objectName, GENERIC_WRITE | GENERIC_READ, 0,\r\n         NULL, OPEN_ALWAYS, FILE_ATTRIBUTE_NORMAL, NULL);\r\n\r\nUsing this procedure I&#39;m able to create ASCII filenames but not unicode filenames.",
            "link": "https://stackoverflow.com/questions/49821811/how-to-decode-unicode-s3-object-names",
            "title": "How to decode unicode S3 object names?",
            "body": "<p>How to decode unicode object names from S3 using the Windows C++ AWS SDK?</p>\n\n<p>I'm facing the weird issue of getting wrongly decoded object names when listing buckets with files having unicode names. What I do is that I run 'mbstowcs_s' or 'MultiByteToWideChar' (Attempted both) to decode the object names that I get from S3.</p>\n\n<p>As I understand, the object names coming from S3 are in multibyte format so I convert it to wide chars string using these converting functions, this seems to not work and I always get some trash names while listing unicode files, but names which use standard symbols from the ASCII table are properly decoded.</p>\n\n<p>For example, having the file 'ธงไชย แม็คอินไตย์.txt' on my bucket, if I attempt to list it from S3 using ListObjectV2Request and I attempt to convert its name to wide chars using mbstowcs_s I always get some noise instead of a valid name. The same method works perfectly with ASCII name.</p>\n\n<p>Any suggestion?</p>\n\n<p><strong>EDIT 1:</strong></p>\n\n<p>Here are a bit more details on how I'm attempting to decode the names (cutting off all the not relevant parts in order to focus on the decoding steps)</p>\n\n<p>1) First I get the list of objects from S3:</p>\n\n<pre><code>pListObjectsOutcome = new Aws::S3::Model::ListObjectsV2Outcome(pS3Client-&gt;ListObjectsV2(*pObjectsRequest));\npObjectList = new (std::nothrow) Aws::Vector&lt;Aws::S3::Model::Object&gt;(pListObjectsOutcome-&gt;GetResult().GetContents());\n</code></pre>\n\n<p>2) Iterate over the list and extract the names:</p>\n\n<pre><code>Aws::Vector&lt;Aws::S3::Model::Object&gt;::iterator it = pObjectList-&gt;begin();\n//loop for each name\nAws::String name = it-&gt;GetKey();\n</code></pre>\n\n<p>3) Now <code>name</code> contains one of the filenames, attempt to decode it:</p>\n\n<pre><code>PWCHAR* objectName = nullptr;\n//Properly initialize objectName and szObjNameLen, then:\nmbstowcs_s(&amp;szCount, objectName, szObjNameLen, name.c_str(), name.length());\n//OR:\nMultiByteToWideChar(CP_ACP, MB_PRECOMPOSED, name.c_str(), name.length(), objectName, szObjNameLen); //Using CP_UTF8 does not work as well\n</code></pre>\n\n<p>4) Later on, try to create a file on disk:</p>\n\n<pre><code>//Prepare the PATH for objectName and create the file...\nhFile = CreateFile(objectName, GENERIC_WRITE | GENERIC_READ, 0,\n     NULL, OPEN_ALWAYS, FILE_ATTRIBUTE_NORMAL, NULL);\n</code></pre>\n\n<p>Using this procedure I'm able to create ASCII filenames but not unicode filenames.</p>\n"
        },
        {
            "tags": [
                "google-cloud-dataflow",
                "apache-beam",
                "google-natural-language"
            ],
            "owner": {
                "reputation": 48,
                "user_id": 1162583,
                "user_type": "registered",
                "accept_rate": 38,
                "profile_image": "https://www.gravatar.com/avatar/e2a9731f760b90e7a60d85c9fbc724a6?s=128&d=identicon&r=PG",
                "display_name": "John Watson",
                "link": "https://stackoverflow.com/users/1162583/john-watson"
            },
            "is_answered": false,
            "view_count": 7,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524203941,
            "creation_date": 1524203941,
            "question_id": 49934963,
            "body_markdown": "I have a dataflow pipepline to use google cloud natural language API for sentimental analysis. So, i include jar for cloud language &amp; dataflow runner as below:\r\n\r\n    &lt;dependency&gt;\r\n      &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;\r\n      &lt;artifactId&gt;google-cloud-language&lt;/artifactId&gt;\r\n      &lt;version&gt;1.25.0&lt;/version&gt;\r\n    &lt;/dependency&gt;\r\n\r\n\t&lt;dependency&gt;\r\n\t\t&lt;groupId&gt;org.apache.beam&lt;/groupId&gt;\r\n\t\t&lt;artifactId&gt;beam-runners-google-cloud-dataflow-java&lt;/artifactId&gt;\r\n\t\t&lt;version&gt;2.4.0&lt;/version&gt;\r\n\t&lt;/dependency&gt;\r\n\r\nBut the problem is, there are crashes on the dependencies\r\n\r\ncloud language is using io.grpc 1.10.1 while beam sdk is using 1.2.0. Is there any way to sort it out? Thanks.\r\n\r\n\r\n\r\ncom.google.cloud:google-cloud-language:jar:1.25.0 -&gt; io.grpc:grpc-stub:jar:1.10.1 -&gt; io.grpc:grpc-core:jar:1.10.1\r\n\r\norg.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-core:jar:1.2.0, \r\n\r\nDetails logs:\r\n\r\n\r\n    [ERROR] Failed to execute goal on project apache-beam-loader: Could not resolve dependencies for project com.sample.cloud:apache-beam-loader:jar:0.0.1-SNAPSHOT: Failed to collect dependencies for com.sample.cloud:apache-beam-loader:jar:0.0.1-SNAPSHOT: Could not resolve version conflict among [org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-core:jar:1.2.0, org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-netty:jar:1.2.0 -&gt; io.grpc:grpc-core:jar:[1.2.0,1.2.0], org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-all:jar:1.2.0 -&gt; io.grpc:grpc-core:jar:[1.2.0,1.2.0], org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-all:jar:1.2.0 -&gt; io.grpc:grpc-okhttp:jar:1.2.0 -&gt; io.grpc:grpc-core:jar:[1.2.0,1.2.0], org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-all:jar:1.2.0 -&gt; io.grpc:grpc-protobuf-lite:jar:1.2.0 -&gt; io.grpc:grpc-core:jar:1.2.0, org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-all:jar:1.2.0 -&gt; io.grpc:grpc-protobuf-nano:jar:1.2.0 -&gt; io.grpc:grpc-core:jar:1.2.0, org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; com.google.cloud.bigtable:bigtable-protos:jar:1.0.0-pre3 -&gt; io.grpc:grpc-core:jar:1.5.0, org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; com.google.cloud.bigtable:bigtable-client-core:jar:1.0.0 -&gt; io.grpc:grpc-core:jar:1.7.0, org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; com.google.cloud.bigtable:bigtable-client-core:jar:1.0.0 -&gt; io.opencensus:opencensus-contrib-grpc-util:jar:0.7.0 -&gt; io.grpc:grpc-core:jar:1.6.1, com.google.cloud:google-cloud-language:jar:1.25.0 -&gt; com.google.cloud:google-cloud-core-grpc:jar:1.25.0 -&gt; io.grpc:grpc-protobuf:jar:1.10.1 -&gt; io.grpc:grpc-core:jar:1.10.1, com.google.cloud:google-cloud-language:jar:1.25.0 -&gt; io.grpc:grpc-netty-shaded:jar:1.10.1 -&gt; io.grpc:grpc-core:jar:[1.10.1,1.10.1], com.google.cloud:google-cloud-language:jar:1.25.0 -&gt; io.grpc:grpc-stub:jar:1.10.1 -&gt; io.grpc:grpc-core:jar:1.10.1, com.google.cloud:google-cloud-language:jar:1.25.0 -&gt; io.grpc:grpc-auth:jar:1.10.1 -&gt; io.grpc:grpc-core:jar:[1.10.1,1.10.1]] -&gt; [Help 1]\r\n\r\n\r\n ",
            "link": "https://stackoverflow.com/questions/49934963/unable-to-use-dataflow-with-google-cloud-nature-language-api",
            "title": "Unable to use dataflow with google cloud nature language API",
            "body": "<p>I have a dataflow pipepline to use google cloud natural language API for sentimental analysis. So, i include jar for cloud language &amp; dataflow runner as below:</p>\n\n<pre><code>&lt;dependency&gt;\n  &lt;groupId&gt;com.google.cloud&lt;/groupId&gt;\n  &lt;artifactId&gt;google-cloud-language&lt;/artifactId&gt;\n  &lt;version&gt;1.25.0&lt;/version&gt;\n&lt;/dependency&gt;\n\n&lt;dependency&gt;\n    &lt;groupId&gt;org.apache.beam&lt;/groupId&gt;\n    &lt;artifactId&gt;beam-runners-google-cloud-dataflow-java&lt;/artifactId&gt;\n    &lt;version&gt;2.4.0&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre>\n\n<p>But the problem is, there are crashes on the dependencies</p>\n\n<p>cloud language is using io.grpc 1.10.1 while beam sdk is using 1.2.0. Is there any way to sort it out? Thanks.</p>\n\n<p>com.google.cloud:google-cloud-language:jar:1.25.0 -> io.grpc:grpc-stub:jar:1.10.1 -> io.grpc:grpc-core:jar:1.10.1</p>\n\n<p>org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -> io.grpc:grpc-core:jar:1.2.0, </p>\n\n<p>Details logs:</p>\n\n<pre><code>[ERROR] Failed to execute goal on project apache-beam-loader: Could not resolve dependencies for project com.sample.cloud:apache-beam-loader:jar:0.0.1-SNAPSHOT: Failed to collect dependencies for com.sample.cloud:apache-beam-loader:jar:0.0.1-SNAPSHOT: Could not resolve version conflict among [org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-core:jar:1.2.0, org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-netty:jar:1.2.0 -&gt; io.grpc:grpc-core:jar:[1.2.0,1.2.0], org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-all:jar:1.2.0 -&gt; io.grpc:grpc-core:jar:[1.2.0,1.2.0], org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-all:jar:1.2.0 -&gt; io.grpc:grpc-okhttp:jar:1.2.0 -&gt; io.grpc:grpc-core:jar:[1.2.0,1.2.0], org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-all:jar:1.2.0 -&gt; io.grpc:grpc-protobuf-lite:jar:1.2.0 -&gt; io.grpc:grpc-core:jar:1.2.0, org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; io.grpc:grpc-all:jar:1.2.0 -&gt; io.grpc:grpc-protobuf-nano:jar:1.2.0 -&gt; io.grpc:grpc-core:jar:1.2.0, org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; com.google.cloud.bigtable:bigtable-protos:jar:1.0.0-pre3 -&gt; io.grpc:grpc-core:jar:1.5.0, org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; com.google.cloud.bigtable:bigtable-client-core:jar:1.0.0 -&gt; io.grpc:grpc-core:jar:1.7.0, org.apache.beam:beam-runners-google-cloud-dataflow-java:jar:2.4.0 -&gt; org.apache.beam:beam-sdks-java-io-google-cloud-platform:jar:2.4.0 -&gt; com.google.cloud.bigtable:bigtable-client-core:jar:1.0.0 -&gt; io.opencensus:opencensus-contrib-grpc-util:jar:0.7.0 -&gt; io.grpc:grpc-core:jar:1.6.1, com.google.cloud:google-cloud-language:jar:1.25.0 -&gt; com.google.cloud:google-cloud-core-grpc:jar:1.25.0 -&gt; io.grpc:grpc-protobuf:jar:1.10.1 -&gt; io.grpc:grpc-core:jar:1.10.1, com.google.cloud:google-cloud-language:jar:1.25.0 -&gt; io.grpc:grpc-netty-shaded:jar:1.10.1 -&gt; io.grpc:grpc-core:jar:[1.10.1,1.10.1], com.google.cloud:google-cloud-language:jar:1.25.0 -&gt; io.grpc:grpc-stub:jar:1.10.1 -&gt; io.grpc:grpc-core:jar:1.10.1, com.google.cloud:google-cloud-language:jar:1.25.0 -&gt; io.grpc:grpc-auth:jar:1.10.1 -&gt; io.grpc:grpc-core:jar:[1.10.1,1.10.1]] -&gt; [Help 1]\n</code></pre>\n"
        },
        {
            "tags": [
                "sql-server",
                "stored-procedures",
                "ssis"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9558903,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/8e3e1c337abdadcb56f7d4732572cded?s=128&d=identicon&r=PG",
                "display_name": "user9558903",
                "link": "https://stackoverflow.com/users/9558903/user9558903"
            },
            "is_answered": false,
            "view_count": 20,
            "answer_count": 0,
            "score": -2,
            "last_activity_date": 1524203937,
            "creation_date": 1524136865,
            "last_edit_date": 1524203937,
            "question_id": 49919820,
            "body_markdown": "I want to execute a stored procedure with parameter (output path) on a different sql server to extract data.\r\nStored Procedure works with a temp table.\r\nI want to put the output then on a different sql server.\r\nCan I do this without SSDT?\r\n\r\n***edit: linked servers is not an option in this case",
            "link": "https://stackoverflow.com/questions/49919820/can-i-execute-a-stored-procedure-on-a-different-server-with-import-export-wizard",
            "title": "can I execute a stored procedure on a different server with import export wizard?",
            "body": "<p>I want to execute a stored procedure with parameter (output path) on a different sql server to extract data.\nStored Procedure works with a temp table.\nI want to put the output then on a different sql server.\nCan I do this without SSDT?</p>\n\n<p>***edit: linked servers is not an option in this case</p>\n"
        },
        {
            "tags": [
                "c#",
                "asp.net",
                ".net",
                "model-view-controller"
            ],
            "owner": {
                "reputation": 65,
                "user_id": 2769810,
                "user_type": "registered",
                "accept_rate": 60,
                "profile_image": "https://www.gravatar.com/avatar/4b25b6612f71146dfca0a0c0c7e9044f?s=128&d=identicon&r=PG&f=1",
                "display_name": "user2769810",
                "link": "https://stackoverflow.com/users/2769810/user2769810"
            },
            "is_answered": false,
            "view_count": 28,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524203932,
            "creation_date": 1524203932,
            "question_id": 49934960,
            "body_markdown": "I am on a C# .NET MVC project, and have a form that can dynamically add/remove n number of complex objects in a list. This complex object, for example represents a Person. This person has FirstName and Address properties. \r\n\r\nWhen the user loads the page, all the People in the system are displayed in a list. When the user presses the &#39;add&#39; button, two new text boxes show up for the Person&#39;s FirstName and Address properties. When the user presses the submit button, it will make a POST request to the server. \r\n\r\nI know that you can write regular html in the View, and can use Javascript to add the new DOM elements for the FirstName and Address properties. \r\n\r\nAnd with regards to when the user submits, I can use javascript to scrape all the data in the screen, and send a POST request to the server. Theoretically, another method is instead of using javascript, just make the button submit the form to the POST action of the controller; if I give my DOM elements the proper name attribute, the Action should recognize the data.\r\n\r\nHowever, is there a MVC way of doing this? Maybe, with the help of Razor Helpers?",
            "link": "https://stackoverflow.com/questions/49934960/net-mvc-dynamically-add-remove-items-without-using-javascript",
            "title": ".NET MVC Dynamically add/remove items without using javascript",
            "body": "<p>I am on a C# .NET MVC project, and have a form that can dynamically add/remove n number of complex objects in a list. This complex object, for example represents a Person. This person has FirstName and Address properties. </p>\n\n<p>When the user loads the page, all the People in the system are displayed in a list. When the user presses the 'add' button, two new text boxes show up for the Person's FirstName and Address properties. When the user presses the submit button, it will make a POST request to the server. </p>\n\n<p>I know that you can write regular html in the View, and can use Javascript to add the new DOM elements for the FirstName and Address properties. </p>\n\n<p>And with regards to when the user submits, I can use javascript to scrape all the data in the screen, and send a POST request to the server. Theoretically, another method is instead of using javascript, just make the button submit the form to the POST action of the controller; if I give my DOM elements the proper name attribute, the Action should recognize the data.</p>\n\n<p>However, is there a MVC way of doing this? Maybe, with the help of Razor Helpers?</p>\n"
        },
        {
            "tags": [
                "sql-server",
                "mongodb",
                "mongodb-query",
                "aggregation"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 7407960,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/96209d1054132e0d4050831bc968537e?s=128&d=identicon&r=PG&f=1",
                "display_name": "addon mehul",
                "link": "https://stackoverflow.com/users/7407960/addon-mehul"
            },
            "is_answered": false,
            "view_count": 30,
            "closed_date": 1524221962,
            "answer_count": 0,
            "score": -5,
            "last_activity_date": 1524203930,
            "creation_date": 1524202769,
            "last_edit_date": 1524203930,
            "question_id": 49934729,
            "body_markdown": "I have an SQL query like this:\r\n\r\n\r\n    WITH marked AS ( SELECT *, grp = ROW_NUMBER() OVER (PARTITION BY no ORDER BY date) - ROW_NUMBER() OVER (PARTITION BY no, ignition ORDER BY date) FROM tbl_data where no= 123 and date between &#39;2018-04-01&#39; and &#39;2018-04-20&#39; )\r\n    \r\n         SELECT\r\n          Start1=(select TOP(1) L1 from tbl_data m where m.date=MIN(mr.date) AND m.no=&#39;123&#39;),\r\n          Start2=(select TOP(1) L2 from tbl_data m where m.date=MIN(mr.date) AND m.no=&#39;123&#39;),\r\n          End1=(select TOP(1) L1 from tbl_data m where m.date=MAX(mr.date) AND m.no=&#39;123&#39;),\r\n          End2=(select TOP(1) L2 from tbl_data m where m.date=MAX(mr.date) AND m.no=&#39;123&#39;),\r\n          startdate = MIN(mr.date),\r\n          enddate=MAX(mr.date),\r\n          Status=(case when ignition=0 then &#39;Stop&#39; Else &#39;Start&#39; END),\r\n          Duration = DATEDIFF(MINUTE, MIN(mr.date), MAX(mr.date)),\r\n          DATEDIFF(second,MIN(mr.date),MAX(mr.date)) as DifferenceData,\r\n          Distance=coalesce((select SUM(CAST(odo as float)) \r\n          from tbl_data m where date between MIN(mr.date) and MAX(mr.date) AND m.no=&#39;123&#39; and m.speed &gt;7),0)\r\n\r\n         FROM marked mr where no= &#39;123&#39;  and  date between &#39;2018-04-01&#39; and &#39;2018-04-20&#39;\r\n\r\n         GROUP BY\r\n          no,\r\n          ignition,\r\n          grp  \r\n          ORDER BY\r\n          MIN(mr.date)\r\n\r\n\r\nhow to convert this query into mongodb?\r\nanyone can help for converting SQL query to mongodb?\r\nThanks.\r\n",
            "link": "https://stackoverflow.com/questions/49934729/how-to-write-row-number-partition-query-in-mongodb",
            "closed_reason": "duplicate",
            "title": "How to write row_number partition query in mongodb",
            "body": "<p>I have an SQL query like this:</p>\n\n<pre><code>WITH marked AS ( SELECT *, grp = ROW_NUMBER() OVER (PARTITION BY no ORDER BY date) - ROW_NUMBER() OVER (PARTITION BY no, ignition ORDER BY date) FROM tbl_data where no= 123 and date between '2018-04-01' and '2018-04-20' )\n\n     SELECT\n      Start1=(select TOP(1) L1 from tbl_data m where m.date=MIN(mr.date) AND m.no='123'),\n      Start2=(select TOP(1) L2 from tbl_data m where m.date=MIN(mr.date) AND m.no='123'),\n      End1=(select TOP(1) L1 from tbl_data m where m.date=MAX(mr.date) AND m.no='123'),\n      End2=(select TOP(1) L2 from tbl_data m where m.date=MAX(mr.date) AND m.no='123'),\n      startdate = MIN(mr.date),\n      enddate=MAX(mr.date),\n      Status=(case when ignition=0 then 'Stop' Else 'Start' END),\n      Duration = DATEDIFF(MINUTE, MIN(mr.date), MAX(mr.date)),\n      DATEDIFF(second,MIN(mr.date),MAX(mr.date)) as DifferenceData,\n      Distance=coalesce((select SUM(CAST(odo as float)) \n      from tbl_data m where date between MIN(mr.date) and MAX(mr.date) AND m.no='123' and m.speed &gt;7),0)\n\n     FROM marked mr where no= '123'  and  date between '2018-04-01' and '2018-04-20'\n\n     GROUP BY\n      no,\n      ignition,\n      grp  \n      ORDER BY\n      MIN(mr.date)\n</code></pre>\n\n<p>how to convert this query into mongodb?\nanyone can help for converting SQL query to mongodb?\nThanks.</p>\n"
        },
        {
            "tags": [
                "python",
                "django",
                "django-rest-framework"
            ],
            "owner": {
                "reputation": 25,
                "user_id": 4613907,
                "user_type": "registered",
                "accept_rate": 75,
                "profile_image": "https://i.stack.imgur.com/Gichx.png?s=128&g=1",
                "display_name": "chronox",
                "link": "https://stackoverflow.com/users/4613907/chronox"
            },
            "is_answered": true,
            "view_count": 193,
            "accepted_answer_id": 45316413,
            "answer_count": 2,
            "score": 1,
            "last_activity_date": 1524203927,
            "creation_date": 1500968630,
            "last_edit_date": 1500974176,
            "question_id": 45296942,
            "body_markdown": "I am currently working on a django rest framework api which involves displaying a list of campaigns and allowing user to create new campaign.\r\n\r\nOn the browsable api, I managed to display the list of campaigns that I wanted as well as have a form for the user to create new campaign.\r\n\r\nThis is a screenshot of the form that the user is using for creating new campaign.\r\n\r\n[A screenshot of the form on browsable API][1]\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/f2bpC.png\r\n\r\nWhile everything is working, I faced an issue that I do not have any idea how to solve even after reading the documentation.\r\n\r\nAs you can see on the screenshot, there is a clist field that allows user to select the contact list he/she want the campaign invitations to be sent to. However, I want to make sure only the contact lists created by the user&#39;s company are shown in that field (currently, all the contact lists from different companies can be selected).\r\n\r\nHere is the codes in the api.py:\r\n\r\n    class EditCampaignViewSet(ModelViewSet):\r\n        queryset = Campaign.objects.all()\r\n        serializer_class = EditCampaignSerializer\r\n        parser_classes = (MultiPartParser, FormParser)\r\n\r\n        def get_serializer_context(self):\r\n            return {&#39;request&#39;: self.request}\r\n\r\n        def list(self, request, p_uuid=None, type=None, *args, **kwargs):\r\n            company = request.user.profile.company\r\n            queryset = Campaign.objects.filter(company=company,\r\n                                               product__uuid=p_uuid,\r\n                                               deleted=False,\r\n                                               campaign_type=type)\\\r\n                                       .order_by(&#39;-created&#39;)\\\r\n                                       .prefetch_related(&#39;user__profile&#39;)\r\n            serializer = EditCampaignSerializer(queryset, many=True)\r\n            return Response(serializer.data)\r\n\r\nThis is the serializers.py\r\n\r\n    class EditCampaignSerializer(serializers.ModelSerializer):\r\n        class Meta:\r\n            model = Campaign\r\n            fields = (&#39;id&#39;, &#39;campaign_id&#39;, &#39;campaign_type&#39;, &#39;name&#39;, &#39;product&#39;, &#39;description&#39;, &#39;status&#39;, &#39;actual_file_name&#39;,\r\n                  &#39;pdf_file&#39;, &#39;download&#39;, &#39;header&#39;, &#39;body&#39;, &#39;footer&#39;, &#39;company&#39;, &#39;created&#39;, &#39;updated&#39;, &#39;deleted&#39;,\r\n                  &#39;clist&#39;, &#39;user&#39;)\r\n            read_only_fields = (&#39;id&#39;, &#39;campaign_id&#39;, &#39;campaign_type&#39;, &#39;product&#39;, &#39;status&#39;, &#39;actual_file_name&#39;, &#39;company&#39;, &#39;created&#39;,\r\n                            &#39;updated&#39;, &#39;deleted&#39;, &#39;user&#39;)\r\n\r\n        def __init__(self, *args, **kwargs):\r\n            super(EditCampaignSerializer, self).__init__(*args, **kwargs)\r\n            user = self.context[&#39;request&#39;].user\r\n            self.fields[&#39;clist&#39;] = ChoiceField(choices=CList.objects.filter(company=user.profile.company))\r\n\r\nI am still pretty new to django rest framework, so please pardon me if the answer is obvious.",
            "link": "https://stackoverflow.com/questions/45296942/django-rest-api-filter-fields-select-options-based-on-request-user",
            "title": "Django Rest Api filter field&#39;s select options based on request.user",
            "body": "<p>I am currently working on a django rest framework api which involves displaying a list of campaigns and allowing user to create new campaign.</p>\n\n<p>On the browsable api, I managed to display the list of campaigns that I wanted as well as have a form for the user to create new campaign.</p>\n\n<p>This is a screenshot of the form that the user is using for creating new campaign.</p>\n\n<p><a href=\"https://i.stack.imgur.com/f2bpC.png\" rel=\"nofollow noreferrer\">A screenshot of the form on browsable API</a></p>\n\n<p>While everything is working, I faced an issue that I do not have any idea how to solve even after reading the documentation.</p>\n\n<p>As you can see on the screenshot, there is a clist field that allows user to select the contact list he/she want the campaign invitations to be sent to. However, I want to make sure only the contact lists created by the user's company are shown in that field (currently, all the contact lists from different companies can be selected).</p>\n\n<p>Here is the codes in the api.py:</p>\n\n<pre><code>class EditCampaignViewSet(ModelViewSet):\n    queryset = Campaign.objects.all()\n    serializer_class = EditCampaignSerializer\n    parser_classes = (MultiPartParser, FormParser)\n\n    def get_serializer_context(self):\n        return {'request': self.request}\n\n    def list(self, request, p_uuid=None, type=None, *args, **kwargs):\n        company = request.user.profile.company\n        queryset = Campaign.objects.filter(company=company,\n                                           product__uuid=p_uuid,\n                                           deleted=False,\n                                           campaign_type=type)\\\n                                   .order_by('-created')\\\n                                   .prefetch_related('user__profile')\n        serializer = EditCampaignSerializer(queryset, many=True)\n        return Response(serializer.data)\n</code></pre>\n\n<p>This is the serializers.py</p>\n\n<pre><code>class EditCampaignSerializer(serializers.ModelSerializer):\n    class Meta:\n        model = Campaign\n        fields = ('id', 'campaign_id', 'campaign_type', 'name', 'product', 'description', 'status', 'actual_file_name',\n              'pdf_file', 'download', 'header', 'body', 'footer', 'company', 'created', 'updated', 'deleted',\n              'clist', 'user')\n        read_only_fields = ('id', 'campaign_id', 'campaign_type', 'product', 'status', 'actual_file_name', 'company', 'created',\n                        'updated', 'deleted', 'user')\n\n    def __init__(self, *args, **kwargs):\n        super(EditCampaignSerializer, self).__init__(*args, **kwargs)\n        user = self.context['request'].user\n        self.fields['clist'] = ChoiceField(choices=CList.objects.filter(company=user.profile.company))\n</code></pre>\n\n<p>I am still pretty new to django rest framework, so please pardon me if the answer is obvious.</p>\n"
        },
        {
            "tags": [
                "angular",
                "rxjs"
            ],
            "owner": {
                "reputation": 28,
                "user_id": 5903172,
                "user_type": "registered",
                "profile_image": "https://graph.facebook.com/10153434809643437/picture?type=large",
                "display_name": "Cristoffer Holm",
                "link": "https://stackoverflow.com/users/5903172/cristoffer-holm"
            },
            "is_answered": true,
            "view_count": 43,
            "accepted_answer_id": 49934957,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1524203915,
            "creation_date": 1524201116,
            "last_edit_date": 1524203059,
            "question_id": 49934416,
            "body_markdown": "I&#39;m building an Angular 5 application that let&#39;s you track the score of an ongoing martial arts match. The score is input into the DB from a desktop application.\r\nThe webapp then gets the score as a single scoreboard object from a Angular service that calls an API with the Id for the match and uses the retrieved object in the component to display the score for the two fighters. \r\nThe user will refreshes the score by a button click (or maybe on a timer.)\r\n\r\nI&#39;m very new to both Angular and RxJs, and the trouble I&#39;m having is finding a examples of how to do this WELL.\r\n\r\nRight now I&#39;ve got it working by getting scoreboards for ALL matches and then using map and find to filter out the one I want. It feels very crude.\r\n\r\nTo summarize what I need to do.\r\nMake a call to a service method with a unique Id.\r\nGet a single object from the API with the Id.\r\nreturn that object to the component and assign it to a property.\r\n\r\nAny and all advice is greatly appreciated!\r\n\r\n",
            "link": "https://stackoverflow.com/questions/49934416/angular-5-best-practice-for-updating-a-single-object-with-rxjs",
            "title": "Angular 5 best practice for updating a single object with RxJS",
            "body": "<p>I'm building an Angular 5 application that let's you track the score of an ongoing martial arts match. The score is input into the DB from a desktop application.\nThe webapp then gets the score as a single scoreboard object from a Angular service that calls an API with the Id for the match and uses the retrieved object in the component to display the score for the two fighters. \nThe user will refreshes the score by a button click (or maybe on a timer.)</p>\n\n<p>I'm very new to both Angular and RxJs, and the trouble I'm having is finding a examples of how to do this WELL.</p>\n\n<p>Right now I've got it working by getting scoreboards for ALL matches and then using map and find to filter out the one I want. It feels very crude.</p>\n\n<p>To summarize what I need to do.\nMake a call to a service method with a unique Id.\nGet a single object from the API with the Id.\nreturn that object to the component and assign it to a property.</p>\n\n<p>Any and all advice is greatly appreciated!</p>\n"
        },
        {
            "tags": [
                "java",
                "hashmap",
                "kotlin",
                "hashset"
            ],
            "owner": {
                "reputation": 115,
                "user_id": 8763097,
                "user_type": "registered",
                "accept_rate": 75,
                "profile_image": "https://www.gravatar.com/avatar/07748010c90ca48d262a588483d84367?s=128&d=identicon&r=PG&f=1",
                "display_name": "PeptideWitch",
                "link": "https://stackoverflow.com/users/8763097/peptidewitch"
            },
            "is_answered": true,
            "view_count": 40,
            "answer_count": 2,
            "score": 2,
            "last_activity_date": 1524203913,
            "creation_date": 1524202038,
            "question_id": 49934576,
            "body_markdown": "I have a data class:\r\n\r\n    data class UniqueProtein(var id: String, var spc: Int, var r: DoubleArray)\r\n\r\nI&#39;ve created a `HashMap&lt;UniqueProtein, UniqueProtein&gt;` that I want to convert to a HashSet so I can find overlapping values between two HashMaps, like so:\r\n\r\n    dottest(control: HashMap&lt;UniqueProtein, UniqueProtein&gt;, treatment: HashMap&lt;UniqueProtein, UniqueProtein&gt;) \r\n\r\n    var combineddata:HashSet&lt;UniqueProtein&gt; = control.keySet().retainAll(treatment.keySet())\r\n\r\nIntelliJ is throwing the following in-line error under both keySet() calls:\r\n    \r\n    is public /*package*/ in &#39;HashMap&#39;\r\n\r\nBut since I&#39;ve declared the object as a data class, I&#39;m not sure what&#39;s going on...",
            "link": "https://stackoverflow.com/questions/49934576/hashmapdata-class-data-class-object-keyset-function-throws-error-relating-t",
            "title": "HashMap&lt;data class, data class&gt; object.keySet() function throws error relating to a public call",
            "body": "<p>I have a data class:</p>\n\n<pre><code>data class UniqueProtein(var id: String, var spc: Int, var r: DoubleArray)\n</code></pre>\n\n<p>I've created a <code>HashMap&lt;UniqueProtein, UniqueProtein&gt;</code> that I want to convert to a HashSet so I can find overlapping values between two HashMaps, like so:</p>\n\n<pre><code>dottest(control: HashMap&lt;UniqueProtein, UniqueProtein&gt;, treatment: HashMap&lt;UniqueProtein, UniqueProtein&gt;) \n\nvar combineddata:HashSet&lt;UniqueProtein&gt; = control.keySet().retainAll(treatment.keySet())\n</code></pre>\n\n<p>IntelliJ is throwing the following in-line error under both keySet() calls:</p>\n\n<pre><code>is public /*package*/ in 'HashMap'\n</code></pre>\n\n<p>But since I've declared the object as a data class, I'm not sure what's going on...</p>\n"
        },
        {
            "tags": [
                "mdm"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9673175,
                "user_type": "registered",
                "profile_image": "https://lh3.googleusercontent.com/-wBZj_wyS_o8/AAAAAAAAAAI/AAAAAAAAAAA/QZGYIccm0k8/photo.jpg?sz=128",
                "display_name": "Chetan Sharma",
                "link": "https://stackoverflow.com/users/9673175/chetan-sharma"
            },
            "is_answered": false,
            "view_count": 7,
            "answer_count": 0,
            "score": -2,
            "last_activity_date": 1524203903,
            "creation_date": 1524203903,
            "question_id": 49934950,
            "body_markdown": "As a tester currently i&#39;m start testing of **mdm**. i want know is there any third party apps or any thing else to break or stop *mdm* \r\n\r\ncurrently my mdm setting is stop using &#39;playstore&#39;, &#39;youtube&#39; and other app\r\n\r\ncheck link regarding mdm\r\nhttps://www.manageengine.com/mobile-device-management/?gclid=EAIaIQobChMI1-Ky6JXI2gIViTUrCh0Z7QWFEAAYASAAEgLTn_D_BwE\r\n\r\n\r\n\r\n\r\n",
            "link": "https://stackoverflow.com/questions/49934950/how-to-break-mdm-functionality",
            "title": "how to break mdm functionality",
            "body": "<p>As a tester currently i'm start testing of <strong>mdm</strong>. i want know is there any third party apps or any thing else to break or stop <em>mdm</em> </p>\n\n<p>currently my mdm setting is stop using 'playstore', 'youtube' and other app</p>\n\n<p>check link regarding mdm\n<a href=\"https://www.manageengine.com/mobile-device-management/?gclid=EAIaIQobChMI1-Ky6JXI2gIViTUrCh0Z7QWFEAAYASAAEgLTn_D_BwE\" rel=\"nofollow noreferrer\">https://www.manageengine.com/mobile-device-management/?gclid=EAIaIQobChMI1-Ky6JXI2gIViTUrCh0Z7QWFEAAYASAAEgLTn_D_BwE</a></p>\n"
        },
        {
            "tags": [
                "kendo-grid"
            ],
            "owner": {
                "reputation": 1118,
                "user_id": 796709,
                "user_type": "registered",
                "accept_rate": 58,
                "profile_image": "https://www.gravatar.com/avatar/28cd359d7f1d76855fc927da52cd48ac?s=128&d=identicon&r=PG",
                "display_name": "Bryan Schmiedeler",
                "link": "https://stackoverflow.com/users/796709/bryan-schmiedeler"
            },
            "is_answered": false,
            "view_count": 1027,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1524203887,
            "creation_date": 1485976439,
            "question_id": 41987788,
            "body_markdown": "I have been using filters to successfully search on my KendoUI grids. However a new application has some fields that are multi-valued and my filtering doesn&#39;t work - it actually just seems to spin forever. \r\n\r\nAn example of a multi-value field:\r\n\r\n    field   : &quot;rspPersons&quot;,\r\n    title   : &quot;Responsible Persons&quot;,\r\n    type    : &quot;Text&quot;,\r\n    template: &quot;# var t=rspPersons.join(&#39;, &#39;);data.tagsString=t; # #=t #&quot;\r\n\r\nAn example of my filter:\r\n\r\n\r\n\t\t\t\t\t\t\torfilter.filters.push( {\r\n\t\t\t\t\t\t\t\tfield : &quot;chgDescription&quot;,\r\n\t\t\t\t\t\t\t\toperator : &quot;contains&quot;,\r\n\t\t\t\t\t\t\t\tvalue : v1\r\n\t\t\t\t\t\t\t},\r\n\t\t\t\t\t\t\torfilter.filters.push( {\r\n\t\t\t\t\t\t\t\tfield : &quot;rspPersons&quot;,\r\n\t\t\t\t\t\t\t\toperator : &quot;contains&quot;,\r\n\t\t\t\t\t\t\t\tvalue : v1\r\n\t\t\t\t\t\t\t} \r\n\r\nThe second filter will make the entire search break down. If I take it out, then the search/filter works just fine.\r\n\r\nSo how can I filter/search on multi-value fields?",
            "link": "https://stackoverflow.com/questions/41987788/kendo-ui-grid-filtering-column-with-multiple-values",
            "title": "Kendo UI Grid Filtering column with multiple values",
            "body": "<p>I have been using filters to successfully search on my KendoUI grids. However a new application has some fields that are multi-valued and my filtering doesn't work - it actually just seems to spin forever. </p>\n\n<p>An example of a multi-value field:</p>\n\n<pre><code>field   : \"rspPersons\",\ntitle   : \"Responsible Persons\",\ntype    : \"Text\",\ntemplate: \"# var t=rspPersons.join(', ');data.tagsString=t; # #=t #\"\n</code></pre>\n\n<p>An example of my filter:</p>\n\n<pre><code>                        orfilter.filters.push( {\n                            field : \"chgDescription\",\n                            operator : \"contains\",\n                            value : v1\n                        },\n                        orfilter.filters.push( {\n                            field : \"rspPersons\",\n                            operator : \"contains\",\n                            value : v1\n                        } \n</code></pre>\n\n<p>The second filter will make the entire search break down. If I take it out, then the search/filter works just fine.</p>\n\n<p>So how can I filter/search on multi-value fields?</p>\n"
        },
        {
            "tags": [
                "javascript",
                "jquery",
                "reactjs",
                "froala"
            ],
            "owner": {
                "reputation": 68,
                "user_id": 8373644,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/585621a7946edc29c9b238474ae5ac1e?s=128&d=identicon&r=PG&f=1",
                "display_name": "Dinesh Kumar",
                "link": "https://stackoverflow.com/users/8373644/dinesh-kumar"
            },
            "is_answered": false,
            "view_count": 10,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524203887,
            "creation_date": 1524203887,
            "question_id": 49934948,
            "body_markdown": "I have implemented the https://github.com/froala/react-froala-wysiwyg module and Froala works awesome.\r\nI added a custom button in the toolar and implemented the checkbox functionality.\r\n\r\n    import &#39;froala-editor/js/froala_editor.pkgd.min&#39;;\r\n    import &#39;froala-editor/css/froala_style.min.css&#39;;\r\n    import &#39;froala-editor/css/froala_editor.pkgd.min.css&#39;;\r\n    import &#39;font-awesome/css/font-awesome.css&#39;;\r\n    import FroalaEditor from &#39;react-froala-wysiwyg&#39;;\r\n    \r\n      componentDidMount() {\r\n        $.FroalaEditor.DefineIcon(&#39;check&#39;, { NAME: &#39;check-square&#39; });\r\n        $.FroalaEditor.RegisterCommand(&#39;check&#39;, {\r\n          title: &#39;check-box&#39;,\r\n          focus: true,\r\n          undo: true,\r\n          refreshAfterCallback: false,\r\n          callback: () =&gt; {\r\n            this.setState({ model: this.state.model + &#39;&lt;input type=&quot;checkbox&quot; /&gt;&#39; });\r\n          },\r\n        });\r\n      }\r\n    \r\n      config = {\r\n        placeholderText: &#39;Your notes here....&#39;,\r\n        heightMin: 250,\r\n        heightMax: 400,\r\n        colorsHEXInput: false,\r\n        autoFocus: true,\r\n        toolbarBottom: true,\r\n        linkAlwaysBlank: true,\r\n        fontFamilySelection: true,\r\n        fontSizeSelection: true,\r\n        paragraphFormatSelection: true,\r\n        htmlExecuteScripts: true,\r\n        iframe: true,\r\n        tabSpaces: 4,\r\n        pluginsEnabled: [&#39;align&#39;, &#39;charCounter&#39;, &#39;codeBeautifier&#39;, &#39;codeView&#39;, &#39;colors&#39;, &#39;draggable&#39;, &#39;embedly&#39;, &#39;emoticons&#39;, &#39;entities&#39;, &#39;file&#39;, &#39;fontFamily&#39;, &#39;fontSize&#39;, &#39;fullscreen&#39;, &#39;image&#39;, &#39;imageManager&#39;, &#39;inlineStyle&#39;, &#39;lineBreaker&#39;, &#39;link&#39;, &#39;lists&#39;, &#39;paragraphFormat&#39;, &#39;paragraphStyle&#39;, &#39;quickInsert&#39;, &#39;quote&#39;, &#39;save&#39;, &#39;table&#39;, &#39;url&#39;, &#39;video&#39;, &#39;wordPaste&#39;],\r\n        toolbarButtons: [&#39;bold&#39;, &#39;italic&#39;, &#39;underline&#39;, &#39;strikeThrough&#39;, &#39;subscript&#39;, &#39;superscript&#39;, &#39;|&#39;, &#39;fontFamily&#39;, &#39;fontSize&#39;, &#39;color&#39;, &#39;inlineStyle&#39;, &#39;paragraphStyle&#39;, &#39;|&#39;, &#39;paragraphFormat&#39;, &#39;align&#39;, &#39;formatOL&#39;, &#39;formatUL&#39;, &#39;outdent&#39;, &#39;indent&#39;, &#39;quote&#39;, &#39;check&#39;, &#39;|&#39;, &#39;insertLink&#39;, &#39;insertImage&#39;, &#39;insertVideo&#39;, &#39;embedly&#39;, &#39;insertFile&#39;, &#39;insertTable&#39;, &#39;|&#39;, &#39;emoticons&#39;, &#39;specialCharacters&#39;, &#39;insertHR&#39;, &#39;selectAll&#39;, &#39;clearFormatting&#39;, &#39;|&#39;, &#39;spellChecker&#39;, &#39;help&#39;, &#39;html&#39;, &#39;|&#39;, &#39;undo&#39;, &#39;redo&#39;],\r\n        toolbarButtonsMD: [&#39;bold&#39;, &#39;italic&#39;, &#39;underline&#39;, &#39;strikeThrough&#39;, &#39;subscript&#39;, &#39;superscript&#39;, &#39;|&#39;, &#39;fontFamily&#39;, &#39;fontSize&#39;, &#39;color&#39;, &#39;inlineStyle&#39;, &#39;paragraphStyle&#39;, &#39;|&#39;, &#39;paragraphFormat&#39;, &#39;align&#39;, &#39;formatOL&#39;, &#39;formatUL&#39;, &#39;outdent&#39;, &#39;indent&#39;, &#39;quote&#39;, &#39;check&#39;, &#39;|&#39;, &#39;insertLink&#39;, &#39;insertImage&#39;, &#39;insertVideo&#39;, &#39;embedly&#39;, &#39;insertFile&#39;, &#39;insertTable&#39;, &#39;|&#39;, &#39;emoticons&#39;, &#39;specialCharacters&#39;, &#39;insertHR&#39;, &#39;selectAll&#39;, &#39;clearFormatting&#39;, &#39;|&#39;, &#39;spellChecker&#39;, &#39;help&#39;, &#39;html&#39;, &#39;|&#39;, &#39;undo&#39;, &#39;redo&#39;],\r\n        toolbarButtonsSM: [&#39;bold&#39;, &#39;italic&#39;, &#39;underline&#39;, &#39;strikeThrough&#39;, &#39;|&#39;, &#39;fontFamily&#39;, &#39;fontSize&#39;, &#39;color&#39;, &#39;|&#39;, &#39;align&#39;, &#39;formatOL&#39;, &#39;formatUL&#39;, &#39;outdent&#39;, &#39;indent&#39;, &#39;quote&#39;, &#39;check&#39;, &#39;|&#39;, &#39;insertLink&#39;, &#39;insertImage&#39;, &#39;insertVideo&#39;, &#39;embedly&#39;, &#39;insertFile&#39;, &#39;insertTable&#39;, &#39;|&#39;, &#39;emoticons&#39;, &#39;specialCharacters&#39;, &#39;insertHR&#39;, &#39;selectAll&#39;, &#39;clearFormatting&#39;, &#39;|&#39;, &#39;spellChecker&#39;, &#39;help&#39;, &#39;html&#39;, &#39;|&#39;, &#39;undo&#39;, &#39;redo&#39;],\r\n        toolbarButtonsXS: [&#39;bold&#39;, &#39;italic&#39;, &#39;underline&#39;, &#39;|&#39;, &#39;fontFamily&#39;, &#39;fontSize&#39;, &#39;color&#39;, &#39;|&#39;, &#39;align&#39;, &#39;formatOL&#39;, &#39;formatUL&#39;, &#39;outdent&#39;, &#39;indent&#39;, &#39;check&#39;, &#39;|&#39;, &#39;insertLink&#39;, &#39;insertImage&#39;, &#39;insertVideo&#39;, &#39;insertFile&#39;, &#39;insertTable&#39;, &#39;|&#39;, &#39;insertHR&#39;, &#39;selectAll&#39;, &#39;clearFormatting&#39;, &#39;|&#39;, &#39;spellChecker&#39;, &#39;|&#39;, &#39;undo&#39;, &#39;redo&#39;],\r\n      };\r\n    \r\n    \r\n      handleModelChange = (data) =&gt; {\r\n        this.setState({\r\n          model: data,\r\n        });\r\n        this.saveEvent();\r\n      }\r\n    \r\n      &lt;FroalaEditor\r\n             config={this.config}\r\n             model={this.state.model}\r\n             onModelChange={this.handleModelChange}\r\n       /&gt;\r\n\r\n  Implementing the above code added a custom checkbox button in the toolbar and on click of the button rendered the checkbox in the editor.\r\n\r\n**Issues which I am facing:**\r\n\r\n - Eventhough, I added undo property as true, the undo and redo doesn&#39;t\r\n   work for adding the checkbox from the custom button. \r\n - The checkbox state change i.e the checking and unchecking of the\r\n   checkbox doesn&#39;t call the onModelChange method.\r\n\r\nCould anyone throw a light on what&#39;s happening? I researched but I was not able to find the working solution.",
            "link": "https://stackoverflow.com/questions/49934948/react-froala-default-on-model-change-method-cannot-capture-the-checking-and-unch",
            "title": "React Froala default on model change method cannot capture the checking and unchecking of the checkbox",
            "body": "<p>I have implemented the <a href=\"https://github.com/froala/react-froala-wysiwyg\" rel=\"nofollow noreferrer\">https://github.com/froala/react-froala-wysiwyg</a> module and Froala works awesome.\nI added a custom button in the toolar and implemented the checkbox functionality.</p>\n\n<pre><code>import 'froala-editor/js/froala_editor.pkgd.min';\nimport 'froala-editor/css/froala_style.min.css';\nimport 'froala-editor/css/froala_editor.pkgd.min.css';\nimport 'font-awesome/css/font-awesome.css';\nimport FroalaEditor from 'react-froala-wysiwyg';\n\n  componentDidMount() {\n    $.FroalaEditor.DefineIcon('check', { NAME: 'check-square' });\n    $.FroalaEditor.RegisterCommand('check', {\n      title: 'check-box',\n      focus: true,\n      undo: true,\n      refreshAfterCallback: false,\n      callback: () =&gt; {\n        this.setState({ model: this.state.model + '&lt;input type=\"checkbox\" /&gt;' });\n      },\n    });\n  }\n\n  config = {\n    placeholderText: 'Your notes here....',\n    heightMin: 250,\n    heightMax: 400,\n    colorsHEXInput: false,\n    autoFocus: true,\n    toolbarBottom: true,\n    linkAlwaysBlank: true,\n    fontFamilySelection: true,\n    fontSizeSelection: true,\n    paragraphFormatSelection: true,\n    htmlExecuteScripts: true,\n    iframe: true,\n    tabSpaces: 4,\n    pluginsEnabled: ['align', 'charCounter', 'codeBeautifier', 'codeView', 'colors', 'draggable', 'embedly', 'emoticons', 'entities', 'file', 'fontFamily', 'fontSize', 'fullscreen', 'image', 'imageManager', 'inlineStyle', 'lineBreaker', 'link', 'lists', 'paragraphFormat', 'paragraphStyle', 'quickInsert', 'quote', 'save', 'table', 'url', 'video', 'wordPaste'],\n    toolbarButtons: ['bold', 'italic', 'underline', 'strikeThrough', 'subscript', 'superscript', '|', 'fontFamily', 'fontSize', 'color', 'inlineStyle', 'paragraphStyle', '|', 'paragraphFormat', 'align', 'formatOL', 'formatUL', 'outdent', 'indent', 'quote', 'check', '|', 'insertLink', 'insertImage', 'insertVideo', 'embedly', 'insertFile', 'insertTable', '|', 'emoticons', 'specialCharacters', 'insertHR', 'selectAll', 'clearFormatting', '|', 'spellChecker', 'help', 'html', '|', 'undo', 'redo'],\n    toolbarButtonsMD: ['bold', 'italic', 'underline', 'strikeThrough', 'subscript', 'superscript', '|', 'fontFamily', 'fontSize', 'color', 'inlineStyle', 'paragraphStyle', '|', 'paragraphFormat', 'align', 'formatOL', 'formatUL', 'outdent', 'indent', 'quote', 'check', '|', 'insertLink', 'insertImage', 'insertVideo', 'embedly', 'insertFile', 'insertTable', '|', 'emoticons', 'specialCharacters', 'insertHR', 'selectAll', 'clearFormatting', '|', 'spellChecker', 'help', 'html', '|', 'undo', 'redo'],\n    toolbarButtonsSM: ['bold', 'italic', 'underline', 'strikeThrough', '|', 'fontFamily', 'fontSize', 'color', '|', 'align', 'formatOL', 'formatUL', 'outdent', 'indent', 'quote', 'check', '|', 'insertLink', 'insertImage', 'insertVideo', 'embedly', 'insertFile', 'insertTable', '|', 'emoticons', 'specialCharacters', 'insertHR', 'selectAll', 'clearFormatting', '|', 'spellChecker', 'help', 'html', '|', 'undo', 'redo'],\n    toolbarButtonsXS: ['bold', 'italic', 'underline', '|', 'fontFamily', 'fontSize', 'color', '|', 'align', 'formatOL', 'formatUL', 'outdent', 'indent', 'check', '|', 'insertLink', 'insertImage', 'insertVideo', 'insertFile', 'insertTable', '|', 'insertHR', 'selectAll', 'clearFormatting', '|', 'spellChecker', '|', 'undo', 'redo'],\n  };\n\n\n  handleModelChange = (data) =&gt; {\n    this.setState({\n      model: data,\n    });\n    this.saveEvent();\n  }\n\n  &lt;FroalaEditor\n         config={this.config}\n         model={this.state.model}\n         onModelChange={this.handleModelChange}\n   /&gt;\n</code></pre>\n\n<p>Implementing the above code added a custom checkbox button in the toolbar and on click of the button rendered the checkbox in the editor.</p>\n\n<p><strong>Issues which I am facing:</strong></p>\n\n<ul>\n<li>Eventhough, I added undo property as true, the undo and redo doesn't\nwork for adding the checkbox from the custom button. </li>\n<li>The checkbox state change i.e the checking and unchecking of the\ncheckbox doesn't call the onModelChange method.</li>\n</ul>\n\n<p>Could anyone throw a light on what's happening? I researched but I was not able to find the working solution.</p>\n"
        },
        {
            "tags": [
                "sql",
                "arrays",
                "aspen"
            ],
            "owner": {
                "reputation": 363,
                "user_id": 4412610,
                "user_type": "registered",
                "accept_rate": 46,
                "profile_image": "https://www.gravatar.com/avatar/de9ba9338d342841ada0d83532f52bd1?s=128&d=identicon&r=PG&f=1",
                "display_name": "Maxime",
                "link": "https://stackoverflow.com/users/4412610/maxime"
            },
            "is_answered": false,
            "view_count": 54,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524203854,
            "creation_date": 1523963005,
            "last_edit_date": 1524203854,
            "question_id": 49876411,
            "body_markdown": "I have a table with three columns, that can be represented as follows:\r\n\r\n    |---|---|---|\r\n    | 1 | 2 | 3 |\r\n    |---|---|---|\r\n    | A | B | C |    \r\n    | B | C | A |\r\n    | C | A | B |\r\n\r\nAnd a 2D-Array, like this:\r\n\r\n    [\r\n     [A,B,C],\r\n     [C,A,B],\r\n     [B,A,C]\r\n            ]\r\nI would like to select only the rows in the table matching one of the rows in the 2D array.\r\n\r\nThe obvious solution is to loop through the and make 3 queries, but I am wondering if there is any way to make this in a single `SELECT`\r\n\r\n---\r\n\r\nEdit: A working example for the &#39;loop method&#39;.\r\n\r\n    SET LOG_ROWS = 0;\r\n    DECLARE LOCAL TEMPORARY TABLE MODULE.X(col1 char(1), col2 char(1), col3 char(1));\r\n    \r\n    local array, i integer;\r\n    \r\n    REDIM(array,3,3);\r\n    array[0,0] = &#39;A&#39;;\r\n    array[0,1] = &#39;B&#39;;\r\n    array[0,2] = &#39;C&#39;;\r\n    array[1,0] = &#39;C&#39;;\r\n    array[1,1] = &#39;A&#39;;\r\n    array[1,2] = &#39;B&#39;;\r\n    array[2,0] = &#39;B&#39;;\r\n    array[2,1] = &#39;A&#39;;\r\n    array[2,2] = &#39;C&#39;;\r\n    \r\n    \r\n    INSERT INTO MODULE.X VALUES(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;);\r\n    INSERT INTO MODULE.X VALUES(&#39;B&#39;,&#39;C&#39;,&#39;A&#39;);\r\n    INSERT INTO MODULE.X VALUES(&#39;C&#39;,&#39;A&#39;,&#39;B&#39;);\r\n    \r\n    FOR i = 0 TO UBound(array) DO\r\n    \tSELECT * FROM MODULE.X WHERE col1 = array[i,0] AND col2 = array[i,1] AND col3 = array[i,2];\r\n    END \r\n\r\nResult:\r\n\r\n    col1 col2 col3\r\n    ---- ---- ----\r\n    A    B    C\r\n    col1 col2 col3\r\n    ---- ---- ----\r\n    C    A    B",
            "link": "https://stackoverflow.com/questions/49876411/is-it-possible-to-filter-values-using-a-2d-array-in-sql",
            "title": "Is it possible to filter values using a 2D array in SQL?",
            "body": "<p>I have a table with three columns, that can be represented as follows:</p>\n\n<pre><code>|---|---|---|\n| 1 | 2 | 3 |\n|---|---|---|\n| A | B | C |    \n| B | C | A |\n| C | A | B |\n</code></pre>\n\n<p>And a 2D-Array, like this:</p>\n\n<pre><code>[\n [A,B,C],\n [C,A,B],\n [B,A,C]\n        ]\n</code></pre>\n\n<p>I would like to select only the rows in the table matching one of the rows in the 2D array.</p>\n\n<p>The obvious solution is to loop through the and make 3 queries, but I am wondering if there is any way to make this in a single <code>SELECT</code></p>\n\n<hr>\n\n<p>Edit: A working example for the 'loop method'.</p>\n\n<pre><code>SET LOG_ROWS = 0;\nDECLARE LOCAL TEMPORARY TABLE MODULE.X(col1 char(1), col2 char(1), col3 char(1));\n\nlocal array, i integer;\n\nREDIM(array,3,3);\narray[0,0] = 'A';\narray[0,1] = 'B';\narray[0,2] = 'C';\narray[1,0] = 'C';\narray[1,1] = 'A';\narray[1,2] = 'B';\narray[2,0] = 'B';\narray[2,1] = 'A';\narray[2,2] = 'C';\n\n\nINSERT INTO MODULE.X VALUES('A','B','C');\nINSERT INTO MODULE.X VALUES('B','C','A');\nINSERT INTO MODULE.X VALUES('C','A','B');\n\nFOR i = 0 TO UBound(array) DO\n    SELECT * FROM MODULE.X WHERE col1 = array[i,0] AND col2 = array[i,1] AND col3 = array[i,2];\nEND \n</code></pre>\n\n<p>Result:</p>\n\n<pre><code>col1 col2 col3\n---- ---- ----\nA    B    C\ncol1 col2 col3\n---- ---- ----\nC    A    B\n</code></pre>\n"
        },
        {
            "tags": [
                "node.js",
                "heroku",
                "angular-cli"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9673079,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/a55342d8b01776093a7c2f95f3d596c5?s=128&d=identicon&r=PG&f=1",
                "display_name": "lozzam",
                "link": "https://stackoverflow.com/users/9673079/lozzam"
            },
            "is_answered": false,
            "view_count": 13,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524203853,
            "creation_date": 1524201806,
            "last_edit_date": 1524203853,
            "question_id": 49934533,
            "body_markdown": "I&#39;ve recently upgraded my angular2 app to angular 5 and I am having trouble deploying it to heroku where it used to work beforehand.\r\n\r\nThe issue here is that I have a Procfile that has this: &#39;web: node dist/server/app.js&#39; - where the launch file is stored but heroku is trying to find it add this path - &#39;app/dist/server/app.js&#39; I&#39;m not sure where this /app is getting appended but it is causing the issue.\r\n\r\nCheers\r\n\r\n[error description][1]\r\n\r\n    {\r\n    &quot;name&quot;: &quot;&quot;,\r\n    &quot;version&quot;: &quot;4.2.4&quot;,\r\n    &quot;license&quot;: &quot;&quot;,\r\n    &quot;author&quot;: &quot;&quot;,\r\n    &quot;description&quot;: &quot;&quot;,\r\n    &quot;angular-cli&quot;: {},\r\n    &quot;engines&quot;: {\r\n        &quot;node&quot;: &quot;8.9.4&quot;,\r\n        &quot;npm&quot;: &quot;5.6.0&quot;\r\n    },\r\n    &quot;scripts&quot;: {\r\n        &quot;ng&quot;: &quot;ng&quot;,\r\n        &quot;build&quot;: &quot;ng build --prod&quot;,\r\n        &quot;start&quot;: &quot;node dist/server/app.js&quot;,\r\n        &quot;predev&quot;: &quot;tsc -p server&quot;,\r\n        &quot;dev&quot;: &quot;node ./node_modules/mongodb-migrate -runmm -dbn dbSettings up &amp;&amp; concurrently \\&quot;mongod\\&quot; \\&quot;ng serve -pc proxy.conf.json --open\\&quot; \\&quot;tsc -w -p server\\&quot; \\&quot;nodemon dist/server/app.js\\&quot;&quot;,\r\n        &quot;prod&quot;: &quot;concurrently \\&quot;mongod\\&quot; \\&quot;ng build -aot -prod &amp;&amp; tsc -p server &amp;&amp; node dist/server/app.js\\&quot;&quot;,\r\n        &quot;test&quot;: &quot;ng test&quot;,\r\n        &quot;testbe&quot;: &quot;tsc -p server &amp;&amp; mocha dist/server/test --exit&quot;,\r\n        &quot;lint&quot;: &quot;ng lint&quot;,\r\n        &quot;lintbe&quot;: &quot;tslint server/**/**.ts{,x}&quot;,\r\n        &quot;e2e&quot;: &quot;ng e2e&quot;,\r\n        &quot;migrations&quot;: &quot;node ./node_modules/mongodb-migrate -runmm -dbn dbSettings up&quot;,\r\n        &quot;create-migration&quot;: &quot;node ./node_modules/mongodb-migrate -runmm create&quot;\r\n    },\r\n    &quot;private&quot;: true,\r\n    &quot;dependencies&quot;: {\r\n        &quot;@angular/animations&quot;: &quot;^5.2.0&quot;,\r\n        &quot;@angular/cdk&quot;: &quot;2.0.0-beta.12&quot;,\r\n        &quot;@angular/common&quot;: &quot;^5.2.0&quot;,\r\n        &quot;@angular/compiler&quot;: &quot;^5.2.0&quot;,\r\n        &quot;@angular/core&quot;: &quot;^5.2.0&quot;,\r\n        &quot;@angular/forms&quot;: &quot;^5.2.0&quot;,\r\n        &quot;@angular/http&quot;: &quot;^5.2.0&quot;,\r\n        &quot;@angular/material&quot;: &quot;2.0.0-beta.12&quot;,\r\n        &quot;@angular/platform-browser&quot;: &quot;^5.2.0&quot;,\r\n        &quot;@angular/platform-browser-dynamic&quot;: &quot;^5.2.0&quot;,\r\n        &quot;@angular/router&quot;: &quot;^5.2.0&quot;,\r\n        &quot;@sendgrid/mail&quot;: &quot;^6.2.1&quot;,\r\n        &quot;angular-bootstrap-md&quot;: &quot;^5.1.2&quot;,\r\n        &quot;angular2-jwt&quot;: &quot;^0.2.3&quot;,\r\n        &quot;bcryptjs&quot;: &quot;^2.4.3&quot;,\r\n        &quot;body-parser&quot;: &quot;^1.18.2&quot;,\r\n        &quot;bootstrap&quot;: &quot;4.0.0-alpha.5&quot;,\r\n        &quot;cheerio&quot;: &quot;^1.0.0-rc.2&quot;,\r\n        &quot;core-js&quot;: &quot;^2.4.1&quot;,\r\n        &quot;cron&quot;: &quot;^1.3.0&quot;,\r\n        &quot;dotenv&quot;: &quot;^4.0.0&quot;,\r\n        &quot;express&quot;: &quot;^4.16.3&quot;,\r\n        &quot;font-awesome&quot;: &quot;^4.7.0&quot;,\r\n        &quot;hammerjs&quot;: &quot;^2.0.8&quot;,\r\n        &quot;jquery&quot;: &quot;^3.3.1&quot;,\r\n        &quot;jsonwebtoken&quot;: &quot;^8.2.1&quot;,\r\n        &quot;migrate-mongo&quot;: &quot;^2.2.1&quot;,\r\n        &quot;moment&quot;: &quot;^2.20.1&quot;,\r\n        &quot;mongodb-migrate&quot;: &quot;^2.0.2&quot;,\r\n        &quot;mongoose&quot;: &quot;^5.0.13&quot;,\r\n        &quot;morgan&quot;: &quot;^1.9.0&quot;,\r\n        &quot;popper.js&quot;: &quot;^1.14.3&quot;,\r\n        &quot;npm-check&quot;: &quot;^5.6.0&quot;,\r\n        &quot;redis&quot;: &quot;^2.8.0&quot;,\r\n        &quot;request&quot;: &quot;^2.85.0&quot;,\r\n        &quot;rxjs&quot;: &quot;^5.5.6&quot;,\r\n        &quot;tether&quot;: &quot;1.4.0&quot;,\r\n        &quot;zone.js&quot;: &quot;^0.8.19&quot;\r\n    },\r\n    &quot;devDependencies&quot;: {\r\n        &quot;@angular/cli&quot;: &quot;~1.7.3&quot;,\r\n        &quot;@angular/compiler-cli&quot;: &quot;^5.2.0&quot;,\r\n        &quot;@angular/language-service&quot;: &quot;^5.2.0&quot;,\r\n        &quot;@types/jasmine&quot;: &quot;^2.8.6&quot;,\r\n        &quot;@types/jasminewd2&quot;: &quot;~2.0.2&quot;,\r\n        &quot;@types/node&quot;: &quot;~6.0.60&quot;,\r\n        &quot;chai&quot;: &quot;^4.1.2&quot;,\r\n        &quot;chai-http&quot;: &quot;^4.0.0&quot;,\r\n        &quot;codelyzer&quot;: &quot;~4.0.1&quot;,\r\n        &quot;concurrently&quot;: &quot;^3.5.1&quot;,\r\n        &quot;jasmine-core&quot;: &quot;~2.8.0&quot;,\r\n        &quot;jasmine-spec-reporter&quot;: &quot;~4.2.1&quot;,\r\n        &quot;karma&quot;: &quot;~2.0.0&quot;,\r\n        &quot;karma-chrome-launcher&quot;: &quot;~2.2.0&quot;,\r\n        &quot;karma-coverage-istanbul-reporter&quot;: &quot;^1.2.1&quot;,\r\n        &quot;karma-jasmine&quot;: &quot;~1.1.0&quot;,\r\n        &quot;karma-jasmine-html-reporter&quot;: &quot;^0.2.2&quot;,\r\n        &quot;mocha&quot;: &quot;^5.0.5&quot;,\r\n        &quot;nodemon&quot;: &quot;^1.17.3&quot;,\r\n        &quot;protractor&quot;: &quot;~5.1.2&quot;,\r\n        &quot;ts-node&quot;: &quot;~4.1.0&quot;,\r\n        &quot;tslint&quot;: &quot;~5.9.1&quot;,\r\n        &quot;typescript&quot;: &quot;~2.5.3&quot;\r\n    }\r\n}\r\n\r\n\r\n  [1]: https://i.stack.imgur.com/8Fw9d.png",
            "link": "https://stackoverflow.com/questions/49934533/heroku-node-deployment-cannot-find-module-app-dist-server-app-js",
            "title": "heroku/node deployment - cannot find module &#39;app/dist/server/app.js&#39;",
            "body": "<p>I've recently upgraded my angular2 app to angular 5 and I am having trouble deploying it to heroku where it used to work beforehand.</p>\n\n<p>The issue here is that I have a Procfile that has this: 'web: node dist/server/app.js' - where the launch file is stored but heroku is trying to find it add this path - 'app/dist/server/app.js' I'm not sure where this /app is getting appended but it is causing the issue.</p>\n\n<p>Cheers</p>\n\n<p><a href=\"https://i.stack.imgur.com/8Fw9d.png\" rel=\"nofollow noreferrer\">error description</a></p>\n\n<pre><code>{\n\"name\": \"\",\n\"version\": \"4.2.4\",\n\"license\": \"\",\n\"author\": \"\",\n\"description\": \"\",\n\"angular-cli\": {},\n\"engines\": {\n    \"node\": \"8.9.4\",\n    \"npm\": \"5.6.0\"\n},\n\"scripts\": {\n    \"ng\": \"ng\",\n    \"build\": \"ng build --prod\",\n    \"start\": \"node dist/server/app.js\",\n    \"predev\": \"tsc -p server\",\n    \"dev\": \"node ./node_modules/mongodb-migrate -runmm -dbn dbSettings up &amp;&amp; concurrently \\\"mongod\\\" \\\"ng serve -pc proxy.conf.json --open\\\" \\\"tsc -w -p server\\\" \\\"nodemon dist/server/app.js\\\"\",\n    \"prod\": \"concurrently \\\"mongod\\\" \\\"ng build -aot -prod &amp;&amp; tsc -p server &amp;&amp; node dist/server/app.js\\\"\",\n    \"test\": \"ng test\",\n    \"testbe\": \"tsc -p server &amp;&amp; mocha dist/server/test --exit\",\n    \"lint\": \"ng lint\",\n    \"lintbe\": \"tslint server/**/**.ts{,x}\",\n    \"e2e\": \"ng e2e\",\n    \"migrations\": \"node ./node_modules/mongodb-migrate -runmm -dbn dbSettings up\",\n    \"create-migration\": \"node ./node_modules/mongodb-migrate -runmm create\"\n},\n\"private\": true,\n\"dependencies\": {\n    \"@angular/animations\": \"^5.2.0\",\n    \"@angular/cdk\": \"2.0.0-beta.12\",\n    \"@angular/common\": \"^5.2.0\",\n    \"@angular/compiler\": \"^5.2.0\",\n    \"@angular/core\": \"^5.2.0\",\n    \"@angular/forms\": \"^5.2.0\",\n    \"@angular/http\": \"^5.2.0\",\n    \"@angular/material\": \"2.0.0-beta.12\",\n    \"@angular/platform-browser\": \"^5.2.0\",\n    \"@angular/platform-browser-dynamic\": \"^5.2.0\",\n    \"@angular/router\": \"^5.2.0\",\n    \"@sendgrid/mail\": \"^6.2.1\",\n    \"angular-bootstrap-md\": \"^5.1.2\",\n    \"angular2-jwt\": \"^0.2.3\",\n    \"bcryptjs\": \"^2.4.3\",\n    \"body-parser\": \"^1.18.2\",\n    \"bootstrap\": \"4.0.0-alpha.5\",\n    \"cheerio\": \"^1.0.0-rc.2\",\n    \"core-js\": \"^2.4.1\",\n    \"cron\": \"^1.3.0\",\n    \"dotenv\": \"^4.0.0\",\n    \"express\": \"^4.16.3\",\n    \"font-awesome\": \"^4.7.0\",\n    \"hammerjs\": \"^2.0.8\",\n    \"jquery\": \"^3.3.1\",\n    \"jsonwebtoken\": \"^8.2.1\",\n    \"migrate-mongo\": \"^2.2.1\",\n    \"moment\": \"^2.20.1\",\n    \"mongodb-migrate\": \"^2.0.2\",\n    \"mongoose\": \"^5.0.13\",\n    \"morgan\": \"^1.9.0\",\n    \"popper.js\": \"^1.14.3\",\n    \"npm-check\": \"^5.6.0\",\n    \"redis\": \"^2.8.0\",\n    \"request\": \"^2.85.0\",\n    \"rxjs\": \"^5.5.6\",\n    \"tether\": \"1.4.0\",\n    \"zone.js\": \"^0.8.19\"\n},\n\"devDependencies\": {\n    \"@angular/cli\": \"~1.7.3\",\n    \"@angular/compiler-cli\": \"^5.2.0\",\n    \"@angular/language-service\": \"^5.2.0\",\n    \"@types/jasmine\": \"^2.8.6\",\n    \"@types/jasminewd2\": \"~2.0.2\",\n    \"@types/node\": \"~6.0.60\",\n    \"chai\": \"^4.1.2\",\n    \"chai-http\": \"^4.0.0\",\n    \"codelyzer\": \"~4.0.1\",\n    \"concurrently\": \"^3.5.1\",\n    \"jasmine-core\": \"~2.8.0\",\n    \"jasmine-spec-reporter\": \"~4.2.1\",\n    \"karma\": \"~2.0.0\",\n    \"karma-chrome-launcher\": \"~2.2.0\",\n    \"karma-coverage-istanbul-reporter\": \"^1.2.1\",\n    \"karma-jasmine\": \"~1.1.0\",\n    \"karma-jasmine-html-reporter\": \"^0.2.2\",\n    \"mocha\": \"^5.0.5\",\n    \"nodemon\": \"^1.17.3\",\n    \"protractor\": \"~5.1.2\",\n    \"ts-node\": \"~4.1.0\",\n    \"tslint\": \"~5.9.1\",\n    \"typescript\": \"~2.5.3\"\n}\n</code></pre>\n\n<p>}</p>\n"
        },
        {
            "tags": [
                "android",
                "firebase",
                "service",
                "playback"
            ],
            "owner": {
                "reputation": 4,
                "user_id": 9582529,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/b90ed775d3fe51fc8b12ea8ffb49a1aa?s=128&d=identicon&r=PG&f=1",
                "display_name": "Jacky Lim",
                "link": "https://stackoverflow.com/users/9582529/jacky-lim"
            },
            "is_answered": false,
            "view_count": 24,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524203847,
            "creation_date": 1524164716,
            "last_edit_date": 1524203847,
            "question_id": 49928467,
            "body_markdown": "Currently created a form that store user into Firebase server. However the app not responding due to an error named\r\n &gt;Firebase wont work unless you update google play services. \r\n\r\nI have follow every step mentioned in the Firebase tutorial and yet it is still not working due to the google play services issue. May i know how to solve the google play services issue? The Firebase version is 15.0.0. Thanks",
            "link": "https://stackoverflow.com/questions/49928467/firebase-wont-work-unless-you-update-google-play-services",
            "title": "Firebase wont work unless you update google play services",
            "body": "<p>Currently created a form that store user into Firebase server. However the app not responding due to an error named</p>\n\n<blockquote>\n  <p>Firebase wont work unless you update google play services. </p>\n</blockquote>\n\n<p>I have follow every step mentioned in the Firebase tutorial and yet it is still not working due to the google play services issue. May i know how to solve the google play services issue? The Firebase version is 15.0.0. Thanks</p>\n"
        },
        {
            "tags": [
                "c#",
                "azure-service-fabric",
                "service-fabric-stateless"
            ],
            "owner": {
                "reputation": 1,
                "user_id": 9670392,
                "user_type": "registered",
                "profile_image": "https://www.gravatar.com/avatar/22aed99fe83074df90ac7b6d8d01bfd6?s=128&d=identicon&r=PG&f=1",
                "display_name": "not_you",
                "link": "https://stackoverflow.com/users/9670392/not-you"
            },
            "is_answered": false,
            "view_count": 33,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1524203845,
            "creation_date": 1524152179,
            "question_id": 49925006,
            "body_markdown": "I&#39;m trying to create instances of stateless services on demand at runtime without additonal entries about types at manifest file.\r\n\r\nAt the begging I thought, that I can use \r\n\r\n    using (FabricRuntime fabricRuntime = FabricRuntime.Create())\r\n          {\r\n              fabricRuntime.RegisterServiceType(&quot;ServiceType&quot;, typeof(Service));\r\n          }\r\n\r\nand\r\n\r\n    ServiceRuntime.RegisterServiceAsync(&quot;ServiceType&quot;, context =&gt; new Service(context)).GetAwaiter().GetResult();\r\n\r\nHowerver I still have to register service type or service template at manifest file, but that&#39;s beside the point.\r\n\r\nNow, I wonder if it is possible to create instance of stateless service without metadata at manifest or powershell scripts? Just write services programmatically?\r\n",
            "link": "https://stackoverflow.com/questions/49925006/how-to-create-a-stateless-service-at-runtime-on-local-service-fabric-server",
            "title": "how to create a stateless service at runtime on local service fabric server",
            "body": "<p>I'm trying to create instances of stateless services on demand at runtime without additonal entries about types at manifest file.</p>\n\n<p>At the begging I thought, that I can use </p>\n\n<pre><code>using (FabricRuntime fabricRuntime = FabricRuntime.Create())\n      {\n          fabricRuntime.RegisterServiceType(\"ServiceType\", typeof(Service));\n      }\n</code></pre>\n\n<p>and</p>\n\n<pre><code>ServiceRuntime.RegisterServiceAsync(\"ServiceType\", context =&gt; new Service(context)).GetAwaiter().GetResult();\n</code></pre>\n\n<p>Howerver I still have to register service type or service template at manifest file, but that's beside the point.</p>\n\n<p>Now, I wonder if it is possible to create instance of stateless service without metadata at manifest or powershell scripts? Just write services programmatically?</p>\n"
        },
        {
            "tags": [
                "django",
                "http-post",
                "httprequest"
            ],
            "owner": {
                "reputation": 75,
                "user_id": 2715898,
                "user_type": "registered",
                "accept_rate": 58,
                "profile_image": "https://www.gravatar.com/avatar/fb3ebfd4f8fdaf8d3af2dfc12b4571c2?s=128&d=identicon&r=PG&f=1",
                "display_name": "user2715898",
                "link": "https://stackoverflow.com/users/2715898/user2715898"
            },
            "is_answered": true,
            "view_count": 26,
            "accepted_answer_id": 49934727,
            "answer_count": 1,
            "score": 0,
            "last_activity_date": 1524203840,
            "creation_date": 1524201780,
            "question_id": 49934531,
            "body_markdown": "I want to make some decision based on the contents of request.payload-\r\n\r\n    XPKHmxbGVRRE9kUz5K6MtwL15NDedDHDb9IcTohZH7Vnrd0k5D4I0zjD4YR6yjhb\r\n    ------WebKitFormBoundaryVL2R7CYuDDjkPX51\r\n    Content-Disposition: form-data; name=&quot;filename&quot;; filename=&quot;abc.pdf&quot;\r\n    Content-Type: application/pdf\r\n\r\nBasically want to read if filename exists in this? How can I do this?\r\n`if &quot;filenanme&quot; in request.POST` seems incorrect.\r\n\r\n",
            "link": "https://stackoverflow.com/questions/49934531/how-to-read-the-request-payload-contents-in-django",
            "title": "how to read the request.payload contents in django?",
            "body": "<p>I want to make some decision based on the contents of request.payload-</p>\n\n<pre><code>XPKHmxbGVRRE9kUz5K6MtwL15NDedDHDb9IcTohZH7Vnrd0k5D4I0zjD4YR6yjhb\n------WebKitFormBoundaryVL2R7CYuDDjkPX51\nContent-Disposition: form-data; name=\"filename\"; filename=\"abc.pdf\"\nContent-Type: application/pdf\n</code></pre>\n\n<p>Basically want to read if filename exists in this? How can I do this?\n<code>if \"filenanme\" in request.POST</code> seems incorrect.</p>\n"
        },
        {
            "tags": [
                "spring-security",
                "spring-oauth2"
            ],
            "owner": {
                "reputation": 1672,
                "user_id": 1531064,
                "user_type": "registered",
                "accept_rate": 76,
                "profile_image": "https://i.stack.imgur.com/Em6sp.png?s=128&g=1",
                "display_name": "Cataclysm",
                "link": "https://stackoverflow.com/users/1531064/cataclysm"
            },
            "is_answered": false,
            "view_count": 17,
            "answer_count": 0,
            "score": 0,
            "last_activity_date": 1524203835,
            "creation_date": 1524203835,
            "question_id": 49934936,
            "body_markdown": "Has there anyway to retrieve all clientIds from `OAuth2Authentication` ?\r\nI can get single clientId from `Oauth2Request` as\r\n\r\n    String clientId = auth.getOAuth2Request().getClientId();\r\n\r\n but I&#39;d like to know them all.\r\n\r\n",
            "link": "https://stackoverflow.com/questions/49934936/spring-oauth2-how-to-get-all-clientids",
            "title": "Spring Oauth2 : How to get all clientIds?",
            "body": "<p>Has there anyway to retrieve all clientIds from <code>OAuth2Authentication</code> ?\nI can get single clientId from <code>Oauth2Request</code> as</p>\n\n<pre><code>String clientId = auth.getOAuth2Request().getClientId();\n</code></pre>\n\n<p>but I'd like to know them all.</p>\n"
        }
    ],
    "has_more": true,
    "quota_max": 300,
    "quota_remaining": 269
}